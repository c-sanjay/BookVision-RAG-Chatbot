[
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial statement analysis reviews financial information found on financial statements to make"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "informed decisions about the business. The income statement, statement of retained earnings,"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "balance sheet, and statement of cash flows, among other financial information, can be analyzed."
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information obtained from this analysis can benefit decision-making for internal and external"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "stakeholders and can give a company valuable information on overall performance and specific"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "areas for improvement. The analysis can help them with budgeting, deciding where to cut costs,"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "how to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "produced needs to be compared to others within industry and close competitors. The company"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "should also consider their past experience and how it corresponds to current and future"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 12,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "performance expectations. Three common analysis tools are used for decision-making; horizontal"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 13,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "analysis, vertical analysis, and financial ratios."
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 14,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "For our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 15,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "merchandising company that sells a variety of products. The image below shows the comparative"
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 16,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "income statements and balance sheets for the past two years."
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 17,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "1f7756fe-24c5-4750-987b-cee2e5787a4c",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 18,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are"
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "864bf2d6-fcb7-414e-a4c6-73cc0857ce62",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "ad999292-9670-45d2-90ee-73e5bc4ad17e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "2c0aea8e-c18b-4591-9acf-d4f465c20b50",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "cbf70b32-aaae-47fa-a94c-18ff6431755f",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "cbf70b32-aaae-47fa-a94c-18ff6431755f",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "cbf70b32-aaae-47fa-a94c-18ff6431755f",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "cbf70b32-aaae-47fa-a94c-18ff6431755f",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "20e86f42-4b63-4422-b13a-aefc6e53e45e",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "20e86f42-4b63-4422-b13a-aefc6e53e45e",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "20e86f42-4b63-4422-b13a-aefc6e53e45e",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "20e86f42-4b63-4422-b13a-aefc6e53e45e",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "19f3e485-4772-4eb9-acca-3002617a7c03",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "19f3e485-4772-4eb9-acca-3002617a7c03",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "19f3e485-4772-4eb9-acca-3002617a7c03",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "19f3e485-4772-4eb9-acca-3002617a7c03",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "3f62af75-8b63-41e0-86ab-0283cb2f894f",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "3f62af75-8b63-41e0-86ab-0283cb2f894f",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "3f62af75-8b63-41e0-86ab-0283cb2f894f",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "3f62af75-8b63-41e0-86ab-0283cb2f894f",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "8f512a08-18ce-441f-94cc-06f9876914eb",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "8f512a08-18ce-441f-94cc-06f9876914eb",
    "book_title": "Accounting_Terminology.pdf",
    "page": 1,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "ACCOUNTING TERMINOLOGY – UNIT 1 TO UNIT 5\nUNIT 1: TRANSACTIONS\n• Transaction – Any financial event that affects the business.\n• Cash Transaction – A transaction settled immediately in cash.\n• Credit Transaction – A transaction where payment is postponed.\n• Business Event – Any occurrence measurable in money.\n• Capital – Money invested by the owner.\n• Liability – Amount payable by the business.\n• Asset – Resource owned by the business.\n• Expense – Cost incurred to earn revenue.\n• Revenue – Income earned from business activities.\nUNIT 2: JOURNAL, LEDGER & TRIAL BALANCE\n• Journal – Book of original entry.\n• Ledger – Book of final entry containing all accounts.\n• Debit – Left side of an account.\n• Credit – Right side of an account.\n• Posting – Transferring entries from journal to ledger."
  },
  {
    "book_id": "8f512a08-18ce-441f-94cc-06f9876914eb",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "8f512a08-18ce-441f-94cc-06f9876914eb",
    "book_title": "Accounting_Terminology.pdf",
    "page": 2,
    "source": "Accounting_Terminology.pdf",
    "chunk_text": "• Net Profit – Gross profit minus all expenses.\n• Balance Sheet – Shows financial position of business.\n• Capital Account – Shows owner’s investment.\n• Current Assets – Assets convertible into cash within a year.\n• Current Liabilities – Amounts payable within a year.\n• Closing Stock – Unsold goods at the end of the period.\nUNIT 4: TOOLS TO CHECK FINANCIAL ACCOUNTS\n• Comparative Statement – Comparison of financial data of different years.\n• Common Size Statement – Expressing items as percentage of a base.\n• Trend Analysis – Studying changes over several periods.\n• Ratio Analysis – Evaluating financial performance using ratios.\n• Funds Flow Statement – Shows changes in working capital.\n• Cash Flow Statement – Shows inflow and outflow of cash."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data\nproduced needs to be compared to others within industry and close competitors. The company\nshould also consider their past experience and how it corresponds to current and future\nperformance expectations. Three common analysis tools are used for decision-making; horizontal\nanalysis, vertical analysis, and financial ratios.\nFor our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a\nmerchandising company that sells a variety of products. The image below shows the comparative\nincome statements and balance sheets for the past two years.\nFigure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are\nsimplified for our calculations and do not fully represent all the accounts a company could maintain.\nLet’s begin our analysis discussion by looking at horizontal analysis.\nHorizontal Analysis\nHorizontal analysis (also known as trend analysis) looks at trends over time on various financial\nstatement line items. A company will look at one period (usually a year) and compare it to another"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Using Banyan Goods as our example, if Banyan wanted to compare net sales in the current year\n(year of analysis) of $120,000 to the prior year (base year) of $100,000, the dollar change would be\nas follows:\nThe percentage change is found by taking the dollar change, dividing by the base year amount, and\nthen multiplying by 100.\nLet’s compute the percentage change for Banyan Goods’ net sales.\nThis means Banyan Goods saw an increase of $20,000 in net sales in the current year as compared\nto the prior year, which was a 20% increase. The same dollar change and percentage change\ncalculations would be used for the income statement line items as well as the balance sheet line\nitems. The image below shows the complete horizontal analysis of the income statement and\nbalance sheet for Banyan Goods."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Dollar change = $120,000– $1000,000 = $20,000\nPercentage change = (\n) × 100 = 20%\n$20,000\n$100,000"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Vertical Analysis\nVertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments.\nThe company will need to determine which line item they are comparing all items to within that\nstatement and then calculate the percentage makeup. These percentages are considered commonsize because they make businesses within industry comparable by taking out fluctuations for size. It\nis typical for an income statement to use net sales (or sales) as the comparison line item. This\nmeans net sales will be set at 100% and all other line items within the income statement will\nrepresent a percentage of net sales."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula to determine the common-size\npercentage is:\nFor example, if Banyan Goods set total assets as the base amount and wanted to see what\npercentage of total assets were made up of cash in the current year, the following calculation would\noccur.\nCash in the current year is $110,000 and total assets equal $250,000, giving a common-size\npercentage of 44%. If the company had an expected cash balance of 40% of total assets, they\nwould be exceeding expectations. This may not be enough of a difference to make a change, but if\nthey notice this deviates from industry standards, they may need to make adjustments, such as\nreducing the amount of cash on hand to reinvest in the business."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The image below shows the\ncommon-size calculations on the comparative income statements and comparative balance sheets\nfor Banyan Goods.\nFigure A3 Income Statements and Vertical Analysis.\nCommon-size percentage = (\n) × 100 = 44%\n$110,000\n$250,000"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A stakeholder could be looking to invest, become a supplier, make a loan, or alter\ninternal operations, among other things, based in part on the outcomes of ratio analysis. The\ninformation resulting from ratio analysis can be used to examine trends in performance, establish\nbenchmarks for success, set budget expectations, and compare industry competitors. There are four\nmain categories of ratios: liquidity, solvency, efficiency, and profitability. Note that while there are\nmore ideal outcomes for some ratios, the industry in which the business operates can change the\ninfluence each of these outcomes has over stakeholder decisions."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "(You will learn more about ratios,\nindustry standards, and ratio interpretation in advanced accounting courses.)\nLiquidity Ratios\nLiquidity ratios show the ability of the company to pay short-term obligations if they came due\nimmediately with assets that can be quickly converted to cash. This is done by comparing current\nassets to current liabilities. Lenders, for example, may consider the outcomes of liquidity ratios when\ndeciding whether to extend a loan to a company. A company would like to be liquid enough to\nmanage any currently due obligations but not too liquid where they may not be effectively investing\nin growth opportunities. Three common liquidity measurements are working capital, current ratio,\nand quick ratio."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working Capital\nWorking capital measures the financial health of an organization in the short-term by finding the\ndifference between current assets and current liabilities. A company will need enough current assets\nto cover current liabilities; otherwise, they may not be able to continue operations in the future.\nBefore a lender extends credit, they will review the working capital of the company to see if the\ncompany can meet their obligations. A larger difference signals that a company can cover their\nshort-term debts and a lender may be more willing to extend the loan. On the other hand, too large\nof a difference may indicate that the company may not be correctly using their assets to grow the\nbusiness."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula for working capital is:\nUsing Banyan Goods, working capital is computed as follows for the current year:\nIn this case, current assets were $200,000, and current liabilities were $100,000. Current assets\nwere far greater than current liabilities for Banyan Goods and they would easily be able to cover\nshort-term debt.\nThe dollar value of the difference for working capital is limited given company size and scope. It is\nmost useful to convert this information to a ratio to determine the company’s current financial health.\nThis ratio is the current ratio.\nCurrent Ratio\nWorking capital = $200,000– $100,000 = $100,000"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Quick Ratio\nThe quick ratio, also known as the acid-test ratio, is similar to the current ratio except current assets\nare more narrowly defined as the most liquid assets, which exclude inventory and prepaid expenses.\nThe conversion of inventory and prepaid expenses to cash can sometimes take more time than the\nliquidation of other current assets. A company will want to know what they have on hand and can\nuse quickly if an immediate obligation is due. The formula for the quick ratio is:\nThe quick ratio for Banyan Goods in the current year is:\nA 1.6:1 ratio means the company has enough quick assets to cover current liabilities.\nAnother category of financial measurement uses solvency ratios."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Solvency Ratios\nSolvency implies that a company can meet its long-term obligations and will likely stay in business in\nthe future. To stay in business the company must generate more revenue than debt in the long-term.\nMeeting long-term obligations includes the ability to pay any interest incurred on long-term debt. Two\nmain solvency ratios are the debt-to-equity ratio and the times interest earned ratio.\nDebt to Equity Ratio\nThe debt-to-equity ratio shows the relationship between debt and equity as it relates to business\nfinancing. A company can take out loans, issue stock, and retain earnings to be used in future\nperiods to keep operations running. It is less risky and less costly to use equity sources for financing\nas compared to debt resources."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "This is mainly due to interest expense repayment that a loan carries\nas opposed to equity, which does not have this requirement. Therefore, a company wants to know\nhow much debt and equity contribute to its financing. Ideally, a company would prefer more equity\nthan debt financing. The formula for the debt to equity ratio is:\nCurrent ratio = (\n) = 2 or 2:1\n$200,000\n$100,000\nQuick ratio = (\n) = 1.6 or 1.6:1\n$110,000 + $20,000 + $30,000\n$100,000"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Lenders will pay attention to\nthis ratio before extending credit. The more times over a company can cover interest, the more likely\na lender will extend long-term credit. The formula for times interest earned is:\nThe information needed to compute times interest earned for Banyan Goods in the current year can\nbe found on the income statement.\nThe $43,000 is the operating income, representing earnings before interest and taxes. The 21.5\ntimes outcome suggests that Banyan Goods can easily repay interest on an outstanding loan and\ncreditors would have little risk that Banyan Goods would be unable to pay.\nAnother category of financial measurement uses efficiency ratios.\nEfficiency Ratios\nEfficiency shows how well a company uses and manages their assets."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Areas of importance with\nefficiency are management of sales, accounts receivable, and inventory. A company that is efficient\ntypically will be able to generate revenues quickly using the assets it acquires. Let’s examine four\nefficiency ratios: accounts receivable turnover, total asset turnover, inventory turnover, and days’\nsales in inventory.\nAccounts Receivable Turnover\nAccounts receivable turnover measures how many times in a period (usually a year) a company will\ncollect cash from accounts receivable. A higher number of times could mean cash is collected more\nquickly and that credit customers are of high quality. A higher number is usually preferable because\nthe cash collected can be reinvested in the business at a quicker rate."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A lower number of times\ncould mean cash is collected slowly on these accounts and customers may not be properly qualified\nto accept the debt. The formula for accounts receivable turnover is:\nDebt-to-equity ratio = (\n) = 1.5 or 1.5:1\n$150,000\n$100,000\nTimes interest earned = (\n) = 21.5 times\n$43,000\n$2,000"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Given this\noutcome, they may want to consider stricter credit lending practices to make sure credit customers\nare of a higher quality. They may also need to be more aggressive with collecting any outstanding\naccounts.\nTotal Asset Turnover\nTotal asset turnover measures the ability of a company to use their assets to generate revenues. A\ncompany would like to use as few assets as possible to generate the most net sales. Therefore, a\nhigher total asset turnover means the company is using their assets very efficiently to produce net\nsales. The formula for total asset turnover is:\nAverage total assets are found by dividing the sum of beginning and ending total assets balances\nfound on the balance sheet."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The beginning total assets balance in the current year is taken from the\nending total assets balance in the prior year.\nBanyan Goods’ total asset turnover is:\nThe outcome of 0.53 means that for every $1 of assets, $0.53 of net sales are generated. Over time,\nBanyan Goods would like to see this turnover ratio increase.\nInventory Turnover\nAverage accounts receivable\nAccounts receivable turnover\n=\n=\n= $25,000\n$20,000+$30,000\n2\n= 4 times\n$100,000\n$25,000\nAverage total assets\nTotal assets turnover\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.53 times (rounded)\n$120,000\n$225,000"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Banyan Goods’ inventory turnover is:\n1.6 times is a very low turnover rate for Banyan Goods. This may mean the company is maintaining\ntoo high an inventory supply to meet a low demand from customers. They may want to decrease\ntheir on-hand inventory to free up more liquid assets to use in other ways.\nDays’ Sales in Inventory\nDays’ sales in inventory expresses the number of days it takes a company to turn inventory into\nsales. This assumes that no new purchase of inventory occurred within that time period. The fewer\nthe number of days, the more quickly the company can sell its inventory. The higher the number of\ndays, the longer it takes to sell its inventory. The formula for days’ sales in inventory is:\nBanyan Goods’ days’ sales in inventory is:\n243 days is a long time to sell inventory."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "While industry dictates what is an acceptable number of\ndays to sell inventory, 243 days is unsustainable long-term. Banyan Goods will need to better\nmanage their inventory and sales strategies to move inventory more quickly.\nThe last category of financial measurement examines profitability ratios.\nProfitability Ratios\nProfitability considers how well a company produces returns given their operational performance.\nThe company needs to leverage its operations to increase profit. To assist with profit goal\nAverage inventory\nInventory turnover\n=\n=\n= $37,500\n$35,000+$40,000\n2\n= 1.6 times\n$60,000\n$37,500\nDays' sales in inventory = (\n) × 365 = 243 days (rounded)\n$40,000\n$60,000"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "If Banyan Goods thinks this is too\nlow, the company would try and find ways to reduce expenses and increase sales.\nReturn on Total Assets\nThe return on total assets measures the company’s ability to use its assets successfully to generate\na profit. The higher the return (ratio outcome), the more profit is created from asset use. Average\ntotal assets are found by dividing the sum of beginning and ending total assets balances found on\nthe balance sheet. The beginning total assets balance in the current year is taken from the ending\ntotal assets balance in the prior year. The formula for return on total assets is:\nFor Banyan Goods, the return on total assets for the current year is:\nThe higher the figure, the better the company is using its assets to create a profit."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Industry standards\ncan dictate what is an acceptable return.\nReturn on Equity\nReturn on equity measures the company’s ability to use its invested capital to generate income. The\ninvested capital comes from stockholders investments in the company’s stock and its retained\nearnings and is leveraged to create profit. The higher the return, the better the company is doing at\nusing its investments to yield a profit. The formula for return on equity is:\nProfit margin = (\n) = 0.29 (rounded) or 29%\n$35,000\n$120,000\nAverage total assets\nReturn on total assets\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.16 (rounded) or 16%\n$35,000\n$225,000"
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Advantages and Disadvantages of Financial\nStatement Analysis\nThere are several advantages and disadvantages to financial statement analysis. Financial\nstatement analysis can show trends over time, which can be helpful in making future business\ndecisions. Converting information to percentages or ratios eliminates some of the disparity between\ncompetitor sizes and operating abilities, making it easier for stakeholders to make informed\ndecisions. It can assist with understanding the makeup of current operations within the business,\nand which shifts need to occur internally to increase productivity.\nA stakeholder needs to keep in mind that past performance does not always dictate future\nperformance."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Attention must be given to possible economic influences that could skew the numbers\nbeing analyzed, such as inflation or a recession. Additionally, the way a company reports information\nwithin accounts may change over time. For example, where and when certain transactions are\nrecorded may shift, which may not be readily evident in the financial statements.\nA company that wants to budget properly, control costs, increase revenues, and make long-term\nexpenditure decisions may want to use financial statement analysis to guide future operations. As\nlong as the company understands the limitations of the information provided, financial statement\nanalysis is a good tool to predict growth and company financial strength."
  },
  {
    "book_id": "e10e2459-c22e-41ca-b13b-bb46354fc9fb",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholder equity\nReturn on equity\n=\n=\n= $95,000\n$90,000+$100,000\n2\n= 0.37 (rounded) or 37%\n$35,000\n$95,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data\nproduced needs to be compared to others within industry and close competitors. The company\nshould also consider their past experience and how it corresponds to current and future\nperformance expectations. Three common analysis tools are used for decision-making; horizontal\nanalysis, vertical analysis, and financial ratios.\nFor our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a\nmerchandising company that sells a variety of products. The image below shows the comparative\nincome statements and balance sheets for the past two years.\nFigure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are\nsimplified for our calculations and do not fully represent all the accounts a company could maintain.\nLet’s begin our analysis discussion by looking at horizontal analysis.\nHorizontal Analysis\nHorizontal analysis (also known as trend analysis) looks at trends over time on various financial\nstatement line items. A company will look at one period (usually a year) and compare it to another"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Using Banyan Goods as our example, if Banyan wanted to compare net sales in the current year\n(year of analysis) of $120,000 to the prior year (base year) of $100,000, the dollar change would be\nas follows:\nThe percentage change is found by taking the dollar change, dividing by the base year amount, and\nthen multiplying by 100.\nLet’s compute the percentage change for Banyan Goods’ net sales.\nThis means Banyan Goods saw an increase of $20,000 in net sales in the current year as compared\nto the prior year, which was a 20% increase. The same dollar change and percentage change\ncalculations would be used for the income statement line items as well as the balance sheet line\nitems. The image below shows the complete horizontal analysis of the income statement and\nbalance sheet for Banyan Goods."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Dollar change = $120,000– $1000,000 = $20,000\nPercentage change = (\n) × 100 = 20%\n$20,000\n$100,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Vertical Analysis\nVertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments.\nThe company will need to determine which line item they are comparing all items to within that\nstatement and then calculate the percentage makeup. These percentages are considered commonsize because they make businesses within industry comparable by taking out fluctuations for size. It\nis typical for an income statement to use net sales (or sales) as the comparison line item. This\nmeans net sales will be set at 100% and all other line items within the income statement will\nrepresent a percentage of net sales."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula to determine the common-size\npercentage is:\nFor example, if Banyan Goods set total assets as the base amount and wanted to see what\npercentage of total assets were made up of cash in the current year, the following calculation would\noccur.\nCash in the current year is $110,000 and total assets equal $250,000, giving a common-size\npercentage of 44%. If the company had an expected cash balance of 40% of total assets, they\nwould be exceeding expectations. This may not be enough of a difference to make a change, but if\nthey notice this deviates from industry standards, they may need to make adjustments, such as\nreducing the amount of cash on hand to reinvest in the business."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The image below shows the\ncommon-size calculations on the comparative income statements and comparative balance sheets\nfor Banyan Goods.\nFigure A3 Income Statements and Vertical Analysis.\nCommon-size percentage = (\n) × 100 = 44%\n$110,000\n$250,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A stakeholder could be looking to invest, become a supplier, make a loan, or alter\ninternal operations, among other things, based in part on the outcomes of ratio analysis. The\ninformation resulting from ratio analysis can be used to examine trends in performance, establish\nbenchmarks for success, set budget expectations, and compare industry competitors. There are four\nmain categories of ratios: liquidity, solvency, efficiency, and profitability. Note that while there are\nmore ideal outcomes for some ratios, the industry in which the business operates can change the\ninfluence each of these outcomes has over stakeholder decisions."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "(You will learn more about ratios,\nindustry standards, and ratio interpretation in advanced accounting courses.)\nLiquidity Ratios\nLiquidity ratios show the ability of the company to pay short-term obligations if they came due\nimmediately with assets that can be quickly converted to cash. This is done by comparing current\nassets to current liabilities. Lenders, for example, may consider the outcomes of liquidity ratios when\ndeciding whether to extend a loan to a company. A company would like to be liquid enough to\nmanage any currently due obligations but not too liquid where they may not be effectively investing\nin growth opportunities. Three common liquidity measurements are working capital, current ratio,\nand quick ratio."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working Capital\nWorking capital measures the financial health of an organization in the short-term by finding the\ndifference between current assets and current liabilities. A company will need enough current assets\nto cover current liabilities; otherwise, they may not be able to continue operations in the future.\nBefore a lender extends credit, they will review the working capital of the company to see if the\ncompany can meet their obligations. A larger difference signals that a company can cover their\nshort-term debts and a lender may be more willing to extend the loan. On the other hand, too large\nof a difference may indicate that the company may not be correctly using their assets to grow the\nbusiness."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula for working capital is:\nUsing Banyan Goods, working capital is computed as follows for the current year:\nIn this case, current assets were $200,000, and current liabilities were $100,000. Current assets\nwere far greater than current liabilities for Banyan Goods and they would easily be able to cover\nshort-term debt.\nThe dollar value of the difference for working capital is limited given company size and scope. It is\nmost useful to convert this information to a ratio to determine the company’s current financial health.\nThis ratio is the current ratio.\nCurrent Ratio\nWorking capital = $200,000– $100,000 = $100,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Quick Ratio\nThe quick ratio, also known as the acid-test ratio, is similar to the current ratio except current assets\nare more narrowly defined as the most liquid assets, which exclude inventory and prepaid expenses.\nThe conversion of inventory and prepaid expenses to cash can sometimes take more time than the\nliquidation of other current assets. A company will want to know what they have on hand and can\nuse quickly if an immediate obligation is due. The formula for the quick ratio is:\nThe quick ratio for Banyan Goods in the current year is:\nA 1.6:1 ratio means the company has enough quick assets to cover current liabilities.\nAnother category of financial measurement uses solvency ratios."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Solvency Ratios\nSolvency implies that a company can meet its long-term obligations and will likely stay in business in\nthe future. To stay in business the company must generate more revenue than debt in the long-term.\nMeeting long-term obligations includes the ability to pay any interest incurred on long-term debt. Two\nmain solvency ratios are the debt-to-equity ratio and the times interest earned ratio.\nDebt to Equity Ratio\nThe debt-to-equity ratio shows the relationship between debt and equity as it relates to business\nfinancing. A company can take out loans, issue stock, and retain earnings to be used in future\nperiods to keep operations running. It is less risky and less costly to use equity sources for financing\nas compared to debt resources."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "This is mainly due to interest expense repayment that a loan carries\nas opposed to equity, which does not have this requirement. Therefore, a company wants to know\nhow much debt and equity contribute to its financing. Ideally, a company would prefer more equity\nthan debt financing. The formula for the debt to equity ratio is:\nCurrent ratio = (\n) = 2 or 2:1\n$200,000\n$100,000\nQuick ratio = (\n) = 1.6 or 1.6:1\n$110,000 + $20,000 + $30,000\n$100,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Lenders will pay attention to\nthis ratio before extending credit. The more times over a company can cover interest, the more likely\na lender will extend long-term credit. The formula for times interest earned is:\nThe information needed to compute times interest earned for Banyan Goods in the current year can\nbe found on the income statement.\nThe $43,000 is the operating income, representing earnings before interest and taxes. The 21.5\ntimes outcome suggests that Banyan Goods can easily repay interest on an outstanding loan and\ncreditors would have little risk that Banyan Goods would be unable to pay.\nAnother category of financial measurement uses efficiency ratios.\nEfficiency Ratios\nEfficiency shows how well a company uses and manages their assets."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Areas of importance with\nefficiency are management of sales, accounts receivable, and inventory. A company that is efficient\ntypically will be able to generate revenues quickly using the assets it acquires. Let’s examine four\nefficiency ratios: accounts receivable turnover, total asset turnover, inventory turnover, and days’\nsales in inventory.\nAccounts Receivable Turnover\nAccounts receivable turnover measures how many times in a period (usually a year) a company will\ncollect cash from accounts receivable. A higher number of times could mean cash is collected more\nquickly and that credit customers are of high quality. A higher number is usually preferable because\nthe cash collected can be reinvested in the business at a quicker rate."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A lower number of times\ncould mean cash is collected slowly on these accounts and customers may not be properly qualified\nto accept the debt. The formula for accounts receivable turnover is:\nDebt-to-equity ratio = (\n) = 1.5 or 1.5:1\n$150,000\n$100,000\nTimes interest earned = (\n) = 21.5 times\n$43,000\n$2,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Given this\noutcome, they may want to consider stricter credit lending practices to make sure credit customers\nare of a higher quality. They may also need to be more aggressive with collecting any outstanding\naccounts.\nTotal Asset Turnover\nTotal asset turnover measures the ability of a company to use their assets to generate revenues. A\ncompany would like to use as few assets as possible to generate the most net sales. Therefore, a\nhigher total asset turnover means the company is using their assets very efficiently to produce net\nsales. The formula for total asset turnover is:\nAverage total assets are found by dividing the sum of beginning and ending total assets balances\nfound on the balance sheet."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The beginning total assets balance in the current year is taken from the\nending total assets balance in the prior year.\nBanyan Goods’ total asset turnover is:\nThe outcome of 0.53 means that for every $1 of assets, $0.53 of net sales are generated. Over time,\nBanyan Goods would like to see this turnover ratio increase.\nInventory Turnover\nAverage accounts receivable\nAccounts receivable turnover\n=\n=\n= $25,000\n$20,000+$30,000\n2\n= 4 times\n$100,000\n$25,000\nAverage total assets\nTotal assets turnover\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.53 times (rounded)\n$120,000\n$225,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Banyan Goods’ inventory turnover is:\n1.6 times is a very low turnover rate for Banyan Goods. This may mean the company is maintaining\ntoo high an inventory supply to meet a low demand from customers. They may want to decrease\ntheir on-hand inventory to free up more liquid assets to use in other ways.\nDays’ Sales in Inventory\nDays’ sales in inventory expresses the number of days it takes a company to turn inventory into\nsales. This assumes that no new purchase of inventory occurred within that time period. The fewer\nthe number of days, the more quickly the company can sell its inventory. The higher the number of\ndays, the longer it takes to sell its inventory. The formula for days’ sales in inventory is:\nBanyan Goods’ days’ sales in inventory is:\n243 days is a long time to sell inventory."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "While industry dictates what is an acceptable number of\ndays to sell inventory, 243 days is unsustainable long-term. Banyan Goods will need to better\nmanage their inventory and sales strategies to move inventory more quickly.\nThe last category of financial measurement examines profitability ratios.\nProfitability Ratios\nProfitability considers how well a company produces returns given their operational performance.\nThe company needs to leverage its operations to increase profit. To assist with profit goal\nAverage inventory\nInventory turnover\n=\n=\n= $37,500\n$35,000+$40,000\n2\n= 1.6 times\n$60,000\n$37,500\nDays' sales in inventory = (\n) × 365 = 243 days (rounded)\n$40,000\n$60,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "If Banyan Goods thinks this is too\nlow, the company would try and find ways to reduce expenses and increase sales.\nReturn on Total Assets\nThe return on total assets measures the company’s ability to use its assets successfully to generate\na profit. The higher the return (ratio outcome), the more profit is created from asset use. Average\ntotal assets are found by dividing the sum of beginning and ending total assets balances found on\nthe balance sheet. The beginning total assets balance in the current year is taken from the ending\ntotal assets balance in the prior year. The formula for return on total assets is:\nFor Banyan Goods, the return on total assets for the current year is:\nThe higher the figure, the better the company is using its assets to create a profit."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Industry standards\ncan dictate what is an acceptable return.\nReturn on Equity\nReturn on equity measures the company’s ability to use its invested capital to generate income. The\ninvested capital comes from stockholders investments in the company’s stock and its retained\nearnings and is leveraged to create profit. The higher the return, the better the company is doing at\nusing its investments to yield a profit. The formula for return on equity is:\nProfit margin = (\n) = 0.29 (rounded) or 29%\n$35,000\n$120,000\nAverage total assets\nReturn on total assets\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.16 (rounded) or 16%\n$35,000\n$225,000"
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Advantages and Disadvantages of Financial\nStatement Analysis\nThere are several advantages and disadvantages to financial statement analysis. Financial\nstatement analysis can show trends over time, which can be helpful in making future business\ndecisions. Converting information to percentages or ratios eliminates some of the disparity between\ncompetitor sizes and operating abilities, making it easier for stakeholders to make informed\ndecisions. It can assist with understanding the makeup of current operations within the business,\nand which shifts need to occur internally to increase productivity.\nA stakeholder needs to keep in mind that past performance does not always dictate future\nperformance."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Attention must be given to possible economic influences that could skew the numbers\nbeing analyzed, such as inflation or a recession. Additionally, the way a company reports information\nwithin accounts may change over time. For example, where and when certain transactions are\nrecorded may shift, which may not be readily evident in the financial statements.\nA company that wants to budget properly, control costs, increase revenues, and make long-term\nexpenditure decisions may want to use financial statement analysis to guide future operations. As\nlong as the company understands the limitations of the information provided, financial statement\nanalysis is a good tool to predict growth and company financial strength."
  },
  {
    "book_id": "7c10ac73-7858-49a4-9362-3064a5c06b9e",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholder equity\nReturn on equity\n=\n=\n= $95,000\n$90,000+$100,000\n2\n= 0.37 (rounded) or 37%\n$35,000\n$95,000"
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 1,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "📘 REPORT ON CMIE-BASED AND CSO-BASED ACTIVITIES\n1. Introduction\nEconomic data plays a crucial role in understanding, analysing, and forecasting the performance of\na country. In India, two major sources of reliable economic and statistical information are:\nCMIE (Centre for Monitoring Indian Economy)\nCSO (Central Statistical Office), now part of NSO\nThis report presents a detailed study of CMIE-based and CSO-based activities, including their\ndatasets, analytical methods, and findings.\n2. CMIE-Based Activities\nCMIE provides extensive databases such as Prowess, Economic Outlook, CapEx, and Consumer\nPyramids. These datasets are used for company-level, sector-level, and macroeconomic analysis."
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 1,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "2.1 Company Financial Analysis (Using CMIE Prowess)\nObjective:\nTo analyse the financial performance of Indian companies.\nActivity Performed:\nExtracted balance sheet, income statement, and cash-flow data from Prowess.\nAnalysed profitability ratios (ROE, ROA, NPM), liquidity ratios (CR, QR), and solvency ratios\n(DER).\nCompared performance of firms within the same industry.\nFindings:\nLarge-cap firms show strong profitability and stable cash flows.\nWorking capital management varies widely across industries.\n2.2 Sector and Industry Analysis\nObjective:\nTo study trends and growth patterns in different sectors.\nActivity:\nUsed Industry Outlook to analyse sectors such as IT, Automobile, Banking, Pharmaceuticals.\nIdentified demand trends, cost structures, and production patterns.\nFindings:"
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 2,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Automobile sector shows recovery due to rising exports and EV demand.\nIT sector maintains stable growth driven by digital transformation.\nPharma shows long-term potential due to global demand.\n2.3 CapEx Project Analysis\nObjective:\nTo evaluate investment trends in India.\nActivity:\nAnalysed projects under categories: new investments, completed projects, and stalled\nprojects.\nFindings:\nNew project announcements increased in infrastructure and energy sectors.\nStalled project percentage remains moderate but improved compared to previous years.\n2.4 Macroeconomic Indicators\nObjective:\nTo analyse the overall economic environment."
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 2,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Indicators Studied:\nGDP growth\nInflation (CPI & WPI)\nTrade balance\nFiscal deficit\nEmployment rate\nFindings:\nGDP shows upward momentum driven by services and manufacturing.\nInflation remains moderate with volatility in food prices.\nEmployment shows gradual improvement post-pandemic.\n2.5 Consumer Pyramids Household Survey (CPHS)\nObjective:\nTo study household-level economic conditions."
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 3,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Activity:\nAnalysed income, spending, and employment status.\nFindings:\nUrban households show higher income stability than rural.\nConsumption on essentials remains dominant after inflationary pressures.\n3. CSO-Based Activities\nThe CSO (now NSO) is responsible for official statistics related to national income, price index,\nindustrial production, and demographics.\n3.1 National Income Estimation\nObjective:\nTo understand how GDP, GNP, NNP, and NDP are computed.\nMethods Studied:\nProduction method\nIncome method\nExpenditure method\nFindings:\nServices sector contributes the highest share to India’s GDP.\nManufacturing growth is improving gradually.\n3.2 IIP (Index of Industrial Production) Analysis\nObjective:\nTo analyse industrial performance monthly."
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 3,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Activity:\nCompared IIP growth in mining, manufacturing, and electricity.\nFindings:\nManufacturing shows steady recovery.\nElectricity generation has seen consistent growth.\nMining output fluctuates due to seasonal factors."
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 4,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "3.3 Inflation and Price Indices\nObjective:\nTo understand price movements in the economy.\nData Covered:\nCPI (Consumer Price Index)\nWPI (Wholesale Price Index)\nFindings:\nCPI inflation affected mainly by food and fuel prices.\nWPI inflation reflects raw material cost fluctuations.\n3.4 Population & Demographic Analysis\nObjective:\nTo study demographic trends in India.\nFindings:\nIndia has a young population with increasing literacy rates.\nLabour force participation is rising but varies by gender and region.\n3.5 Employment Analysis (PLFS)\nObjective:\nTo study labour market conditions.\nActivity:\nAnalysed unemployment rate, employment distribution, urban-rural comparisons.\nFindings:\nUrban unemployment remains higher.\nSelf-employment is a major source of livelihood in rural areas.\n4."
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 4,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Combined CMIE + CSO Analysis\nBoth datasets were used together for deeper insights.\n4.1 GDP vs Sectoral Performance\nCSO GDP data combined with CMIE industry data shows services as the main growth driver."
  },
  {
    "book_id": "6a63105d-6fdb-4c29-b5c1-6978971ef1f2",
    "book_title": "CMIE OR CSO.pdf",
    "page": 5,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "4.2 Inflation vs Company Costs\nCSO CPI/WPI data compared with CMIE Prowess cost structure:\nRising input prices impact automotive and FMCG margins.\n4.3 Employment (PLFS vs CPHS)\nCSO PLFS employment rate compared with CMIE CPHS shows differing patterns due to\nmethodology differences.\n5. Conclusion\nCMIE and CSO are two of the most important sources of economic data in India.\nCMIE is more micro-level (companies, industries, households).\nCSO provides macro-level, official national statistics.\nTogether, they help create a complete understanding of the Indian economy—covering\nproduction, prices, employment, investment, and industry trends.\nSuch data-driven analysis supports business decisions, government policy-making, and\nacademic research."
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 1,
    "source": "HYUNDAI.pdf",
    "chunk_text": "1. Key Financial Statements\nIncome Statement (Consolidated) — FY 2024-25\nBased on EquityMaster analysis: \nMetric\nFY 2023-24\nFY 2024-25\n% Change\nNet Sales\n₹ 698,291 million\n₹ 691,929 million\n–0.9% \nOther Income\n₹ 14,733 m\n₹ 8,700 m\n–40.9% \nTotal Revenues\n₹ 713,023 m\n₹ 700,629 m\n–1.7% \nGross Profit\n₹ 91,326 m\n₹ 89,538 m\n–2.0% \nDepreciation\n₹ 22,079 m\n₹ 21,053 m\n–4.7% \nInterest Expense\n₹ 1,581 m\n₹ 1,272 m\n–19.5% \nProfit Before Tax (PBT)\n₹ 82,399 m\n₹ 75,913 m\n–7.9% \nTax\n₹ 21,798 m\n₹ 19,511 m\n–10.5% \nProfit After Tax (PAT)\n₹ 60,600 m\n₹ 56,402 m\n–6.9% \nNet Profit Margin (PAT /\nTotal Revenues)\n~ 8.7%\n~ 8.2% \nBalance Sheet (Consolidated) — as on March 31, 2025\nUsing data from EquityMaster’s analysis."
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 1,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Item\nMarch 31, 2024\nMarch 31, 2025\n% Change\nNet Worth /\nShareholders’ Equity\n₹ 106,657 m\n₹ 162,965 m\n+52.8% \nCurrent Liabilities\n₹ 129,972 m\n₹ 112,663 m\n–13.3% \nLong-Term Debt\n₹ 6,228 m\n₹ 5,360 m\n–13.9% \nTotal Liabilities\n₹ 254,015 m\n₹ 290,653 m\n+14.4% \nCurrent Assets\n₹ 161,240 m\n₹ 160,323 m\n–0.6% \nEquitymaster\nEquitymaster +1\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster"
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 2,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Item\nMarch 31, 2024\nMarch 31, 2025\n% Change\nFixed Assets\n₹ 92,774 m\n₹ 130,329 m\n+40.5% \nTotal Assets\n₹ 254,015 m\n₹ 290,653 m\n+14.4% \nCash Flow (FY 2024-25)\nFrom EquityMaster’s summary: \nCash flow from operations: ₹ 43,449 million (decline from previous year) \nCash flow from investing activities: –₹ 4,138 million \nCash flow from financing activities: –₹ 629 million (much lower outflow vs prior) \nNet cash flow: +₹ 38,725 million \nAlternate / Supplemental Financials\nStockAnalysis gives a detailed breakdown of the income statement (up to TTM) including cost\nof revenue, SG&A etc. \nBalance-sheet details from StockAnalysis: cash, receivables, inventory, etc. \nDirectors’ report gives expense breakdown in their annual report. \n2."
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 2,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Financial Analysis & Interpretation\nHere’s a breakdown of what these numbers mean in terms of business performance, strengths,\nrisks, and future outlook.\nStrengths / Positives\n1. Strong Equity Growth\nNet worth jumped ~52.8% in one year. \nThis suggests the company is either retaining earnings strongly or revaluing its assets (or\nboth).\n2. Lower Debt Burden\nLong-term debt has reduced (–13.9%). \nLower leverage reduces financial risk; good for cash flows and flexibility.\n3. Capex / Fixed Asset Growth\nFixed assets increased by +40.5%. \nThis could imply that Hyundai is investing in capacity expansion, modernization, or new\nprojects — potentially preparing for future growth (exports, EV, etc).\n4."
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 2,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Positive Net Cash Flow\nDespite weaker operating cash flow, the company has positive net cash flow, primarily\nbecause financing outflows have dropped significantly. \nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nStockAnalysis\nStockAnalysis\nCapital Market\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster"
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 3,
    "source": "HYUNDAI.pdf",
    "chunk_text": "This is good, because it gives the company liquidity to invest or return cash.\nWeaknesses / Risks\n1. Decline in Operating Profitability\nGross profit fell slightly; PAT also declined. \nThis could indicate margin pressure, possibly from higher costs (raw materials,\nmanufacturing) or competitive pricing.\n2. Operating Cash Flow Drop\nCash from operations halved year on year. \nThis is a red flag because cash from core business is what sustains long-term operations. If\nthis continues, it could squeeze liquidity.\n3. Rising Liabilities\nTotal liabilities increased 14.4%. \nCombined with lower current liabilities but higher total liabilities — need to check what\nportion is non-current liabilities, provisions, or long-term payables.\n4."
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 3,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Volatility in Other Income\nOther income dropped significantly (–40.9%) in FY25. \nIf the company relies on other income (investments, non-core income), a fall there could\nhurt net profitability.\nKey Financial Ratios & Metrics (Derived)\nUsing the data above, some key metrics to watch / interpret:\nReturn on Equity (RoE): With large increase in equity, the RoE may come under\npressure unless profit also grows strongly.\nFixed Asset Turnover: Rising fixed assets + slightly declining revenue suggests turnover might\nworsen unless sales improve.\nCash Conversion Efficiency: The drop in operating cash flow suggests cash conversion is weak;\nneed to monitor working capital changes.\nLiquidity: Positive net cash flow is a good sign, but working capital and short-term liabilities\nneed further checking.\n3."
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 3,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Outlook & Considerations\nGrowth Opportunities: Hyundai’s strong asset investment hints at capacity expansion —\npossibly for export growth or EV manufacturing.\nExport-Driven Strategy: If exports grow (as seen in some recent quarters), they could drive\nprofitability and help absorb the higher fixed costs.\nCost Risks: Rising input costs (steel, semiconductors) or labor costs could pressurize margins\nfurther.\nCapital Structure: With reduced debt, the company is less risky from a financial leverage\nperspective, but equity is rising — so future profits have to scale to keep returns attractive.\nEquitymaster +1\nEquitymaster\nEquitymaster\nEquitymaster"
  },
  {
    "book_id": "196cf7a2-3dca-49f8-a9b1-582efab7264e",
    "book_title": "HYUNDAI.pdf",
    "page": 4,
    "source": "HYUNDAI.pdf",
    "chunk_text": "4. Conclusion\nOverall, Hyundai Motor India shows solid financial health: significant increase in net worth,\nreduced debt, and positive cash flow are all good signs.\nBut there are warning signals: dip in operating profitability, decline in operating cash flow, and\nincreasing liabilities need to be watched closely.\nThe company seems to be in an investment / expansion phase (based on fixed asset growth),\nwhich could pay off if demand (especially exports) holds up.\nFor long-term investors: this looks promising, but it's not without risk — especially operational\nrisk and cash flow risk."
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 1,
    "source": "HYUNDAI.pdf",
    "chunk_text": "1. Key Financial Statements\nIncome Statement (Consolidated) — FY 2024-25\nBased on EquityMaster analysis: \nMetric\nFY 2023-24\nFY 2024-25\n% Change\nNet Sales\n₹ 698,291 million\n₹ 691,929 million\n–0.9% \nOther Income\n₹ 14,733 m\n₹ 8,700 m\n–40.9% \nTotal Revenues\n₹ 713,023 m\n₹ 700,629 m\n–1.7% \nGross Profit\n₹ 91,326 m\n₹ 89,538 m\n–2.0% \nDepreciation\n₹ 22,079 m\n₹ 21,053 m\n–4.7% \nInterest Expense\n₹ 1,581 m\n₹ 1,272 m\n–19.5% \nProfit Before Tax (PBT)\n₹ 82,399 m\n₹ 75,913 m\n–7.9% \nTax\n₹ 21,798 m\n₹ 19,511 m\n–10.5% \nProfit After Tax (PAT)\n₹ 60,600 m\n₹ 56,402 m\n–6.9% \nNet Profit Margin (PAT /\nTotal Revenues)\n~ 8.7%\n~ 8.2% \nBalance Sheet (Consolidated) — as on March 31, 2025\nUsing data from EquityMaster’s analysis."
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 1,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Item\nMarch 31, 2024\nMarch 31, 2025\n% Change\nNet Worth /\nShareholders’ Equity\n₹ 106,657 m\n₹ 162,965 m\n+52.8% \nCurrent Liabilities\n₹ 129,972 m\n₹ 112,663 m\n–13.3% \nLong-Term Debt\n₹ 6,228 m\n₹ 5,360 m\n–13.9% \nTotal Liabilities\n₹ 254,015 m\n₹ 290,653 m\n+14.4% \nCurrent Assets\n₹ 161,240 m\n₹ 160,323 m\n–0.6% \nEquitymaster\nEquitymaster +1\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster"
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 2,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Item\nMarch 31, 2024\nMarch 31, 2025\n% Change\nFixed Assets\n₹ 92,774 m\n₹ 130,329 m\n+40.5% \nTotal Assets\n₹ 254,015 m\n₹ 290,653 m\n+14.4% \nCash Flow (FY 2024-25)\nFrom EquityMaster’s summary: \nCash flow from operations: ₹ 43,449 million (decline from previous year) \nCash flow from investing activities: –₹ 4,138 million \nCash flow from financing activities: –₹ 629 million (much lower outflow vs prior) \nNet cash flow: +₹ 38,725 million \nAlternate / Supplemental Financials\nStockAnalysis gives a detailed breakdown of the income statement (up to TTM) including cost\nof revenue, SG&A etc. \nBalance-sheet details from StockAnalysis: cash, receivables, inventory, etc. \nDirectors’ report gives expense breakdown in their annual report. \n2."
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 2,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Financial Analysis & Interpretation\nHere’s a breakdown of what these numbers mean in terms of business performance, strengths,\nrisks, and future outlook.\nStrengths / Positives\n1. Strong Equity Growth\nNet worth jumped ~52.8% in one year. \nThis suggests the company is either retaining earnings strongly or revaluing its assets (or\nboth).\n2. Lower Debt Burden\nLong-term debt has reduced (–13.9%). \nLower leverage reduces financial risk; good for cash flows and flexibility.\n3. Capex / Fixed Asset Growth\nFixed assets increased by +40.5%. \nThis could imply that Hyundai is investing in capacity expansion, modernization, or new\nprojects — potentially preparing for future growth (exports, EV, etc).\n4."
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 2,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Positive Net Cash Flow\nDespite weaker operating cash flow, the company has positive net cash flow, primarily\nbecause financing outflows have dropped significantly. \nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster\nStockAnalysis\nStockAnalysis\nCapital Market\nEquitymaster\nEquitymaster\nEquitymaster\nEquitymaster"
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 3,
    "source": "HYUNDAI.pdf",
    "chunk_text": "This is good, because it gives the company liquidity to invest or return cash.\nWeaknesses / Risks\n1. Decline in Operating Profitability\nGross profit fell slightly; PAT also declined. \nThis could indicate margin pressure, possibly from higher costs (raw materials,\nmanufacturing) or competitive pricing.\n2. Operating Cash Flow Drop\nCash from operations halved year on year. \nThis is a red flag because cash from core business is what sustains long-term operations. If\nthis continues, it could squeeze liquidity.\n3. Rising Liabilities\nTotal liabilities increased 14.4%. \nCombined with lower current liabilities but higher total liabilities — need to check what\nportion is non-current liabilities, provisions, or long-term payables.\n4."
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 3,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Volatility in Other Income\nOther income dropped significantly (–40.9%) in FY25. \nIf the company relies on other income (investments, non-core income), a fall there could\nhurt net profitability.\nKey Financial Ratios & Metrics (Derived)\nUsing the data above, some key metrics to watch / interpret:\nReturn on Equity (RoE): With large increase in equity, the RoE may come under\npressure unless profit also grows strongly.\nFixed Asset Turnover: Rising fixed assets + slightly declining revenue suggests turnover might\nworsen unless sales improve.\nCash Conversion Efficiency: The drop in operating cash flow suggests cash conversion is weak;\nneed to monitor working capital changes.\nLiquidity: Positive net cash flow is a good sign, but working capital and short-term liabilities\nneed further checking.\n3."
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 3,
    "source": "HYUNDAI.pdf",
    "chunk_text": "Outlook & Considerations\nGrowth Opportunities: Hyundai’s strong asset investment hints at capacity expansion —\npossibly for export growth or EV manufacturing.\nExport-Driven Strategy: If exports grow (as seen in some recent quarters), they could drive\nprofitability and help absorb the higher fixed costs.\nCost Risks: Rising input costs (steel, semiconductors) or labor costs could pressurize margins\nfurther.\nCapital Structure: With reduced debt, the company is less risky from a financial leverage\nperspective, but equity is rising — so future profits have to scale to keep returns attractive.\nEquitymaster +1\nEquitymaster\nEquitymaster\nEquitymaster"
  },
  {
    "book_id": "38adaf87-177b-40ea-b171-5773023f0eec",
    "book_title": "HYUNDAI.pdf",
    "page": 4,
    "source": "HYUNDAI.pdf",
    "chunk_text": "4. Conclusion\nOverall, Hyundai Motor India shows solid financial health: significant increase in net worth,\nreduced debt, and positive cash flow are all good signs.\nBut there are warning signals: dip in operating profitability, decline in operating cash flow, and\nincreasing liabilities need to be watched closely.\nThe company seems to be in an investment / expansion phase (based on fixed asset growth),\nwhich could pay off if demand (especially exports) holds up.\nFor long-term investors: this looks promising, but it's not without risk — especially operational\nrisk and cash flow risk."
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 1,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "📘 REPORT ON CMIE-BASED AND CSO-BASED ACTIVITIES\n1. Introduction\nEconomic data plays a crucial role in understanding, analysing, and forecasting the performance of\na country. In India, two major sources of reliable economic and statistical information are:\nCMIE (Centre for Monitoring Indian Economy)\nCSO (Central Statistical Office), now part of NSO\nThis report presents a detailed study of CMIE-based and CSO-based activities, including their\ndatasets, analytical methods, and findings.\n2. CMIE-Based Activities\nCMIE provides extensive databases such as Prowess, Economic Outlook, CapEx, and Consumer\nPyramids. These datasets are used for company-level, sector-level, and macroeconomic analysis."
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 1,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "2.1 Company Financial Analysis (Using CMIE Prowess)\nObjective:\nTo analyse the financial performance of Indian companies.\nActivity Performed:\nExtracted balance sheet, income statement, and cash-flow data from Prowess.\nAnalysed profitability ratios (ROE, ROA, NPM), liquidity ratios (CR, QR), and solvency ratios\n(DER).\nCompared performance of firms within the same industry.\nFindings:\nLarge-cap firms show strong profitability and stable cash flows.\nWorking capital management varies widely across industries.\n2.2 Sector and Industry Analysis\nObjective:\nTo study trends and growth patterns in different sectors.\nActivity:\nUsed Industry Outlook to analyse sectors such as IT, Automobile, Banking, Pharmaceuticals.\nIdentified demand trends, cost structures, and production patterns.\nFindings:"
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 2,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Automobile sector shows recovery due to rising exports and EV demand.\nIT sector maintains stable growth driven by digital transformation.\nPharma shows long-term potential due to global demand.\n2.3 CapEx Project Analysis\nObjective:\nTo evaluate investment trends in India.\nActivity:\nAnalysed projects under categories: new investments, completed projects, and stalled\nprojects.\nFindings:\nNew project announcements increased in infrastructure and energy sectors.\nStalled project percentage remains moderate but improved compared to previous years.\n2.4 Macroeconomic Indicators\nObjective:\nTo analyse the overall economic environment."
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 2,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Indicators Studied:\nGDP growth\nInflation (CPI & WPI)\nTrade balance\nFiscal deficit\nEmployment rate\nFindings:\nGDP shows upward momentum driven by services and manufacturing.\nInflation remains moderate with volatility in food prices.\nEmployment shows gradual improvement post-pandemic.\n2.5 Consumer Pyramids Household Survey (CPHS)\nObjective:\nTo study household-level economic conditions."
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 3,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Activity:\nAnalysed income, spending, and employment status.\nFindings:\nUrban households show higher income stability than rural.\nConsumption on essentials remains dominant after inflationary pressures.\n3. CSO-Based Activities\nThe CSO (now NSO) is responsible for official statistics related to national income, price index,\nindustrial production, and demographics.\n3.1 National Income Estimation\nObjective:\nTo understand how GDP, GNP, NNP, and NDP are computed.\nMethods Studied:\nProduction method\nIncome method\nExpenditure method\nFindings:\nServices sector contributes the highest share to India’s GDP.\nManufacturing growth is improving gradually.\n3.2 IIP (Index of Industrial Production) Analysis\nObjective:\nTo analyse industrial performance monthly."
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 3,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Activity:\nCompared IIP growth in mining, manufacturing, and electricity.\nFindings:\nManufacturing shows steady recovery.\nElectricity generation has seen consistent growth.\nMining output fluctuates due to seasonal factors."
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 4,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "3.3 Inflation and Price Indices\nObjective:\nTo understand price movements in the economy.\nData Covered:\nCPI (Consumer Price Index)\nWPI (Wholesale Price Index)\nFindings:\nCPI inflation affected mainly by food and fuel prices.\nWPI inflation reflects raw material cost fluctuations.\n3.4 Population & Demographic Analysis\nObjective:\nTo study demographic trends in India.\nFindings:\nIndia has a young population with increasing literacy rates.\nLabour force participation is rising but varies by gender and region.\n3.5 Employment Analysis (PLFS)\nObjective:\nTo study labour market conditions.\nActivity:\nAnalysed unemployment rate, employment distribution, urban-rural comparisons.\nFindings:\nUrban unemployment remains higher.\nSelf-employment is a major source of livelihood in rural areas.\n4."
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 4,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "Combined CMIE + CSO Analysis\nBoth datasets were used together for deeper insights.\n4.1 GDP vs Sectoral Performance\nCSO GDP data combined with CMIE industry data shows services as the main growth driver."
  },
  {
    "book_id": "72023c19-c86a-40e9-89d0-471949f22999",
    "book_title": "CMIE OR CSO.pdf",
    "page": 5,
    "source": "CMIE OR CSO.pdf",
    "chunk_text": "4.2 Inflation vs Company Costs\nCSO CPI/WPI data compared with CMIE Prowess cost structure:\nRising input prices impact automotive and FMCG margins.\n4.3 Employment (PLFS vs CPHS)\nCSO PLFS employment rate compared with CMIE CPHS shows differing patterns due to\nmethodology differences.\n5. Conclusion\nCMIE and CSO are two of the most important sources of economic data in India.\nCMIE is more micro-level (companies, industries, households).\nCSO provides macro-level, official national statistics.\nTogether, they help create a complete understanding of the Indian economy—covering\nproduction, prices, employment, investment, and industry trends.\nSuch data-driven analysis supports business decisions, government policy-making, and\nacademic research."
  },
  {
    "book_id": "1b5a1f0e-0f37-4a2e-b976-e2d0f04bdf30",
    "book_title": "sample.pdf",
    "page": 2,
    "source": "sample.pdf",
    "chunk_text": "Shine Healthcare Hackathon 2025\nDescribe \nthe \npain \npoint \nyou \nare\nalleviating\nShow how your customers are impacted\nby the problem\nGive data or facts supporting your\nproblem statement, if needed\nDeliver What’s Your Solution ? \nHow you will solve this Problem ?\nProblem and Solution"
  },
  {
    "book_id": "1b5a1f0e-0f37-4a2e-b976-e2d0f04bdf30",
    "book_title": "sample.pdf",
    "page": 3,
    "source": "sample.pdf",
    "chunk_text": "Overview of your product/service\nDescribe how the solution can address the problems that you have outlined\nIf possible, use real life examples to detail the solution to the problem \nYour value to the customers\nYour Unique Selling Point (USP)\nAny Patents/IP you have\nStage of your product/service (ideation, validation, early traction, scaling, etc.)\nImages, schematics, flowcharts and diagrams\nShine Healthcare Hackathon 2025\nExplain your Product / Service"
  },
  {
    "book_id": "1b5a1f0e-0f37-4a2e-b976-e2d0f04bdf30",
    "book_title": "sample.pdf",
    "page": 4,
    "source": "sample.pdf",
    "chunk_text": "What is the size of your market? \nCapture data points from credible sources and\nmention them as sources on your presentation\nCarry out your own guesstimation exercise, if needed \nYour ideal customer and niche\nSpecify how are/will you approach your target\ncustomer with your product/service\nHighlight the uniqueness of your strategy when\ncompared to your competitors\nShine Healthcare Hackathon 2025\nTARGET MARKET\nAdult\nWomen\nTeenager"
  },
  {
    "book_id": "1b5a1f0e-0f37-4a2e-b976-e2d0f04bdf30",
    "book_title": "sample.pdf",
    "page": 5,
    "source": "sample.pdf",
    "chunk_text": "Shine Healthcare Hackathon 2025\nSpecify a framework for generating financial income\nIdentify different sources and channels of revenue\nSpecify the pricing model being used and the price\nbeing charged to the target customer.\nMention pricing slabs for different customer segments\nCompare your pricing with that of competitors, if needed\nRevenue"
  },
  {
    "book_id": "1b5a1f0e-0f37-4a2e-b976-e2d0f04bdf30",
    "book_title": "sample.pdf",
    "page": 6,
    "source": "sample.pdf",
    "chunk_text": "Shine Healthcare Hackathon 2025\nNeil Tran\nTeam Founder\nB.Tech (Agri)\n8+ years Experience\nOur Great Team\nNeil Tran\nTeam Founder\nB.Tech (Agri)\n8+ years Experience\nNeil Tran\nTeam Founder\nB.Tech (Agri)\n8+ years Experience"
  },
  {
    "book_id": "1b5a1f0e-0f37-4a2e-b976-e2d0f04bdf30",
    "book_title": "sample.pdf",
    "page": 7,
    "source": "sample.pdf",
    "chunk_text": "Quantum of fund required\nType of fund required: Equity/Debt\nInstalment-wise Fund requirement\nBreakup of fund deployment: Fund allocation to various activities such\nas Marketing, Team, Product, etc. with a timeline of expenses\nShine Healthcare Hackathon 2025\nFund Ask"
  },
  {
    "book_id": "1b5a1f0e-0f37-4a2e-b976-e2d0f04bdf30",
    "book_title": "sample.pdf",
    "page": 8,
    "source": "sample.pdf",
    "chunk_text": "Shine Healthcare Hackathon 2025\n(Mention in Rupees)\nFund Utilization"
  },
  {
    "book_id": "1b5a1f0e-0f37-4a2e-b976-e2d0f04bdf30",
    "book_title": "sample.pdf",
    "page": 9,
    "source": "sample.pdf",
    "chunk_text": "Pitch Deck Template\nShine Healthcare Hackathon 2025\nThank You"
  },
  {
    "book_id": "68a901c2-f914-456f-9ff7-af006db20f76",
    "book_title": "sample.pdf",
    "page": 2,
    "source": "sample.pdf",
    "chunk_text": "Shine Healthcare Hackathon 2025\nDescribe \nthe \npain \npoint \nyou \nare\nalleviating\nShow how your customers are impacted\nby the problem\nGive data or facts supporting your\nproblem statement, if needed\nDeliver What’s Your Solution ? \nHow you will solve this Problem ?\nProblem and Solution"
  },
  {
    "book_id": "68a901c2-f914-456f-9ff7-af006db20f76",
    "book_title": "sample.pdf",
    "page": 3,
    "source": "sample.pdf",
    "chunk_text": "Overview of your product/service\nDescribe how the solution can address the problems that you have outlined\nIf possible, use real life examples to detail the solution to the problem \nYour value to the customers\nYour Unique Selling Point (USP)\nAny Patents/IP you have\nStage of your product/service (ideation, validation, early traction, scaling, etc.)\nImages, schematics, flowcharts and diagrams\nShine Healthcare Hackathon 2025\nExplain your Product / Service"
  },
  {
    "book_id": "68a901c2-f914-456f-9ff7-af006db20f76",
    "book_title": "sample.pdf",
    "page": 4,
    "source": "sample.pdf",
    "chunk_text": "What is the size of your market? \nCapture data points from credible sources and\nmention them as sources on your presentation\nCarry out your own guesstimation exercise, if needed \nYour ideal customer and niche\nSpecify how are/will you approach your target\ncustomer with your product/service\nHighlight the uniqueness of your strategy when\ncompared to your competitors\nShine Healthcare Hackathon 2025\nTARGET MARKET\nAdult\nWomen\nTeenager"
  },
  {
    "book_id": "68a901c2-f914-456f-9ff7-af006db20f76",
    "book_title": "sample.pdf",
    "page": 5,
    "source": "sample.pdf",
    "chunk_text": "Shine Healthcare Hackathon 2025\nSpecify a framework for generating financial income\nIdentify different sources and channels of revenue\nSpecify the pricing model being used and the price\nbeing charged to the target customer.\nMention pricing slabs for different customer segments\nCompare your pricing with that of competitors, if needed\nRevenue"
  },
  {
    "book_id": "68a901c2-f914-456f-9ff7-af006db20f76",
    "book_title": "sample.pdf",
    "page": 6,
    "source": "sample.pdf",
    "chunk_text": "Shine Healthcare Hackathon 2025\nNeil Tran\nTeam Founder\nB.Tech (Agri)\n8+ years Experience\nOur Great Team\nNeil Tran\nTeam Founder\nB.Tech (Agri)\n8+ years Experience\nNeil Tran\nTeam Founder\nB.Tech (Agri)\n8+ years Experience"
  },
  {
    "book_id": "68a901c2-f914-456f-9ff7-af006db20f76",
    "book_title": "sample.pdf",
    "page": 7,
    "source": "sample.pdf",
    "chunk_text": "Quantum of fund required\nType of fund required: Equity/Debt\nInstalment-wise Fund requirement\nBreakup of fund deployment: Fund allocation to various activities such\nas Marketing, Team, Product, etc. with a timeline of expenses\nShine Healthcare Hackathon 2025\nFund Ask"
  },
  {
    "book_id": "68a901c2-f914-456f-9ff7-af006db20f76",
    "book_title": "sample.pdf",
    "page": 8,
    "source": "sample.pdf",
    "chunk_text": "Shine Healthcare Hackathon 2025\n(Mention in Rupees)\nFund Utilization"
  },
  {
    "book_id": "68a901c2-f914-456f-9ff7-af006db20f76",
    "book_title": "sample.pdf",
    "page": 9,
    "source": "sample.pdf",
    "chunk_text": "Pitch Deck Template\nShine Healthcare Hackathon 2025\nThank You"
  },
  {
    "book_id": "22f890ab-c22b-44ab-abb3-94f3e35a4746",
    "book_title": "Screenshot_20251125_080602.jpg",
    "page": 1,
    "source": "Screenshot_20251125_080602.jpg",
    "chunk_text": "Apply Huffman algorithm for the following intensity distribution for a 64 X 64 image, Obtain the coding efficiency,\n10 = 1008, r1 = 320, 12 = 456, 13 = 686, r4 = 803,15 = 105, 16 = 417 and 17 = 301"
  },
  {
    "book_id": "22f890ab-c22b-44ab-abb3-94f3e35a4746",
    "book_title": "Screenshot_20251125_080602.jpg",
    "page": 1,
    "source": "Screenshot_20251125_080602.jpg",
    "chunk_text": "Encode the word a} a2 23 and ag using arithmetic code and generate the tag for the given symbol with probabilities.\nay = 0.2, ap = 0.2, a3 = 04, and ay = 0.2"
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 1,
    "source": "DIP-QR.pdf",
    "chunk_text": "1 \n \nQA101 Write any four applications of DIP. \nQA102 Compare the difference between Cones and Rods \nQA103 List the applications of color models \nQA104 Define Photopic vision in human vision system \nQA105 Relate the difference between Regions and Boundaries \nQA106 Define 4 and 8 neighbors of a pixel \nQA107 What is the purpose of using Fourier Transform in image processing? \nQA108 Find the number of bits required to store a 256 X 256 image with 32 gray levels \nQA201 List various Gray level Transformation techniques \nQA202 Specify the objective & categories of image enhancement technique \nQA203 Compare the difference between Smoothing and Sharpening filters \nQA204 What is meant by bit plane slicing? \nQA205 Define the term spatial filtering?"
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 1,
    "source": "DIP-QR.pdf",
    "chunk_text": "QA206 What is the purpose of using homomorphic filtering in image processing? \nQA207 What are the characteristics of Butterworth filters in image processing? \nQA208 How does histogram equalization improve the quality of a color image? \nQA301 Compare and contrast gradient and Laplacian operators in edge detection. \nQA302 Define thresholding and mention its types. \nQA303 Classify the types of edges in the digital image. \nQA304 Compare and contrast gradient and Laplacian operators in edgedetection. \nQA305 What is region-based segmentation?Name one advantage of region- based segmentation"
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 2,
    "source": "DIP-QR.pdf",
    "chunk_text": "2 \n \nQA306 Define global thresholding in the segmentation of images. \nQA307 Compare Prewitt and Sobel operators in edge detection. \nQA308 What is the gradient vector of an image. \nQA401 Justify how does the structuring element's shape affect erosion and dilation? \nQA402 State how does erosion affect object boundaries? \nQA403 What is the Hit-or-Miss Transform used for? Which operation forms the basis of Hit-orMiss transform? \nQA404 Differentiate the erosion and dilation in morphological image processing. \nQA405 Summarize all the mathematical operations are used in erosion? \nQA406 What is morphological reconstruction? Name one application of morphological \nreconstruction."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 2,
    "source": "DIP-QR.pdf",
    "chunk_text": "QA407 Discuss all the trade-offs associated with morphological smoothing and how the choice of \nstructuring element influences the smoothing outcome? \nQA408 Define the term top-hat transformation. \nQA501 State the need for data compression and compare lossy and lossless compression techniques. \nQA502 Differentiate Coding redundancy and Interpixel redundancy. \nQA503 Evaluate whether the given Huffman code 0, 10, 01, 011 for the symbols a1, a2, a3 and a4 \nare uniquely decodable or not. \nQA504 Define the role of the DCT in the JPEG image compression standard. \nQA505 State is the significance of lossy compression through transform coding to achieve the \nacceptable image quality? \nQA506 List the various image and video compression standards."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 2,
    "source": "DIP-QR.pdf",
    "chunk_text": "QA507 Discuss the operations performed by error free compression? \nQA508 Discuss briefly on Shift codes, and interpret how do they contribute to image compression?"
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 3,
    "source": "DIP-QR.pdf",
    "chunk_text": "UNIT-2\n1 \n \n \n \n \n(a) \ni) Transform the given image by using Power law transformation. Given C = 1, r = 0.2 \n 110 120 \n90 \nF(x, y) = \n 91 94 \n98 \n 90 91 \n 99 \n(7) \nii) \nDevelop a python program with OpenCV to find the histogram of the Grayscale image \nand histogram of any one channel from a color image \n(6) \n(b) \ni) Perform Histogram Equalization of Image \n4 \n4 \n4 \n4 \n4 \n3 \n4 \n5 \n4 \n3 \n3 \n5 \n5 \n5 \n3 \n3 \n4 \n5 \n4 \n3 \n4 \n4 \n4 \n4 \n4 \n(7) \nii) Develop a python program with OpenCV to find the histogram of any one channel from a \ncolor image. Also, enhance the image using histogram equalization. \n(6) \n2 \n \n(a) \n(i)How does histogram equalization improve image quality by flattening the continuous \nintensity distribution."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 3,
    "source": "DIP-QR.pdf",
    "chunk_text": "(7) \n(ii)Write a Python program using OpenCV to perform the following image transformations: \n translation, scaling,shearing, reflection, rotation, and cropping \n(b) \ni) \nDiscuss any two image smoothing filters in the spatial domain . \n(7) \nii) \nWrite the Python code using OpenCV to implement these filters. \n(6) \n3 \n \n(a) \ni) Discuss any two image smoothing filters in the frequency domain \n(7) \nii) write the Python code using OpenCV to implement these filters. \n(6) \n(b) \nExplain Homomorphic filtering approach for image enhancement \n4 \n(a) \nDiscuss any two image sharpening filters in the spatial domain and write the Python code using \nOpenCV to implement these filters (7+6) \nUNIT-1 \n1 \n(a) \nExplain the fundamental steps involved in digital image processing in details."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 3,
    "source": "DIP-QR.pdf",
    "chunk_text": "(b) \ni)List the components of the image processing system . (6) \n(ii) Develop a python program using OpenCV to convert the color from BGR to YCrCb and \ndisplay the ‘Cb’ plane.(7) \n2 \n(a) \nIllustrate how the Image is digitalized by sampling and Quantization process \n(b) \ni) Develop a python program using OpenCV to convert the color from BGR to RGB. \nSplit the RGB into R, G and B planes and merge them. \n ii) Convert RGB (29, 104, 215) Values to HSI Values. \n3 \n(a) \nExplain the basic relationships between pixels such as neighborhood, adjacency and distance \nmeasures \n(b) \nExplain the properties of 2D Fourier Transform."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 3,
    "source": "DIP-QR.pdf",
    "chunk_text": "4 \n(a) \nExplain about three sensor arrangement for image sensing and acquisition \n(b) \nWrite a Python program using OpenCV to perform the following webcam operations: \ni) Save the five frames as a JPG file. (4) \nii) Display the video stream. (2) \niii) Display the video stream with a resized window. (2) \niv)Rotate and display the video stream? (5)"
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 4,
    "source": "DIP-QR.pdf",
    "chunk_text": "(b) \nDiscuss any two image sharpening filters in the frequency domain and write the Python code \nusing OpenCV to implement these filters (7+6) \nUNIT-3 \n1 \n(a) \nExplain how image segmentation algorithms are categorized. Discuss how the point detection \nalgorithm works \n(b) \nExplain the concept of Robert Operator (Gradient) &Prewitt operator \n2 \n \n \n(a) \nExplain the concept of edge linking using HoughTransform \n(b) \ni) Discuss the edge detection in an image using Sobel Operator. \n(7) \nii) Develop a Python program with OpenCV to detect the edges using the Sobel edge detection \nmethod \n3 \n \n(a) \nExplain the details about segmentation techniques based on similarity using region growing, \nregional splitting and merge strategy."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 4,
    "source": "DIP-QR.pdf",
    "chunk_text": "(b) \nAnalyze the segmentation method by Morphological watershed method with dam construction in \ndetail. \n \n4 \n \n(a) \ni) Develop a python program with OpenCV to detect the lines by finding the edges using a \ncanny edge detector and link the edges using Hough transform. \nii) Develop a python program with OpenCV to segment the images using the Global \nThresholding, Adaptive Thresholding method and Otsu’s method."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 4,
    "source": "DIP-QR.pdf",
    "chunk_text": "a) \nRead the Image and convert to grayscale \nb) \nUse Global thresholding to segment the image \nc) \nUse Adaptive thresholding to segment the image \nd) \nUse Otsu's method to segment the image \n(b) \nExplain in detail about the Marr-Hildreth edge detection \nusing in image segmentation with the necessary equation \n \nUNIT-4 \n1 \n(a) \ni) Apply the morphological operations of Dilation and Erosion in an image in detail.(7) \n(ii)Using OpenCV develop a Python code to apply the morphological operation Erosion in the \n text. (6) \n(b) \ni) Apply Hit-or-Miss Transformation in an image and discuss the results. (7) \nii) Using OpenCV develop a Python code to apply the morphological operation Dilation in \nthe text."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 4,
    "source": "DIP-QR.pdf",
    "chunk_text": "(6) \n \n \n2 \n(a) \nExplain Gray level morphology with \ni) Erosion and Dilation by Flat and non-flat Structuring.(7) \nii) Gray-scale Opening and Closing.(6) \n(b) \nExplain in detail about morphological reconstruction by Erosion and Dilation with geodesic \ndilation & geodesic erosion \n \n \n3 \n(a) \nUsing open CV develop a Python code to create your name and apply the following \nmorphological operation in text \ni) \nErosion and Dilation \nii) \nii) Opening and closing \n(b) \nAnalyze the technique for linking edges to detect the lines by converting the (x, y) plane to the \nparameter space (ρ, θ)."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 5,
    "source": "DIP-QR.pdf",
    "chunk_text": "4 \n(a) \nExplain in detail about non flat structural element and flat structural element operation in \ngrayscale morphology. \n(b) \nExplain in detail the top-hat and bottom-hat transformations in grayscale morphology \nUNIT-5 \n \n1 \n \n(a) \n \nA source emits letters from an alphabet A = (a1, a2, a3, a4, a5) with probabilities P(a1) = 0.3, \nP(a2) = 0.4, P(a3) = 0.15, P(a4) = 0.05 and P(a5) = 0.1. \na) \nFind a Huffman code for this source? \nb) \nFind the average length of the code and its redundancy? \nc) \nFind the Efficiency of the Code? \n(b) \ni) Describe run-length encoding with examples \nii) Explain image compression standards in detail."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 5,
    "source": "DIP-QR.pdf",
    "chunk_text": "2 \n \n \n \n(a) \nGenerate the tag for the sequence 1 3 2 1 using arithmetic \ncoding for the probabilities \nP (1) = 0.8, P (2) = 0.02, and P(3) = 0.18 \n(b) \ni) Design a coder which a source emits letters from an alphabet A= {k1, k2, k3, k4, k5} \nwith probabilities P(k1) =p(k3) = 0.2, P(k2) =0.4, P(k4) = P(k5) = 0.1 \na) \nFind a Huffman code for this source? \nb) \nFind the average length of the code and its redundancy? \nc) \nFind the efficiency of the code? \n3 \n(a) \nWhat is Run-Length Coding (RLE), and how does it function as a basic image compression \ntechnique with examples along with advantages and limitations \n(b) \ni) Discuss various types image compression methods (8) \nii) Mention the significance of the discrete cosine transform (DCT) in JPEG compression."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 5,
    "source": "DIP-QR.pdf",
    "chunk_text": "and \nwhat role does it play in the encoding process? (5) \n4 \n(a) \nExplain in detail about data compression, including its types, methods, and the need for it. \n(b) \nWith a neat block diagram, explain the transform-based image compression scheme using the \nJPEG compression \nstandard."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 7,
    "source": "DIP-QR.pdf",
    "chunk_text": "PART-C \n1 \n(a) Elaborate the types of colour models in detail and illustrate \nthe colour conversion with an example using RGB to CMY the value (35, 98, 156). \n(b) Analyze how the image formation takes place in the eye and \nanalyze the brightness adaptation and discrimination of the human eye \n2 \n \n \n \n \n \n(a) \nExplain in detail: \ni) Basic Intensity transformations \nii) Piecewise linear transformation \n(b) Perform histogram Equalization on a 3-bit image of size 64 \nx 64 pixels. The intensity distribution of the image is given below \nGray level \nvalue rk \n0 \n1 \n2 \n3 \n4 \n5 \n6 \n7 \nNo. of \nPixels nk \n790 1023 850 656 329 245 122 \n081 \n \n3 \n(a) ) Define Edge. Explain the procedure of extracting edge using ‘Canny’ edge detector."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 7,
    "source": "DIP-QR.pdf",
    "chunk_text": "ii) Develop a python program with OpenCV to detect the lines by finding the edges using a \ncanny edge detector and link the edges using Hough transform \n(b) i) Explain the Otsu Thresholding Algorithm in Image \nProcessing. \nii) Discuss the Challenges and Considerations in Implementing Otsu Thresholding for \nNoisy Images. \n \n \n4 \n(a) Explain Gray level morphology with the following steps in \ndetail \na) \nMorphological smoothing \nb) \nMorphological Gradient \nc) \nTop-hat and Bottom-hat Transformations. \n(b) Discuss the trade-offs involved in selecting kernel sizes and iterations for erosion, dilation, \nopening, and closing \noperations in the context of medical image segmentation \n5 \n(a) Apply Huffman algorithm for the following intensity distribution for a 64 X 64 image."
  },
  {
    "book_id": "f7ecf506-9e1a-4f56-a1c7-3fa46eef590e",
    "book_title": "DIP-QR.pdf",
    "page": 7,
    "source": "DIP-QR.pdf",
    "chunk_text": "Obtain the \ncoding efficiency. \nr0 = 1008, r1 = 320, r2 = 456, r3 = 686, r4 = 803, r5 = 105, r6 =417 and r7 = 301. \n \n(b) Encode the word a1 a2 a3 and a4 using arithmetic code and generate the tag for the given symbol \nwith probabilities. \na1 = 0.2, a2 = 0.2, a3 = 0.4, and a4 = 0.2"
  },
  {
    "book_id": "b0ee12df-8d75-42f9-b09b-a424fd4eab5a",
    "book_title": "Screenshot 2025-11-16 123525.png",
    "page": 1,
    "source": "Screenshot 2025-11-16 123525.png",
    "chunk_text": "constraint- determine the independence relationships\nbased between variables. The structure is built\nmethods by testing conditional independence and"
  },
  {
    "book_id": "b0ee12df-8d75-42f9-b09b-a424fd4eab5a",
    "book_title": "Screenshot 2025-11-16 123525.png",
    "page": 1,
    "source": "Screenshot 2025-11-16 123525.png",
    "chunk_text": "score- structures based on how well they explain\nbased the data. Common scoring methods include\nmethods Bayesian Information Criterion (BIC) and"
  },
  {
    "book_id": "b0ee12df-8d75-42f9-b09b-a424fd4eab5a",
    "book_title": "Screenshot 2025-11-16 123525.png",
    "page": 1,
    "source": "Screenshot 2025-11-16 123525.png",
    "chunk_text": "expectation- learning in the presence of hidden\nmaximization variables. The EM algorithm consists of\n(em) algorithm two steps: E-step (Expectation) and Mstep (Maximization)"
  },
  {
    "book_id": "b0ee12df-8d75-42f9-b09b-a424fd4eab5a",
    "book_title": "Screenshot 2025-11-16 123525.png",
    "page": 1,
    "source": "Screenshot 2025-11-16 123525.png",
    "chunk_text": "maximum This approach estimates parameters by\nlikelihood maximizing the likelihood of the observed\nestimation data. However, with hidden variables,\n(mle) direct computation can be challenging."
  },
  {
    "book_id": "b0ee12df-8d75-42f9-b09b-a424fd4eab5a",
    "book_title": "Screenshot 2025-11-16 123525.png",
    "page": 1,
    "source": "Screenshot 2025-11-16 123525.png",
    "chunk_text": "struct\nleoning structure itself, which includes identifying\n9 which variables are connected and how.\nhis involves estimating the conditional\nparameter probability distributions (CPDs) for the\nlearning observed and hidden variables given the"
  },
  {
    "book_id": "5b28d139-8dc3-4e60-956b-1dde995c38aa",
    "book_title": "Screenshot 2025-11-29 231942.png",
    "page": 1,
    "source": "Screenshot 2025-11-29 231942.png",
    "chunk_text": "Vertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "5b28d139-8dc3-4e60-956b-1dde995c38aa",
    "book_title": "Screenshot 2025-11-29 231942.png",
    "page": 1,
    "source": "Screenshot 2025-11-29 231942.png",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data\nproduced needs to be compared to others within industry and close competitors. The company\nshould also consider their past experience and how it corresponds to current and future\nperformance expectations. Three common analysis tools are used for decision-making; horizontal\nanalysis, vertical analysis, and financial ratios.\nFor our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a\nmerchandising company that sells a variety of products. The image below shows the comparative\nincome statements and balance sheets for the past two years.\nFigure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are\nsimplified for our calculations and do not fully represent all the accounts a company could maintain.\nLet’s begin our analysis discussion by looking at horizontal analysis.\nHorizontal Analysis\nHorizontal analysis (also known as trend analysis) looks at trends over time on various financial\nstatement line items. A company will look at one period (usually a year) and compare it to another"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Using Banyan Goods as our example, if Banyan wanted to compare net sales in the current year\n(year of analysis) of $120,000 to the prior year (base year) of $100,000, the dollar change would be\nas follows:\nThe percentage change is found by taking the dollar change, dividing by the base year amount, and\nthen multiplying by 100.\nLet’s compute the percentage change for Banyan Goods’ net sales.\nThis means Banyan Goods saw an increase of $20,000 in net sales in the current year as compared\nto the prior year, which was a 20% increase. The same dollar change and percentage change\ncalculations would be used for the income statement line items as well as the balance sheet line\nitems. The image below shows the complete horizontal analysis of the income statement and\nbalance sheet for Banyan Goods."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Dollar change = $120,000– $1000,000 = $20,000\nPercentage change = (\n) × 100 = 20%\n$20,000\n$100,000"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Vertical Analysis\nVertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments.\nThe company will need to determine which line item they are comparing all items to within that\nstatement and then calculate the percentage makeup. These percentages are considered commonsize because they make businesses within industry comparable by taking out fluctuations for size. It\nis typical for an income statement to use net sales (or sales) as the comparison line item. This\nmeans net sales will be set at 100% and all other line items within the income statement will\nrepresent a percentage of net sales."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula to determine the common-size\npercentage is:\nFor example, if Banyan Goods set total assets as the base amount and wanted to see what\npercentage of total assets were made up of cash in the current year, the following calculation would\noccur.\nCash in the current year is $110,000 and total assets equal $250,000, giving a common-size\npercentage of 44%. If the company had an expected cash balance of 40% of total assets, they\nwould be exceeding expectations. This may not be enough of a difference to make a change, but if\nthey notice this deviates from industry standards, they may need to make adjustments, such as\nreducing the amount of cash on hand to reinvest in the business."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The image below shows the\ncommon-size calculations on the comparative income statements and comparative balance sheets\nfor Banyan Goods.\nFigure A3 Income Statements and Vertical Analysis.\nCommon-size percentage = (\n) × 100 = 44%\n$110,000\n$250,000"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A stakeholder could be looking to invest, become a supplier, make a loan, or alter\ninternal operations, among other things, based in part on the outcomes of ratio analysis. The\ninformation resulting from ratio analysis can be used to examine trends in performance, establish\nbenchmarks for success, set budget expectations, and compare industry competitors. There are four\nmain categories of ratios: liquidity, solvency, efficiency, and profitability. Note that while there are\nmore ideal outcomes for some ratios, the industry in which the business operates can change the\ninfluence each of these outcomes has over stakeholder decisions."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "(You will learn more about ratios,\nindustry standards, and ratio interpretation in advanced accounting courses.)\nLiquidity Ratios\nLiquidity ratios show the ability of the company to pay short-term obligations if they came due\nimmediately with assets that can be quickly converted to cash. This is done by comparing current\nassets to current liabilities. Lenders, for example, may consider the outcomes of liquidity ratios when\ndeciding whether to extend a loan to a company. A company would like to be liquid enough to\nmanage any currently due obligations but not too liquid where they may not be effectively investing\nin growth opportunities. Three common liquidity measurements are working capital, current ratio,\nand quick ratio."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working Capital\nWorking capital measures the financial health of an organization in the short-term by finding the\ndifference between current assets and current liabilities. A company will need enough current assets\nto cover current liabilities; otherwise, they may not be able to continue operations in the future.\nBefore a lender extends credit, they will review the working capital of the company to see if the\ncompany can meet their obligations. A larger difference signals that a company can cover their\nshort-term debts and a lender may be more willing to extend the loan. On the other hand, too large\nof a difference may indicate that the company may not be correctly using their assets to grow the\nbusiness."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula for working capital is:\nUsing Banyan Goods, working capital is computed as follows for the current year:\nIn this case, current assets were $200,000, and current liabilities were $100,000. Current assets\nwere far greater than current liabilities for Banyan Goods and they would easily be able to cover\nshort-term debt.\nThe dollar value of the difference for working capital is limited given company size and scope. It is\nmost useful to convert this information to a ratio to determine the company’s current financial health.\nThis ratio is the current ratio.\nCurrent Ratio\nWorking capital = $200,000– $100,000 = $100,000"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Quick Ratio\nThe quick ratio, also known as the acid-test ratio, is similar to the current ratio except current assets\nare more narrowly defined as the most liquid assets, which exclude inventory and prepaid expenses.\nThe conversion of inventory and prepaid expenses to cash can sometimes take more time than the\nliquidation of other current assets. A company will want to know what they have on hand and can\nuse quickly if an immediate obligation is due. The formula for the quick ratio is:\nThe quick ratio for Banyan Goods in the current year is:\nA 1.6:1 ratio means the company has enough quick assets to cover current liabilities.\nAnother category of financial measurement uses solvency ratios."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Solvency Ratios\nSolvency implies that a company can meet its long-term obligations and will likely stay in business in\nthe future. To stay in business the company must generate more revenue than debt in the long-term.\nMeeting long-term obligations includes the ability to pay any interest incurred on long-term debt. Two\nmain solvency ratios are the debt-to-equity ratio and the times interest earned ratio.\nDebt to Equity Ratio\nThe debt-to-equity ratio shows the relationship between debt and equity as it relates to business\nfinancing. A company can take out loans, issue stock, and retain earnings to be used in future\nperiods to keep operations running. It is less risky and less costly to use equity sources for financing\nas compared to debt resources."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "This is mainly due to interest expense repayment that a loan carries\nas opposed to equity, which does not have this requirement. Therefore, a company wants to know\nhow much debt and equity contribute to its financing. Ideally, a company would prefer more equity\nthan debt financing. The formula for the debt to equity ratio is:\nCurrent ratio = (\n) = 2 or 2:1\n$200,000\n$100,000\nQuick ratio = (\n) = 1.6 or 1.6:1\n$110,000 + $20,000 + $30,000\n$100,000"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Lenders will pay attention to\nthis ratio before extending credit. The more times over a company can cover interest, the more likely\na lender will extend long-term credit. The formula for times interest earned is:\nThe information needed to compute times interest earned for Banyan Goods in the current year can\nbe found on the income statement.\nThe $43,000 is the operating income, representing earnings before interest and taxes. The 21.5\ntimes outcome suggests that Banyan Goods can easily repay interest on an outstanding loan and\ncreditors would have little risk that Banyan Goods would be unable to pay.\nAnother category of financial measurement uses efficiency ratios.\nEfficiency Ratios\nEfficiency shows how well a company uses and manages their assets."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Areas of importance with\nefficiency are management of sales, accounts receivable, and inventory. A company that is efficient\ntypically will be able to generate revenues quickly using the assets it acquires. Let’s examine four\nefficiency ratios: accounts receivable turnover, total asset turnover, inventory turnover, and days’\nsales in inventory.\nAccounts Receivable Turnover\nAccounts receivable turnover measures how many times in a period (usually a year) a company will\ncollect cash from accounts receivable. A higher number of times could mean cash is collected more\nquickly and that credit customers are of high quality. A higher number is usually preferable because\nthe cash collected can be reinvested in the business at a quicker rate."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A lower number of times\ncould mean cash is collected slowly on these accounts and customers may not be properly qualified\nto accept the debt. The formula for accounts receivable turnover is:\nDebt-to-equity ratio = (\n) = 1.5 or 1.5:1\n$150,000\n$100,000\nTimes interest earned = (\n) = 21.5 times\n$43,000\n$2,000"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Given this\noutcome, they may want to consider stricter credit lending practices to make sure credit customers\nare of a higher quality. They may also need to be more aggressive with collecting any outstanding\naccounts.\nTotal Asset Turnover\nTotal asset turnover measures the ability of a company to use their assets to generate revenues. A\ncompany would like to use as few assets as possible to generate the most net sales. Therefore, a\nhigher total asset turnover means the company is using their assets very efficiently to produce net\nsales. The formula for total asset turnover is:\nAverage total assets are found by dividing the sum of beginning and ending total assets balances\nfound on the balance sheet."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The beginning total assets balance in the current year is taken from the\nending total assets balance in the prior year.\nBanyan Goods’ total asset turnover is:\nThe outcome of 0.53 means that for every $1 of assets, $0.53 of net sales are generated. Over time,\nBanyan Goods would like to see this turnover ratio increase.\nInventory Turnover\nAverage accounts receivable\nAccounts receivable turnover\n=\n=\n= $25,000\n$20,000+$30,000\n2\n= 4 times\n$100,000\n$25,000\nAverage total assets\nTotal assets turnover\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.53 times (rounded)\n$120,000\n$225,000"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Banyan Goods’ inventory turnover is:\n1.6 times is a very low turnover rate for Banyan Goods. This may mean the company is maintaining\ntoo high an inventory supply to meet a low demand from customers. They may want to decrease\ntheir on-hand inventory to free up more liquid assets to use in other ways.\nDays’ Sales in Inventory\nDays’ sales in inventory expresses the number of days it takes a company to turn inventory into\nsales. This assumes that no new purchase of inventory occurred within that time period. The fewer\nthe number of days, the more quickly the company can sell its inventory. The higher the number of\ndays, the longer it takes to sell its inventory. The formula for days’ sales in inventory is:\nBanyan Goods’ days’ sales in inventory is:\n243 days is a long time to sell inventory."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "While industry dictates what is an acceptable number of\ndays to sell inventory, 243 days is unsustainable long-term. Banyan Goods will need to better\nmanage their inventory and sales strategies to move inventory more quickly.\nThe last category of financial measurement examines profitability ratios.\nProfitability Ratios\nProfitability considers how well a company produces returns given their operational performance.\nThe company needs to leverage its operations to increase profit. To assist with profit goal\nAverage inventory\nInventory turnover\n=\n=\n= $37,500\n$35,000+$40,000\n2\n= 1.6 times\n$60,000\n$37,500\nDays' sales in inventory = (\n) × 365 = 243 days (rounded)\n$40,000\n$60,000"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "If Banyan Goods thinks this is too\nlow, the company would try and find ways to reduce expenses and increase sales.\nReturn on Total Assets\nThe return on total assets measures the company’s ability to use its assets successfully to generate\na profit. The higher the return (ratio outcome), the more profit is created from asset use. Average\ntotal assets are found by dividing the sum of beginning and ending total assets balances found on\nthe balance sheet. The beginning total assets balance in the current year is taken from the ending\ntotal assets balance in the prior year. The formula for return on total assets is:\nFor Banyan Goods, the return on total assets for the current year is:\nThe higher the figure, the better the company is using its assets to create a profit."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Industry standards\ncan dictate what is an acceptable return.\nReturn on Equity\nReturn on equity measures the company’s ability to use its invested capital to generate income. The\ninvested capital comes from stockholders investments in the company’s stock and its retained\nearnings and is leveraged to create profit. The higher the return, the better the company is doing at\nusing its investments to yield a profit. The formula for return on equity is:\nProfit margin = (\n) = 0.29 (rounded) or 29%\n$35,000\n$120,000\nAverage total assets\nReturn on total assets\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.16 (rounded) or 16%\n$35,000\n$225,000"
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Advantages and Disadvantages of Financial\nStatement Analysis\nThere are several advantages and disadvantages to financial statement analysis. Financial\nstatement analysis can show trends over time, which can be helpful in making future business\ndecisions. Converting information to percentages or ratios eliminates some of the disparity between\ncompetitor sizes and operating abilities, making it easier for stakeholders to make informed\ndecisions. It can assist with understanding the makeup of current operations within the business,\nand which shifts need to occur internally to increase productivity.\nA stakeholder needs to keep in mind that past performance does not always dictate future\nperformance."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Attention must be given to possible economic influences that could skew the numbers\nbeing analyzed, such as inflation or a recession. Additionally, the way a company reports information\nwithin accounts may change over time. For example, where and when certain transactions are\nrecorded may shift, which may not be readily evident in the financial statements.\nA company that wants to budget properly, control costs, increase revenues, and make long-term\nexpenditure decisions may want to use financial statement analysis to guide future operations. As\nlong as the company understands the limitations of the information provided, financial statement\nanalysis is a good tool to predict growth and company financial strength."
  },
  {
    "book_id": "4aead0a0-336f-4c57-a08f-a65ed7807823",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholder equity\nReturn on equity\n=\n=\n= $95,000\n$90,000+$100,000\n2\n= 0.37 (rounded) or 37%\n$35,000\n$95,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data\nproduced needs to be compared to others within industry and close competitors. The company\nshould also consider their past experience and how it corresponds to current and future\nperformance expectations. Three common analysis tools are used for decision-making; horizontal\nanalysis, vertical analysis, and financial ratios.\nFor our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a\nmerchandising company that sells a variety of products. The image below shows the comparative\nincome statements and balance sheets for the past two years.\nFigure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are\nsimplified for our calculations and do not fully represent all the accounts a company could maintain.\nLet’s begin our analysis discussion by looking at horizontal analysis.\nHorizontal Analysis\nHorizontal analysis (also known as trend analysis) looks at trends over time on various financial\nstatement line items. A company will look at one period (usually a year) and compare it to another"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Using Banyan Goods as our example, if Banyan wanted to compare net sales in the current year\n(year of analysis) of $120,000 to the prior year (base year) of $100,000, the dollar change would be\nas follows:\nThe percentage change is found by taking the dollar change, dividing by the base year amount, and\nthen multiplying by 100.\nLet’s compute the percentage change for Banyan Goods’ net sales.\nThis means Banyan Goods saw an increase of $20,000 in net sales in the current year as compared\nto the prior year, which was a 20% increase. The same dollar change and percentage change\ncalculations would be used for the income statement line items as well as the balance sheet line\nitems. The image below shows the complete horizontal analysis of the income statement and\nbalance sheet for Banyan Goods."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Dollar change = $120,000– $1000,000 = $20,000\nPercentage change = (\n) × 100 = 20%\n$20,000\n$100,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Vertical Analysis\nVertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments.\nThe company will need to determine which line item they are comparing all items to within that\nstatement and then calculate the percentage makeup. These percentages are considered commonsize because they make businesses within industry comparable by taking out fluctuations for size. It\nis typical for an income statement to use net sales (or sales) as the comparison line item. This\nmeans net sales will be set at 100% and all other line items within the income statement will\nrepresent a percentage of net sales."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula to determine the common-size\npercentage is:\nFor example, if Banyan Goods set total assets as the base amount and wanted to see what\npercentage of total assets were made up of cash in the current year, the following calculation would\noccur.\nCash in the current year is $110,000 and total assets equal $250,000, giving a common-size\npercentage of 44%. If the company had an expected cash balance of 40% of total assets, they\nwould be exceeding expectations. This may not be enough of a difference to make a change, but if\nthey notice this deviates from industry standards, they may need to make adjustments, such as\nreducing the amount of cash on hand to reinvest in the business."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The image below shows the\ncommon-size calculations on the comparative income statements and comparative balance sheets\nfor Banyan Goods.\nFigure A3 Income Statements and Vertical Analysis.\nCommon-size percentage = (\n) × 100 = 44%\n$110,000\n$250,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A stakeholder could be looking to invest, become a supplier, make a loan, or alter\ninternal operations, among other things, based in part on the outcomes of ratio analysis. The\ninformation resulting from ratio analysis can be used to examine trends in performance, establish\nbenchmarks for success, set budget expectations, and compare industry competitors. There are four\nmain categories of ratios: liquidity, solvency, efficiency, and profitability. Note that while there are\nmore ideal outcomes for some ratios, the industry in which the business operates can change the\ninfluence each of these outcomes has over stakeholder decisions."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "(You will learn more about ratios,\nindustry standards, and ratio interpretation in advanced accounting courses.)\nLiquidity Ratios\nLiquidity ratios show the ability of the company to pay short-term obligations if they came due\nimmediately with assets that can be quickly converted to cash. This is done by comparing current\nassets to current liabilities. Lenders, for example, may consider the outcomes of liquidity ratios when\ndeciding whether to extend a loan to a company. A company would like to be liquid enough to\nmanage any currently due obligations but not too liquid where they may not be effectively investing\nin growth opportunities. Three common liquidity measurements are working capital, current ratio,\nand quick ratio."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working Capital\nWorking capital measures the financial health of an organization in the short-term by finding the\ndifference between current assets and current liabilities. A company will need enough current assets\nto cover current liabilities; otherwise, they may not be able to continue operations in the future.\nBefore a lender extends credit, they will review the working capital of the company to see if the\ncompany can meet their obligations. A larger difference signals that a company can cover their\nshort-term debts and a lender may be more willing to extend the loan. On the other hand, too large\nof a difference may indicate that the company may not be correctly using their assets to grow the\nbusiness."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula for working capital is:\nUsing Banyan Goods, working capital is computed as follows for the current year:\nIn this case, current assets were $200,000, and current liabilities were $100,000. Current assets\nwere far greater than current liabilities for Banyan Goods and they would easily be able to cover\nshort-term debt.\nThe dollar value of the difference for working capital is limited given company size and scope. It is\nmost useful to convert this information to a ratio to determine the company’s current financial health.\nThis ratio is the current ratio.\nCurrent Ratio\nWorking capital = $200,000– $100,000 = $100,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Quick Ratio\nThe quick ratio, also known as the acid-test ratio, is similar to the current ratio except current assets\nare more narrowly defined as the most liquid assets, which exclude inventory and prepaid expenses.\nThe conversion of inventory and prepaid expenses to cash can sometimes take more time than the\nliquidation of other current assets. A company will want to know what they have on hand and can\nuse quickly if an immediate obligation is due. The formula for the quick ratio is:\nThe quick ratio for Banyan Goods in the current year is:\nA 1.6:1 ratio means the company has enough quick assets to cover current liabilities.\nAnother category of financial measurement uses solvency ratios."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Solvency Ratios\nSolvency implies that a company can meet its long-term obligations and will likely stay in business in\nthe future. To stay in business the company must generate more revenue than debt in the long-term.\nMeeting long-term obligations includes the ability to pay any interest incurred on long-term debt. Two\nmain solvency ratios are the debt-to-equity ratio and the times interest earned ratio.\nDebt to Equity Ratio\nThe debt-to-equity ratio shows the relationship between debt and equity as it relates to business\nfinancing. A company can take out loans, issue stock, and retain earnings to be used in future\nperiods to keep operations running. It is less risky and less costly to use equity sources for financing\nas compared to debt resources."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "This is mainly due to interest expense repayment that a loan carries\nas opposed to equity, which does not have this requirement. Therefore, a company wants to know\nhow much debt and equity contribute to its financing. Ideally, a company would prefer more equity\nthan debt financing. The formula for the debt to equity ratio is:\nCurrent ratio = (\n) = 2 or 2:1\n$200,000\n$100,000\nQuick ratio = (\n) = 1.6 or 1.6:1\n$110,000 + $20,000 + $30,000\n$100,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Lenders will pay attention to\nthis ratio before extending credit. The more times over a company can cover interest, the more likely\na lender will extend long-term credit. The formula for times interest earned is:\nThe information needed to compute times interest earned for Banyan Goods in the current year can\nbe found on the income statement.\nThe $43,000 is the operating income, representing earnings before interest and taxes. The 21.5\ntimes outcome suggests that Banyan Goods can easily repay interest on an outstanding loan and\ncreditors would have little risk that Banyan Goods would be unable to pay.\nAnother category of financial measurement uses efficiency ratios.\nEfficiency Ratios\nEfficiency shows how well a company uses and manages their assets."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Areas of importance with\nefficiency are management of sales, accounts receivable, and inventory. A company that is efficient\ntypically will be able to generate revenues quickly using the assets it acquires. Let’s examine four\nefficiency ratios: accounts receivable turnover, total asset turnover, inventory turnover, and days’\nsales in inventory.\nAccounts Receivable Turnover\nAccounts receivable turnover measures how many times in a period (usually a year) a company will\ncollect cash from accounts receivable. A higher number of times could mean cash is collected more\nquickly and that credit customers are of high quality. A higher number is usually preferable because\nthe cash collected can be reinvested in the business at a quicker rate."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A lower number of times\ncould mean cash is collected slowly on these accounts and customers may not be properly qualified\nto accept the debt. The formula for accounts receivable turnover is:\nDebt-to-equity ratio = (\n) = 1.5 or 1.5:1\n$150,000\n$100,000\nTimes interest earned = (\n) = 21.5 times\n$43,000\n$2,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Given this\noutcome, they may want to consider stricter credit lending practices to make sure credit customers\nare of a higher quality. They may also need to be more aggressive with collecting any outstanding\naccounts.\nTotal Asset Turnover\nTotal asset turnover measures the ability of a company to use their assets to generate revenues. A\ncompany would like to use as few assets as possible to generate the most net sales. Therefore, a\nhigher total asset turnover means the company is using their assets very efficiently to produce net\nsales. The formula for total asset turnover is:\nAverage total assets are found by dividing the sum of beginning and ending total assets balances\nfound on the balance sheet."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The beginning total assets balance in the current year is taken from the\nending total assets balance in the prior year.\nBanyan Goods’ total asset turnover is:\nThe outcome of 0.53 means that for every $1 of assets, $0.53 of net sales are generated. Over time,\nBanyan Goods would like to see this turnover ratio increase.\nInventory Turnover\nAverage accounts receivable\nAccounts receivable turnover\n=\n=\n= $25,000\n$20,000+$30,000\n2\n= 4 times\n$100,000\n$25,000\nAverage total assets\nTotal assets turnover\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.53 times (rounded)\n$120,000\n$225,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Banyan Goods’ inventory turnover is:\n1.6 times is a very low turnover rate for Banyan Goods. This may mean the company is maintaining\ntoo high an inventory supply to meet a low demand from customers. They may want to decrease\ntheir on-hand inventory to free up more liquid assets to use in other ways.\nDays’ Sales in Inventory\nDays’ sales in inventory expresses the number of days it takes a company to turn inventory into\nsales. This assumes that no new purchase of inventory occurred within that time period. The fewer\nthe number of days, the more quickly the company can sell its inventory. The higher the number of\ndays, the longer it takes to sell its inventory. The formula for days’ sales in inventory is:\nBanyan Goods’ days’ sales in inventory is:\n243 days is a long time to sell inventory."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "While industry dictates what is an acceptable number of\ndays to sell inventory, 243 days is unsustainable long-term. Banyan Goods will need to better\nmanage their inventory and sales strategies to move inventory more quickly.\nThe last category of financial measurement examines profitability ratios.\nProfitability Ratios\nProfitability considers how well a company produces returns given their operational performance.\nThe company needs to leverage its operations to increase profit. To assist with profit goal\nAverage inventory\nInventory turnover\n=\n=\n= $37,500\n$35,000+$40,000\n2\n= 1.6 times\n$60,000\n$37,500\nDays' sales in inventory = (\n) × 365 = 243 days (rounded)\n$40,000\n$60,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "If Banyan Goods thinks this is too\nlow, the company would try and find ways to reduce expenses and increase sales.\nReturn on Total Assets\nThe return on total assets measures the company’s ability to use its assets successfully to generate\na profit. The higher the return (ratio outcome), the more profit is created from asset use. Average\ntotal assets are found by dividing the sum of beginning and ending total assets balances found on\nthe balance sheet. The beginning total assets balance in the current year is taken from the ending\ntotal assets balance in the prior year. The formula for return on total assets is:\nFor Banyan Goods, the return on total assets for the current year is:\nThe higher the figure, the better the company is using its assets to create a profit."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Industry standards\ncan dictate what is an acceptable return.\nReturn on Equity\nReturn on equity measures the company’s ability to use its invested capital to generate income. The\ninvested capital comes from stockholders investments in the company’s stock and its retained\nearnings and is leveraged to create profit. The higher the return, the better the company is doing at\nusing its investments to yield a profit. The formula for return on equity is:\nProfit margin = (\n) = 0.29 (rounded) or 29%\n$35,000\n$120,000\nAverage total assets\nReturn on total assets\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.16 (rounded) or 16%\n$35,000\n$225,000"
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Advantages and Disadvantages of Financial\nStatement Analysis\nThere are several advantages and disadvantages to financial statement analysis. Financial\nstatement analysis can show trends over time, which can be helpful in making future business\ndecisions. Converting information to percentages or ratios eliminates some of the disparity between\ncompetitor sizes and operating abilities, making it easier for stakeholders to make informed\ndecisions. It can assist with understanding the makeup of current operations within the business,\nand which shifts need to occur internally to increase productivity.\nA stakeholder needs to keep in mind that past performance does not always dictate future\nperformance."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Attention must be given to possible economic influences that could skew the numbers\nbeing analyzed, such as inflation or a recession. Additionally, the way a company reports information\nwithin accounts may change over time. For example, where and when certain transactions are\nrecorded may shift, which may not be readily evident in the financial statements.\nA company that wants to budget properly, control costs, increase revenues, and make long-term\nexpenditure decisions may want to use financial statement analysis to guide future operations. As\nlong as the company understands the limitations of the information provided, financial statement\nanalysis is a good tool to predict growth and company financial strength."
  },
  {
    "book_id": "2aca4391-a8f2-4f51-a73b-4a2b9ba459da",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholder equity\nReturn on equity\n=\n=\n= $95,000\n$90,000+$100,000\n2\n= 0.37 (rounded) or 37%\n$35,000\n$95,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data\nproduced needs to be compared to others within industry and close competitors. The company\nshould also consider their past experience and how it corresponds to current and future\nperformance expectations. Three common analysis tools are used for decision-making; horizontal\nanalysis, vertical analysis, and financial ratios.\nFor our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a\nmerchandising company that sells a variety of products. The image below shows the comparative\nincome statements and balance sheets for the past two years.\nFigure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are\nsimplified for our calculations and do not fully represent all the accounts a company could maintain.\nLet’s begin our analysis discussion by looking at horizontal analysis.\nHorizontal Analysis\nHorizontal analysis (also known as trend analysis) looks at trends over time on various financial\nstatement line items. A company will look at one period (usually a year) and compare it to another"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Using Banyan Goods as our example, if Banyan wanted to compare net sales in the current year\n(year of analysis) of $120,000 to the prior year (base year) of $100,000, the dollar change would be\nas follows:\nThe percentage change is found by taking the dollar change, dividing by the base year amount, and\nthen multiplying by 100.\nLet’s compute the percentage change for Banyan Goods’ net sales.\nThis means Banyan Goods saw an increase of $20,000 in net sales in the current year as compared\nto the prior year, which was a 20% increase. The same dollar change and percentage change\ncalculations would be used for the income statement line items as well as the balance sheet line\nitems. The image below shows the complete horizontal analysis of the income statement and\nbalance sheet for Banyan Goods."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Dollar change = $120,000– $1000,000 = $20,000\nPercentage change = (\n) × 100 = 20%\n$20,000\n$100,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Vertical Analysis\nVertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments.\nThe company will need to determine which line item they are comparing all items to within that\nstatement and then calculate the percentage makeup. These percentages are considered commonsize because they make businesses within industry comparable by taking out fluctuations for size. It\nis typical for an income statement to use net sales (or sales) as the comparison line item. This\nmeans net sales will be set at 100% and all other line items within the income statement will\nrepresent a percentage of net sales."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula to determine the common-size\npercentage is:\nFor example, if Banyan Goods set total assets as the base amount and wanted to see what\npercentage of total assets were made up of cash in the current year, the following calculation would\noccur.\nCash in the current year is $110,000 and total assets equal $250,000, giving a common-size\npercentage of 44%. If the company had an expected cash balance of 40% of total assets, they\nwould be exceeding expectations. This may not be enough of a difference to make a change, but if\nthey notice this deviates from industry standards, they may need to make adjustments, such as\nreducing the amount of cash on hand to reinvest in the business."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The image below shows the\ncommon-size calculations on the comparative income statements and comparative balance sheets\nfor Banyan Goods.\nFigure A3 Income Statements and Vertical Analysis.\nCommon-size percentage = (\n) × 100 = 44%\n$110,000\n$250,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A stakeholder could be looking to invest, become a supplier, make a loan, or alter\ninternal operations, among other things, based in part on the outcomes of ratio analysis. The\ninformation resulting from ratio analysis can be used to examine trends in performance, establish\nbenchmarks for success, set budget expectations, and compare industry competitors. There are four\nmain categories of ratios: liquidity, solvency, efficiency, and profitability. Note that while there are\nmore ideal outcomes for some ratios, the industry in which the business operates can change the\ninfluence each of these outcomes has over stakeholder decisions."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "(You will learn more about ratios,\nindustry standards, and ratio interpretation in advanced accounting courses.)\nLiquidity Ratios\nLiquidity ratios show the ability of the company to pay short-term obligations if they came due\nimmediately with assets that can be quickly converted to cash. This is done by comparing current\nassets to current liabilities. Lenders, for example, may consider the outcomes of liquidity ratios when\ndeciding whether to extend a loan to a company. A company would like to be liquid enough to\nmanage any currently due obligations but not too liquid where they may not be effectively investing\nin growth opportunities. Three common liquidity measurements are working capital, current ratio,\nand quick ratio."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working Capital\nWorking capital measures the financial health of an organization in the short-term by finding the\ndifference between current assets and current liabilities. A company will need enough current assets\nto cover current liabilities; otherwise, they may not be able to continue operations in the future.\nBefore a lender extends credit, they will review the working capital of the company to see if the\ncompany can meet their obligations. A larger difference signals that a company can cover their\nshort-term debts and a lender may be more willing to extend the loan. On the other hand, too large\nof a difference may indicate that the company may not be correctly using their assets to grow the\nbusiness."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula for working capital is:\nUsing Banyan Goods, working capital is computed as follows for the current year:\nIn this case, current assets were $200,000, and current liabilities were $100,000. Current assets\nwere far greater than current liabilities for Banyan Goods and they would easily be able to cover\nshort-term debt.\nThe dollar value of the difference for working capital is limited given company size and scope. It is\nmost useful to convert this information to a ratio to determine the company’s current financial health.\nThis ratio is the current ratio.\nCurrent Ratio\nWorking capital = $200,000– $100,000 = $100,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Quick Ratio\nThe quick ratio, also known as the acid-test ratio, is similar to the current ratio except current assets\nare more narrowly defined as the most liquid assets, which exclude inventory and prepaid expenses.\nThe conversion of inventory and prepaid expenses to cash can sometimes take more time than the\nliquidation of other current assets. A company will want to know what they have on hand and can\nuse quickly if an immediate obligation is due. The formula for the quick ratio is:\nThe quick ratio for Banyan Goods in the current year is:\nA 1.6:1 ratio means the company has enough quick assets to cover current liabilities.\nAnother category of financial measurement uses solvency ratios."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Solvency Ratios\nSolvency implies that a company can meet its long-term obligations and will likely stay in business in\nthe future. To stay in business the company must generate more revenue than debt in the long-term.\nMeeting long-term obligations includes the ability to pay any interest incurred on long-term debt. Two\nmain solvency ratios are the debt-to-equity ratio and the times interest earned ratio.\nDebt to Equity Ratio\nThe debt-to-equity ratio shows the relationship between debt and equity as it relates to business\nfinancing. A company can take out loans, issue stock, and retain earnings to be used in future\nperiods to keep operations running. It is less risky and less costly to use equity sources for financing\nas compared to debt resources."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "This is mainly due to interest expense repayment that a loan carries\nas opposed to equity, which does not have this requirement. Therefore, a company wants to know\nhow much debt and equity contribute to its financing. Ideally, a company would prefer more equity\nthan debt financing. The formula for the debt to equity ratio is:\nCurrent ratio = (\n) = 2 or 2:1\n$200,000\n$100,000\nQuick ratio = (\n) = 1.6 or 1.6:1\n$110,000 + $20,000 + $30,000\n$100,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Lenders will pay attention to\nthis ratio before extending credit. The more times over a company can cover interest, the more likely\na lender will extend long-term credit. The formula for times interest earned is:\nThe information needed to compute times interest earned for Banyan Goods in the current year can\nbe found on the income statement.\nThe $43,000 is the operating income, representing earnings before interest and taxes. The 21.5\ntimes outcome suggests that Banyan Goods can easily repay interest on an outstanding loan and\ncreditors would have little risk that Banyan Goods would be unable to pay.\nAnother category of financial measurement uses efficiency ratios.\nEfficiency Ratios\nEfficiency shows how well a company uses and manages their assets."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Areas of importance with\nefficiency are management of sales, accounts receivable, and inventory. A company that is efficient\ntypically will be able to generate revenues quickly using the assets it acquires. Let’s examine four\nefficiency ratios: accounts receivable turnover, total asset turnover, inventory turnover, and days’\nsales in inventory.\nAccounts Receivable Turnover\nAccounts receivable turnover measures how many times in a period (usually a year) a company will\ncollect cash from accounts receivable. A higher number of times could mean cash is collected more\nquickly and that credit customers are of high quality. A higher number is usually preferable because\nthe cash collected can be reinvested in the business at a quicker rate."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A lower number of times\ncould mean cash is collected slowly on these accounts and customers may not be properly qualified\nto accept the debt. The formula for accounts receivable turnover is:\nDebt-to-equity ratio = (\n) = 1.5 or 1.5:1\n$150,000\n$100,000\nTimes interest earned = (\n) = 21.5 times\n$43,000\n$2,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Given this\noutcome, they may want to consider stricter credit lending practices to make sure credit customers\nare of a higher quality. They may also need to be more aggressive with collecting any outstanding\naccounts.\nTotal Asset Turnover\nTotal asset turnover measures the ability of a company to use their assets to generate revenues. A\ncompany would like to use as few assets as possible to generate the most net sales. Therefore, a\nhigher total asset turnover means the company is using their assets very efficiently to produce net\nsales. The formula for total asset turnover is:\nAverage total assets are found by dividing the sum of beginning and ending total assets balances\nfound on the balance sheet."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The beginning total assets balance in the current year is taken from the\nending total assets balance in the prior year.\nBanyan Goods’ total asset turnover is:\nThe outcome of 0.53 means that for every $1 of assets, $0.53 of net sales are generated. Over time,\nBanyan Goods would like to see this turnover ratio increase.\nInventory Turnover\nAverage accounts receivable\nAccounts receivable turnover\n=\n=\n= $25,000\n$20,000+$30,000\n2\n= 4 times\n$100,000\n$25,000\nAverage total assets\nTotal assets turnover\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.53 times (rounded)\n$120,000\n$225,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Banyan Goods’ inventory turnover is:\n1.6 times is a very low turnover rate for Banyan Goods. This may mean the company is maintaining\ntoo high an inventory supply to meet a low demand from customers. They may want to decrease\ntheir on-hand inventory to free up more liquid assets to use in other ways.\nDays’ Sales in Inventory\nDays’ sales in inventory expresses the number of days it takes a company to turn inventory into\nsales. This assumes that no new purchase of inventory occurred within that time period. The fewer\nthe number of days, the more quickly the company can sell its inventory. The higher the number of\ndays, the longer it takes to sell its inventory. The formula for days’ sales in inventory is:\nBanyan Goods’ days’ sales in inventory is:\n243 days is a long time to sell inventory."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "While industry dictates what is an acceptable number of\ndays to sell inventory, 243 days is unsustainable long-term. Banyan Goods will need to better\nmanage their inventory and sales strategies to move inventory more quickly.\nThe last category of financial measurement examines profitability ratios.\nProfitability Ratios\nProfitability considers how well a company produces returns given their operational performance.\nThe company needs to leverage its operations to increase profit. To assist with profit goal\nAverage inventory\nInventory turnover\n=\n=\n= $37,500\n$35,000+$40,000\n2\n= 1.6 times\n$60,000\n$37,500\nDays' sales in inventory = (\n) × 365 = 243 days (rounded)\n$40,000\n$60,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "If Banyan Goods thinks this is too\nlow, the company would try and find ways to reduce expenses and increase sales.\nReturn on Total Assets\nThe return on total assets measures the company’s ability to use its assets successfully to generate\na profit. The higher the return (ratio outcome), the more profit is created from asset use. Average\ntotal assets are found by dividing the sum of beginning and ending total assets balances found on\nthe balance sheet. The beginning total assets balance in the current year is taken from the ending\ntotal assets balance in the prior year. The formula for return on total assets is:\nFor Banyan Goods, the return on total assets for the current year is:\nThe higher the figure, the better the company is using its assets to create a profit."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Industry standards\ncan dictate what is an acceptable return.\nReturn on Equity\nReturn on equity measures the company’s ability to use its invested capital to generate income. The\ninvested capital comes from stockholders investments in the company’s stock and its retained\nearnings and is leveraged to create profit. The higher the return, the better the company is doing at\nusing its investments to yield a profit. The formula for return on equity is:\nProfit margin = (\n) = 0.29 (rounded) or 29%\n$35,000\n$120,000\nAverage total assets\nReturn on total assets\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.16 (rounded) or 16%\n$35,000\n$225,000"
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Advantages and Disadvantages of Financial\nStatement Analysis\nThere are several advantages and disadvantages to financial statement analysis. Financial\nstatement analysis can show trends over time, which can be helpful in making future business\ndecisions. Converting information to percentages or ratios eliminates some of the disparity between\ncompetitor sizes and operating abilities, making it easier for stakeholders to make informed\ndecisions. It can assist with understanding the makeup of current operations within the business,\nand which shifts need to occur internally to increase productivity.\nA stakeholder needs to keep in mind that past performance does not always dictate future\nperformance."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Attention must be given to possible economic influences that could skew the numbers\nbeing analyzed, such as inflation or a recession. Additionally, the way a company reports information\nwithin accounts may change over time. For example, where and when certain transactions are\nrecorded may shift, which may not be readily evident in the financial statements.\nA company that wants to budget properly, control costs, increase revenues, and make long-term\nexpenditure decisions may want to use financial statement analysis to guide future operations. As\nlong as the company understands the limitations of the information provided, financial statement\nanalysis is a good tool to predict growth and company financial strength."
  },
  {
    "book_id": "43ed74c4-221d-41d6-ae18-8db315135edd",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholder equity\nReturn on equity\n=\n=\n= $95,000\n$90,000+$100,000\n2\n= 0.37 (rounded) or 37%\n$35,000\n$95,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data\nproduced needs to be compared to others within industry and close competitors. The company\nshould also consider their past experience and how it corresponds to current and future\nperformance expectations. Three common analysis tools are used for decision-making; horizontal\nanalysis, vertical analysis, and financial ratios.\nFor our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a\nmerchandising company that sells a variety of products. The image below shows the comparative\nincome statements and balance sheets for the past two years.\nFigure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are\nsimplified for our calculations and do not fully represent all the accounts a company could maintain.\nLet’s begin our analysis discussion by looking at horizontal analysis.\nHorizontal Analysis\nHorizontal analysis (also known as trend analysis) looks at trends over time on various financial\nstatement line items. A company will look at one period (usually a year) and compare it to another"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Using Banyan Goods as our example, if Banyan wanted to compare net sales in the current year\n(year of analysis) of $120,000 to the prior year (base year) of $100,000, the dollar change would be\nas follows:\nThe percentage change is found by taking the dollar change, dividing by the base year amount, and\nthen multiplying by 100.\nLet’s compute the percentage change for Banyan Goods’ net sales.\nThis means Banyan Goods saw an increase of $20,000 in net sales in the current year as compared\nto the prior year, which was a 20% increase. The same dollar change and percentage change\ncalculations would be used for the income statement line items as well as the balance sheet line\nitems. The image below shows the complete horizontal analysis of the income statement and\nbalance sheet for Banyan Goods."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Dollar change = $120,000– $1000,000 = $20,000\nPercentage change = (\n) × 100 = 20%\n$20,000\n$100,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Vertical Analysis\nVertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments.\nThe company will need to determine which line item they are comparing all items to within that\nstatement and then calculate the percentage makeup. These percentages are considered commonsize because they make businesses within industry comparable by taking out fluctuations for size. It\nis typical for an income statement to use net sales (or sales) as the comparison line item. This\nmeans net sales will be set at 100% and all other line items within the income statement will\nrepresent a percentage of net sales."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula to determine the common-size\npercentage is:\nFor example, if Banyan Goods set total assets as the base amount and wanted to see what\npercentage of total assets were made up of cash in the current year, the following calculation would\noccur.\nCash in the current year is $110,000 and total assets equal $250,000, giving a common-size\npercentage of 44%. If the company had an expected cash balance of 40% of total assets, they\nwould be exceeding expectations. This may not be enough of a difference to make a change, but if\nthey notice this deviates from industry standards, they may need to make adjustments, such as\nreducing the amount of cash on hand to reinvest in the business."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The image below shows the\ncommon-size calculations on the comparative income statements and comparative balance sheets\nfor Banyan Goods.\nFigure A3 Income Statements and Vertical Analysis.\nCommon-size percentage = (\n) × 100 = 44%\n$110,000\n$250,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A stakeholder could be looking to invest, become a supplier, make a loan, or alter\ninternal operations, among other things, based in part on the outcomes of ratio analysis. The\ninformation resulting from ratio analysis can be used to examine trends in performance, establish\nbenchmarks for success, set budget expectations, and compare industry competitors. There are four\nmain categories of ratios: liquidity, solvency, efficiency, and profitability. Note that while there are\nmore ideal outcomes for some ratios, the industry in which the business operates can change the\ninfluence each of these outcomes has over stakeholder decisions."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "(You will learn more about ratios,\nindustry standards, and ratio interpretation in advanced accounting courses.)\nLiquidity Ratios\nLiquidity ratios show the ability of the company to pay short-term obligations if they came due\nimmediately with assets that can be quickly converted to cash. This is done by comparing current\nassets to current liabilities. Lenders, for example, may consider the outcomes of liquidity ratios when\ndeciding whether to extend a loan to a company. A company would like to be liquid enough to\nmanage any currently due obligations but not too liquid where they may not be effectively investing\nin growth opportunities. Three common liquidity measurements are working capital, current ratio,\nand quick ratio."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working Capital\nWorking capital measures the financial health of an organization in the short-term by finding the\ndifference between current assets and current liabilities. A company will need enough current assets\nto cover current liabilities; otherwise, they may not be able to continue operations in the future.\nBefore a lender extends credit, they will review the working capital of the company to see if the\ncompany can meet their obligations. A larger difference signals that a company can cover their\nshort-term debts and a lender may be more willing to extend the loan. On the other hand, too large\nof a difference may indicate that the company may not be correctly using their assets to grow the\nbusiness."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula for working capital is:\nUsing Banyan Goods, working capital is computed as follows for the current year:\nIn this case, current assets were $200,000, and current liabilities were $100,000. Current assets\nwere far greater than current liabilities for Banyan Goods and they would easily be able to cover\nshort-term debt.\nThe dollar value of the difference for working capital is limited given company size and scope. It is\nmost useful to convert this information to a ratio to determine the company’s current financial health.\nThis ratio is the current ratio.\nCurrent Ratio\nWorking capital = $200,000– $100,000 = $100,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Quick Ratio\nThe quick ratio, also known as the acid-test ratio, is similar to the current ratio except current assets\nare more narrowly defined as the most liquid assets, which exclude inventory and prepaid expenses.\nThe conversion of inventory and prepaid expenses to cash can sometimes take more time than the\nliquidation of other current assets. A company will want to know what they have on hand and can\nuse quickly if an immediate obligation is due. The formula for the quick ratio is:\nThe quick ratio for Banyan Goods in the current year is:\nA 1.6:1 ratio means the company has enough quick assets to cover current liabilities.\nAnother category of financial measurement uses solvency ratios."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Solvency Ratios\nSolvency implies that a company can meet its long-term obligations and will likely stay in business in\nthe future. To stay in business the company must generate more revenue than debt in the long-term.\nMeeting long-term obligations includes the ability to pay any interest incurred on long-term debt. Two\nmain solvency ratios are the debt-to-equity ratio and the times interest earned ratio.\nDebt to Equity Ratio\nThe debt-to-equity ratio shows the relationship between debt and equity as it relates to business\nfinancing. A company can take out loans, issue stock, and retain earnings to be used in future\nperiods to keep operations running. It is less risky and less costly to use equity sources for financing\nas compared to debt resources."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "This is mainly due to interest expense repayment that a loan carries\nas opposed to equity, which does not have this requirement. Therefore, a company wants to know\nhow much debt and equity contribute to its financing. Ideally, a company would prefer more equity\nthan debt financing. The formula for the debt to equity ratio is:\nCurrent ratio = (\n) = 2 or 2:1\n$200,000\n$100,000\nQuick ratio = (\n) = 1.6 or 1.6:1\n$110,000 + $20,000 + $30,000\n$100,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Lenders will pay attention to\nthis ratio before extending credit. The more times over a company can cover interest, the more likely\na lender will extend long-term credit. The formula for times interest earned is:\nThe information needed to compute times interest earned for Banyan Goods in the current year can\nbe found on the income statement.\nThe $43,000 is the operating income, representing earnings before interest and taxes. The 21.5\ntimes outcome suggests that Banyan Goods can easily repay interest on an outstanding loan and\ncreditors would have little risk that Banyan Goods would be unable to pay.\nAnother category of financial measurement uses efficiency ratios.\nEfficiency Ratios\nEfficiency shows how well a company uses and manages their assets."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Areas of importance with\nefficiency are management of sales, accounts receivable, and inventory. A company that is efficient\ntypically will be able to generate revenues quickly using the assets it acquires. Let’s examine four\nefficiency ratios: accounts receivable turnover, total asset turnover, inventory turnover, and days’\nsales in inventory.\nAccounts Receivable Turnover\nAccounts receivable turnover measures how many times in a period (usually a year) a company will\ncollect cash from accounts receivable. A higher number of times could mean cash is collected more\nquickly and that credit customers are of high quality. A higher number is usually preferable because\nthe cash collected can be reinvested in the business at a quicker rate."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A lower number of times\ncould mean cash is collected slowly on these accounts and customers may not be properly qualified\nto accept the debt. The formula for accounts receivable turnover is:\nDebt-to-equity ratio = (\n) = 1.5 or 1.5:1\n$150,000\n$100,000\nTimes interest earned = (\n) = 21.5 times\n$43,000\n$2,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Given this\noutcome, they may want to consider stricter credit lending practices to make sure credit customers\nare of a higher quality. They may also need to be more aggressive with collecting any outstanding\naccounts.\nTotal Asset Turnover\nTotal asset turnover measures the ability of a company to use their assets to generate revenues. A\ncompany would like to use as few assets as possible to generate the most net sales. Therefore, a\nhigher total asset turnover means the company is using their assets very efficiently to produce net\nsales. The formula for total asset turnover is:\nAverage total assets are found by dividing the sum of beginning and ending total assets balances\nfound on the balance sheet."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The beginning total assets balance in the current year is taken from the\nending total assets balance in the prior year.\nBanyan Goods’ total asset turnover is:\nThe outcome of 0.53 means that for every $1 of assets, $0.53 of net sales are generated. Over time,\nBanyan Goods would like to see this turnover ratio increase.\nInventory Turnover\nAverage accounts receivable\nAccounts receivable turnover\n=\n=\n= $25,000\n$20,000+$30,000\n2\n= 4 times\n$100,000\n$25,000\nAverage total assets\nTotal assets turnover\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.53 times (rounded)\n$120,000\n$225,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Banyan Goods’ inventory turnover is:\n1.6 times is a very low turnover rate for Banyan Goods. This may mean the company is maintaining\ntoo high an inventory supply to meet a low demand from customers. They may want to decrease\ntheir on-hand inventory to free up more liquid assets to use in other ways.\nDays’ Sales in Inventory\nDays’ sales in inventory expresses the number of days it takes a company to turn inventory into\nsales. This assumes that no new purchase of inventory occurred within that time period. The fewer\nthe number of days, the more quickly the company can sell its inventory. The higher the number of\ndays, the longer it takes to sell its inventory. The formula for days’ sales in inventory is:\nBanyan Goods’ days’ sales in inventory is:\n243 days is a long time to sell inventory."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "While industry dictates what is an acceptable number of\ndays to sell inventory, 243 days is unsustainable long-term. Banyan Goods will need to better\nmanage their inventory and sales strategies to move inventory more quickly.\nThe last category of financial measurement examines profitability ratios.\nProfitability Ratios\nProfitability considers how well a company produces returns given their operational performance.\nThe company needs to leverage its operations to increase profit. To assist with profit goal\nAverage inventory\nInventory turnover\n=\n=\n= $37,500\n$35,000+$40,000\n2\n= 1.6 times\n$60,000\n$37,500\nDays' sales in inventory = (\n) × 365 = 243 days (rounded)\n$40,000\n$60,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "If Banyan Goods thinks this is too\nlow, the company would try and find ways to reduce expenses and increase sales.\nReturn on Total Assets\nThe return on total assets measures the company’s ability to use its assets successfully to generate\na profit. The higher the return (ratio outcome), the more profit is created from asset use. Average\ntotal assets are found by dividing the sum of beginning and ending total assets balances found on\nthe balance sheet. The beginning total assets balance in the current year is taken from the ending\ntotal assets balance in the prior year. The formula for return on total assets is:\nFor Banyan Goods, the return on total assets for the current year is:\nThe higher the figure, the better the company is using its assets to create a profit."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Industry standards\ncan dictate what is an acceptable return.\nReturn on Equity\nReturn on equity measures the company’s ability to use its invested capital to generate income. The\ninvested capital comes from stockholders investments in the company’s stock and its retained\nearnings and is leveraged to create profit. The higher the return, the better the company is doing at\nusing its investments to yield a profit. The formula for return on equity is:\nProfit margin = (\n) = 0.29 (rounded) or 29%\n$35,000\n$120,000\nAverage total assets\nReturn on total assets\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.16 (rounded) or 16%\n$35,000\n$225,000"
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Advantages and Disadvantages of Financial\nStatement Analysis\nThere are several advantages and disadvantages to financial statement analysis. Financial\nstatement analysis can show trends over time, which can be helpful in making future business\ndecisions. Converting information to percentages or ratios eliminates some of the disparity between\ncompetitor sizes and operating abilities, making it easier for stakeholders to make informed\ndecisions. It can assist with understanding the makeup of current operations within the business,\nand which shifts need to occur internally to increase productivity.\nA stakeholder needs to keep in mind that past performance does not always dictate future\nperformance."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Attention must be given to possible economic influences that could skew the numbers\nbeing analyzed, such as inflation or a recession. Additionally, the way a company reports information\nwithin accounts may change over time. For example, where and when certain transactions are\nrecorded may shift, which may not be readily evident in the financial statements.\nA company that wants to budget properly, control costs, increase revenues, and make long-term\nexpenditure decisions may want to use financial statement analysis to guide future operations. As\nlong as the company understands the limitations of the information provided, financial statement\nanalysis is a good tool to predict growth and company financial strength."
  },
  {
    "book_id": "452f9bbe-9db9-4724-b613-1e2074b89109",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholder equity\nReturn on equity\n=\n=\n= $95,000\n$90,000+$100,000\n2\n= 0.37 (rounded) or 37%\n$35,000\n$95,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data\nproduced needs to be compared to others within industry and close competitors. The company\nshould also consider their past experience and how it corresponds to current and future\nperformance expectations. Three common analysis tools are used for decision-making; horizontal\nanalysis, vertical analysis, and financial ratios.\nFor our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a\nmerchandising company that sells a variety of products. The image below shows the comparative\nincome statements and balance sheets for the past two years.\nFigure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are\nsimplified for our calculations and do not fully represent all the accounts a company could maintain.\nLet’s begin our analysis discussion by looking at horizontal analysis.\nHorizontal Analysis\nHorizontal analysis (also known as trend analysis) looks at trends over time on various financial\nstatement line items. A company will look at one period (usually a year) and compare it to another"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Using Banyan Goods as our example, if Banyan wanted to compare net sales in the current year\n(year of analysis) of $120,000 to the prior year (base year) of $100,000, the dollar change would be\nas follows:\nThe percentage change is found by taking the dollar change, dividing by the base year amount, and\nthen multiplying by 100.\nLet’s compute the percentage change for Banyan Goods’ net sales.\nThis means Banyan Goods saw an increase of $20,000 in net sales in the current year as compared\nto the prior year, which was a 20% increase. The same dollar change and percentage change\ncalculations would be used for the income statement line items as well as the balance sheet line\nitems. The image below shows the complete horizontal analysis of the income statement and\nbalance sheet for Banyan Goods."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Dollar change = $120,000– $1000,000 = $20,000\nPercentage change = (\n) × 100 = 20%\n$20,000\n$100,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Vertical Analysis\nVertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments.\nThe company will need to determine which line item they are comparing all items to within that\nstatement and then calculate the percentage makeup. These percentages are considered commonsize because they make businesses within industry comparable by taking out fluctuations for size. It\nis typical for an income statement to use net sales (or sales) as the comparison line item. This\nmeans net sales will be set at 100% and all other line items within the income statement will\nrepresent a percentage of net sales."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula to determine the common-size\npercentage is:\nFor example, if Banyan Goods set total assets as the base amount and wanted to see what\npercentage of total assets were made up of cash in the current year, the following calculation would\noccur.\nCash in the current year is $110,000 and total assets equal $250,000, giving a common-size\npercentage of 44%. If the company had an expected cash balance of 40% of total assets, they\nwould be exceeding expectations. This may not be enough of a difference to make a change, but if\nthey notice this deviates from industry standards, they may need to make adjustments, such as\nreducing the amount of cash on hand to reinvest in the business."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The image below shows the\ncommon-size calculations on the comparative income statements and comparative balance sheets\nfor Banyan Goods.\nFigure A3 Income Statements and Vertical Analysis.\nCommon-size percentage = (\n) × 100 = 44%\n$110,000\n$250,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A stakeholder could be looking to invest, become a supplier, make a loan, or alter\ninternal operations, among other things, based in part on the outcomes of ratio analysis. The\ninformation resulting from ratio analysis can be used to examine trends in performance, establish\nbenchmarks for success, set budget expectations, and compare industry competitors. There are four\nmain categories of ratios: liquidity, solvency, efficiency, and profitability. Note that while there are\nmore ideal outcomes for some ratios, the industry in which the business operates can change the\ninfluence each of these outcomes has over stakeholder decisions."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "(You will learn more about ratios,\nindustry standards, and ratio interpretation in advanced accounting courses.)\nLiquidity Ratios\nLiquidity ratios show the ability of the company to pay short-term obligations if they came due\nimmediately with assets that can be quickly converted to cash. This is done by comparing current\nassets to current liabilities. Lenders, for example, may consider the outcomes of liquidity ratios when\ndeciding whether to extend a loan to a company. A company would like to be liquid enough to\nmanage any currently due obligations but not too liquid where they may not be effectively investing\nin growth opportunities. Three common liquidity measurements are working capital, current ratio,\nand quick ratio."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working Capital\nWorking capital measures the financial health of an organization in the short-term by finding the\ndifference between current assets and current liabilities. A company will need enough current assets\nto cover current liabilities; otherwise, they may not be able to continue operations in the future.\nBefore a lender extends credit, they will review the working capital of the company to see if the\ncompany can meet their obligations. A larger difference signals that a company can cover their\nshort-term debts and a lender may be more willing to extend the loan. On the other hand, too large\nof a difference may indicate that the company may not be correctly using their assets to grow the\nbusiness."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula for working capital is:\nUsing Banyan Goods, working capital is computed as follows for the current year:\nIn this case, current assets were $200,000, and current liabilities were $100,000. Current assets\nwere far greater than current liabilities for Banyan Goods and they would easily be able to cover\nshort-term debt.\nThe dollar value of the difference for working capital is limited given company size and scope. It is\nmost useful to convert this information to a ratio to determine the company’s current financial health.\nThis ratio is the current ratio.\nCurrent Ratio\nWorking capital = $200,000– $100,000 = $100,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Quick Ratio\nThe quick ratio, also known as the acid-test ratio, is similar to the current ratio except current assets\nare more narrowly defined as the most liquid assets, which exclude inventory and prepaid expenses.\nThe conversion of inventory and prepaid expenses to cash can sometimes take more time than the\nliquidation of other current assets. A company will want to know what they have on hand and can\nuse quickly if an immediate obligation is due. The formula for the quick ratio is:\nThe quick ratio for Banyan Goods in the current year is:\nA 1.6:1 ratio means the company has enough quick assets to cover current liabilities.\nAnother category of financial measurement uses solvency ratios."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Solvency Ratios\nSolvency implies that a company can meet its long-term obligations and will likely stay in business in\nthe future. To stay in business the company must generate more revenue than debt in the long-term.\nMeeting long-term obligations includes the ability to pay any interest incurred on long-term debt. Two\nmain solvency ratios are the debt-to-equity ratio and the times interest earned ratio.\nDebt to Equity Ratio\nThe debt-to-equity ratio shows the relationship between debt and equity as it relates to business\nfinancing. A company can take out loans, issue stock, and retain earnings to be used in future\nperiods to keep operations running. It is less risky and less costly to use equity sources for financing\nas compared to debt resources."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "This is mainly due to interest expense repayment that a loan carries\nas opposed to equity, which does not have this requirement. Therefore, a company wants to know\nhow much debt and equity contribute to its financing. Ideally, a company would prefer more equity\nthan debt financing. The formula for the debt to equity ratio is:\nCurrent ratio = (\n) = 2 or 2:1\n$200,000\n$100,000\nQuick ratio = (\n) = 1.6 or 1.6:1\n$110,000 + $20,000 + $30,000\n$100,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Lenders will pay attention to\nthis ratio before extending credit. The more times over a company can cover interest, the more likely\na lender will extend long-term credit. The formula for times interest earned is:\nThe information needed to compute times interest earned for Banyan Goods in the current year can\nbe found on the income statement.\nThe $43,000 is the operating income, representing earnings before interest and taxes. The 21.5\ntimes outcome suggests that Banyan Goods can easily repay interest on an outstanding loan and\ncreditors would have little risk that Banyan Goods would be unable to pay.\nAnother category of financial measurement uses efficiency ratios.\nEfficiency Ratios\nEfficiency shows how well a company uses and manages their assets."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Areas of importance with\nefficiency are management of sales, accounts receivable, and inventory. A company that is efficient\ntypically will be able to generate revenues quickly using the assets it acquires. Let’s examine four\nefficiency ratios: accounts receivable turnover, total asset turnover, inventory turnover, and days’\nsales in inventory.\nAccounts Receivable Turnover\nAccounts receivable turnover measures how many times in a period (usually a year) a company will\ncollect cash from accounts receivable. A higher number of times could mean cash is collected more\nquickly and that credit customers are of high quality. A higher number is usually preferable because\nthe cash collected can be reinvested in the business at a quicker rate."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A lower number of times\ncould mean cash is collected slowly on these accounts and customers may not be properly qualified\nto accept the debt. The formula for accounts receivable turnover is:\nDebt-to-equity ratio = (\n) = 1.5 or 1.5:1\n$150,000\n$100,000\nTimes interest earned = (\n) = 21.5 times\n$43,000\n$2,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Given this\noutcome, they may want to consider stricter credit lending practices to make sure credit customers\nare of a higher quality. They may also need to be more aggressive with collecting any outstanding\naccounts.\nTotal Asset Turnover\nTotal asset turnover measures the ability of a company to use their assets to generate revenues. A\ncompany would like to use as few assets as possible to generate the most net sales. Therefore, a\nhigher total asset turnover means the company is using their assets very efficiently to produce net\nsales. The formula for total asset turnover is:\nAverage total assets are found by dividing the sum of beginning and ending total assets balances\nfound on the balance sheet."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The beginning total assets balance in the current year is taken from the\nending total assets balance in the prior year.\nBanyan Goods’ total asset turnover is:\nThe outcome of 0.53 means that for every $1 of assets, $0.53 of net sales are generated. Over time,\nBanyan Goods would like to see this turnover ratio increase.\nInventory Turnover\nAverage accounts receivable\nAccounts receivable turnover\n=\n=\n= $25,000\n$20,000+$30,000\n2\n= 4 times\n$100,000\n$25,000\nAverage total assets\nTotal assets turnover\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.53 times (rounded)\n$120,000\n$225,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Banyan Goods’ inventory turnover is:\n1.6 times is a very low turnover rate for Banyan Goods. This may mean the company is maintaining\ntoo high an inventory supply to meet a low demand from customers. They may want to decrease\ntheir on-hand inventory to free up more liquid assets to use in other ways.\nDays’ Sales in Inventory\nDays’ sales in inventory expresses the number of days it takes a company to turn inventory into\nsales. This assumes that no new purchase of inventory occurred within that time period. The fewer\nthe number of days, the more quickly the company can sell its inventory. The higher the number of\ndays, the longer it takes to sell its inventory. The formula for days’ sales in inventory is:\nBanyan Goods’ days’ sales in inventory is:\n243 days is a long time to sell inventory."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "While industry dictates what is an acceptable number of\ndays to sell inventory, 243 days is unsustainable long-term. Banyan Goods will need to better\nmanage their inventory and sales strategies to move inventory more quickly.\nThe last category of financial measurement examines profitability ratios.\nProfitability Ratios\nProfitability considers how well a company produces returns given their operational performance.\nThe company needs to leverage its operations to increase profit. To assist with profit goal\nAverage inventory\nInventory turnover\n=\n=\n= $37,500\n$35,000+$40,000\n2\n= 1.6 times\n$60,000\n$37,500\nDays' sales in inventory = (\n) × 365 = 243 days (rounded)\n$40,000\n$60,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "If Banyan Goods thinks this is too\nlow, the company would try and find ways to reduce expenses and increase sales.\nReturn on Total Assets\nThe return on total assets measures the company’s ability to use its assets successfully to generate\na profit. The higher the return (ratio outcome), the more profit is created from asset use. Average\ntotal assets are found by dividing the sum of beginning and ending total assets balances found on\nthe balance sheet. The beginning total assets balance in the current year is taken from the ending\ntotal assets balance in the prior year. The formula for return on total assets is:\nFor Banyan Goods, the return on total assets for the current year is:\nThe higher the figure, the better the company is using its assets to create a profit."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Industry standards\ncan dictate what is an acceptable return.\nReturn on Equity\nReturn on equity measures the company’s ability to use its invested capital to generate income. The\ninvested capital comes from stockholders investments in the company’s stock and its retained\nearnings and is leveraged to create profit. The higher the return, the better the company is doing at\nusing its investments to yield a profit. The formula for return on equity is:\nProfit margin = (\n) = 0.29 (rounded) or 29%\n$35,000\n$120,000\nAverage total assets\nReturn on total assets\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.16 (rounded) or 16%\n$35,000\n$225,000"
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Advantages and Disadvantages of Financial\nStatement Analysis\nThere are several advantages and disadvantages to financial statement analysis. Financial\nstatement analysis can show trends over time, which can be helpful in making future business\ndecisions. Converting information to percentages or ratios eliminates some of the disparity between\ncompetitor sizes and operating abilities, making it easier for stakeholders to make informed\ndecisions. It can assist with understanding the makeup of current operations within the business,\nand which shifts need to occur internally to increase productivity.\nA stakeholder needs to keep in mind that past performance does not always dictate future\nperformance."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Attention must be given to possible economic influences that could skew the numbers\nbeing analyzed, such as inflation or a recession. Additionally, the way a company reports information\nwithin accounts may change over time. For example, where and when certain transactions are\nrecorded may shift, which may not be readily evident in the financial statements.\nA company that wants to budget properly, control costs, increase revenues, and make long-term\nexpenditure decisions may want to use financial statement analysis to guide future operations. As\nlong as the company understands the limitations of the information provided, financial statement\nanalysis is a good tool to predict growth and company financial strength."
  },
  {
    "book_id": "37fe4268-399e-424d-b4ec-843979418421",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholder equity\nReturn on equity\n=\n=\n= $95,000\n$90,000+$100,000\n2\n= 0.37 (rounded) or 37%\n$35,000\n$95,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Financial Statement Analysis\nFinancial statement analysis reviews financial information found on financial statements to make\ninformed decisions about the business. The income statement, statement of retained earnings,\nbalance sheet, and statement of cash flows, among other financial information, can be analyzed.\nThe information obtained from this analysis can benefit decision-making for internal and external\nstakeholders and can give a company valuable information on overall performance and specific\nareas for improvement. The analysis can help them with budgeting, deciding where to cut costs,\nhow to increase revenues, and future capital investments opportunities."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "When considering the outcomes from analysis, it is important for a company to understand that data\nproduced needs to be compared to others within industry and close competitors. The company\nshould also consider their past experience and how it corresponds to current and future\nperformance expectations. Three common analysis tools are used for decision-making; horizontal\nanalysis, vertical analysis, and financial ratios.\nFor our discussion of financial statement analysis, we will use Banyan Goods. Banyan Goods is a\nmerchandising company that sells a variety of products. The image below shows the comparative\nincome statements and balance sheets for the past two years.\nFigure A1 Comparative Income Statements and Balance Sheets."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 1,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Keep in mind that the comparative income statements and balance sheets for Banyan Goods are\nsimplified for our calculations and do not fully represent all the accounts a company could maintain.\nLet’s begin our analysis discussion by looking at horizontal analysis.\nHorizontal Analysis\nHorizontal analysis (also known as trend analysis) looks at trends over time on various financial\nstatement line items. A company will look at one period (usually a year) and compare it to another"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "period. For example, a company may compare sales from their current year to sales from the prior\nyear. The trending of items on these financial statements can give a company valuable information\non overall performance and specific areas for improvement. It is most valuable to do horizontal\nanalysis for information over multiple periods to see how change is occurring for each line item. If\nmultiple periods are not used, it can be difficult to identify a trend. The year being used for\ncomparison purposes is called the base year (usually the prior period). The year of comparison for\nhorizontal analysis is analyzed for dollar and percent changes against the base year.\nThe dollar change is found by taking the dollar amount in the base year and subtracting that from\nthe year of analysis."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Using Banyan Goods as our example, if Banyan wanted to compare net sales in the current year\n(year of analysis) of $120,000 to the prior year (base year) of $100,000, the dollar change would be\nas follows:\nThe percentage change is found by taking the dollar change, dividing by the base year amount, and\nthen multiplying by 100.\nLet’s compute the percentage change for Banyan Goods’ net sales.\nThis means Banyan Goods saw an increase of $20,000 in net sales in the current year as compared\nto the prior year, which was a 20% increase. The same dollar change and percentage change\ncalculations would be used for the income statement line items as well as the balance sheet line\nitems. The image below shows the complete horizontal analysis of the income statement and\nbalance sheet for Banyan Goods."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 2,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Dollar change = $120,000– $1000,000 = $20,000\nPercentage change = (\n) × 100 = 20%\n$20,000\n$100,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Figure A2 Income Statements and Horizontal Analysis.\nDepending on their expectations, Banyan Goods could make decisions to alter operations to\nproduce expected outcomes. For example, Banyan saw a 50% accounts receivable increase from\nthe prior year to the current year. If they were only expecting a 20% increase, they may need to\nexplore this line item further to determine what caused this difference and how to correct it going\nforward. It could possibly be that they are extending credit more readily than anticipated or not\ncollecting as rapidly on outstanding accounts receivable. The company will need to further examine\nthis difference before deciding on a course of action. Another method of analysis Banyan might\nconsider before making a decision is vertical analysis."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Vertical Analysis\nVertical analysis shows a comparison of a line item within a statement to another line item within that\nsame statement. For example, a company may compare cash to total assets in the current year.\nThis allows a company to see what percentage of cash (the comparison line item) makes up total\nassets (the other line item) during the period. This is different from horizontal analysis, which\ncompares across years. Vertical analysis compares line items within a statement in the current year.\nThis can help a business to know how much of one item is contributing to overall operations. For\nexample, a company may want to know how much inventory contributes to total assets."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 3,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "They can\nthen use this information to make business decisions such as preparing the budget, cutting costs,\nincreasing revenues, or capital investments.\nThe company will need to determine which line item they are comparing all items to within that\nstatement and then calculate the percentage makeup. These percentages are considered commonsize because they make businesses within industry comparable by taking out fluctuations for size. It\nis typical for an income statement to use net sales (or sales) as the comparison line item. This\nmeans net sales will be set at 100% and all other line items within the income statement will\nrepresent a percentage of net sales."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "On the balance sheet, a company will typically look at two areas: (1) total assets, and (2) total\nliabilities and stockholders’ equity. Total assets will be set at 100% and all assets will represent a\npercentage of total assets. Total liabilities and stockholders’ equity will also be set at 100% and all\nline items within liabilities and equity will be represented as a percentage of total liabilities and\nstockholders’ equity. The line item set at 100% is considered the base amount and the comparison\nline item is considered the comparison amount."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula to determine the common-size\npercentage is:\nFor example, if Banyan Goods set total assets as the base amount and wanted to see what\npercentage of total assets were made up of cash in the current year, the following calculation would\noccur.\nCash in the current year is $110,000 and total assets equal $250,000, giving a common-size\npercentage of 44%. If the company had an expected cash balance of 40% of total assets, they\nwould be exceeding expectations. This may not be enough of a difference to make a change, but if\nthey notice this deviates from industry standards, they may need to make adjustments, such as\nreducing the amount of cash on hand to reinvest in the business."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 4,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The image below shows the\ncommon-size calculations on the comparative income statements and comparative balance sheets\nfor Banyan Goods.\nFigure A3 Income Statements and Vertical Analysis.\nCommon-size percentage = (\n) × 100 = 44%\n$110,000\n$250,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Even though vertical analysis is a statement comparison within the same year, Banyan can use\ninformation from the prior year’s vertical analysis to make sure the business is operating as\nexpected. For example, unearned revenues increased from the prior year to the current year and\nmade up a larger portion of total liabilities and stockholders’ equity. This could be due to many\nfactors, and Banyan Goods will need to examine this further to see why this change has occurred.\nLet’s turn to financial statement analysis using financial ratios.\nOverview of Financial Ratios\nFinancial ratios help both internal and external users of information make informed decisions about\na company."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A stakeholder could be looking to invest, become a supplier, make a loan, or alter\ninternal operations, among other things, based in part on the outcomes of ratio analysis. The\ninformation resulting from ratio analysis can be used to examine trends in performance, establish\nbenchmarks for success, set budget expectations, and compare industry competitors. There are four\nmain categories of ratios: liquidity, solvency, efficiency, and profitability. Note that while there are\nmore ideal outcomes for some ratios, the industry in which the business operates can change the\ninfluence each of these outcomes has over stakeholder decisions."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "(You will learn more about ratios,\nindustry standards, and ratio interpretation in advanced accounting courses.)\nLiquidity Ratios\nLiquidity ratios show the ability of the company to pay short-term obligations if they came due\nimmediately with assets that can be quickly converted to cash. This is done by comparing current\nassets to current liabilities. Lenders, for example, may consider the outcomes of liquidity ratios when\ndeciding whether to extend a loan to a company. A company would like to be liquid enough to\nmanage any currently due obligations but not too liquid where they may not be effectively investing\nin growth opportunities. Three common liquidity measurements are working capital, current ratio,\nand quick ratio."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working Capital\nWorking capital measures the financial health of an organization in the short-term by finding the\ndifference between current assets and current liabilities. A company will need enough current assets\nto cover current liabilities; otherwise, they may not be able to continue operations in the future.\nBefore a lender extends credit, they will review the working capital of the company to see if the\ncompany can meet their obligations. A larger difference signals that a company can cover their\nshort-term debts and a lender may be more willing to extend the loan. On the other hand, too large\nof a difference may indicate that the company may not be correctly using their assets to grow the\nbusiness."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 5,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The formula for working capital is:\nUsing Banyan Goods, working capital is computed as follows for the current year:\nIn this case, current assets were $200,000, and current liabilities were $100,000. Current assets\nwere far greater than current liabilities for Banyan Goods and they would easily be able to cover\nshort-term debt.\nThe dollar value of the difference for working capital is limited given company size and scope. It is\nmost useful to convert this information to a ratio to determine the company’s current financial health.\nThis ratio is the current ratio.\nCurrent Ratio\nWorking capital = $200,000– $100,000 = $100,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Working capital expressed as a ratio is the current ratio. The current ratio considers the amount of\ncurrent assets available to cover current liabilities. The higher the current ratio, the more likely the\ncompany can cover its short-term debt. The formula for current ratio is:\nThe current ratio in the current year for Banyan Goods is:\nA 2:1 ratio means the company has twice as many current assets as current liabilities; typically, this\nwould be plenty to cover obligations. This may be an acceptable ratio for Banyan Goods, but if it is\ntoo high, they may want to consider using those assets in a different way to grow the company."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Quick Ratio\nThe quick ratio, also known as the acid-test ratio, is similar to the current ratio except current assets\nare more narrowly defined as the most liquid assets, which exclude inventory and prepaid expenses.\nThe conversion of inventory and prepaid expenses to cash can sometimes take more time than the\nliquidation of other current assets. A company will want to know what they have on hand and can\nuse quickly if an immediate obligation is due. The formula for the quick ratio is:\nThe quick ratio for Banyan Goods in the current year is:\nA 1.6:1 ratio means the company has enough quick assets to cover current liabilities.\nAnother category of financial measurement uses solvency ratios."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Solvency Ratios\nSolvency implies that a company can meet its long-term obligations and will likely stay in business in\nthe future. To stay in business the company must generate more revenue than debt in the long-term.\nMeeting long-term obligations includes the ability to pay any interest incurred on long-term debt. Two\nmain solvency ratios are the debt-to-equity ratio and the times interest earned ratio.\nDebt to Equity Ratio\nThe debt-to-equity ratio shows the relationship between debt and equity as it relates to business\nfinancing. A company can take out loans, issue stock, and retain earnings to be used in future\nperiods to keep operations running. It is less risky and less costly to use equity sources for financing\nas compared to debt resources."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 6,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "This is mainly due to interest expense repayment that a loan carries\nas opposed to equity, which does not have this requirement. Therefore, a company wants to know\nhow much debt and equity contribute to its financing. Ideally, a company would prefer more equity\nthan debt financing. The formula for the debt to equity ratio is:\nCurrent ratio = (\n) = 2 or 2:1\n$200,000\n$100,000\nQuick ratio = (\n) = 1.6 or 1.6:1\n$110,000 + $20,000 + $30,000\n$100,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The information needed to compute the debt-to-equity ratio for Banyan Goods in the current year\ncan be found on the balance sheet.\nThis means that for every $1 of equity contributed toward financing, $1.50 is contributed from\nlenders. This would be a concern for Banyan Goods. This could be a red flag for potential investors\nthat the company could be trending toward insolvency. Banyan Goods might want to get the ratio\nbelow 1:1 to improve their long-term business viability.\nTimes Interest Earned Ratio\nTime interest earned measures the company’s ability to pay interest expense on long-term debt\nincurred. This ability to pay is determined by the available earnings before interest and taxes (EBIT)\nare deducted. These earnings are considered the operating income."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Lenders will pay attention to\nthis ratio before extending credit. The more times over a company can cover interest, the more likely\na lender will extend long-term credit. The formula for times interest earned is:\nThe information needed to compute times interest earned for Banyan Goods in the current year can\nbe found on the income statement.\nThe $43,000 is the operating income, representing earnings before interest and taxes. The 21.5\ntimes outcome suggests that Banyan Goods can easily repay interest on an outstanding loan and\ncreditors would have little risk that Banyan Goods would be unable to pay.\nAnother category of financial measurement uses efficiency ratios.\nEfficiency Ratios\nEfficiency shows how well a company uses and manages their assets."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Areas of importance with\nefficiency are management of sales, accounts receivable, and inventory. A company that is efficient\ntypically will be able to generate revenues quickly using the assets it acquires. Let’s examine four\nefficiency ratios: accounts receivable turnover, total asset turnover, inventory turnover, and days’\nsales in inventory.\nAccounts Receivable Turnover\nAccounts receivable turnover measures how many times in a period (usually a year) a company will\ncollect cash from accounts receivable. A higher number of times could mean cash is collected more\nquickly and that credit customers are of high quality. A higher number is usually preferable because\nthe cash collected can be reinvested in the business at a quicker rate."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 7,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "A lower number of times\ncould mean cash is collected slowly on these accounts and customers may not be properly qualified\nto accept the debt. The formula for accounts receivable turnover is:\nDebt-to-equity ratio = (\n) = 1.5 or 1.5:1\n$150,000\n$100,000\nTimes interest earned = (\n) = 21.5 times\n$43,000\n$2,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Many companies do not split credit and cash sales, in which case net sales would be used to\ncompute accounts receivable turnover. Average accounts receivable is found by dividing the sum of\nbeginning and ending accounts receivable balances found on the balance sheet. The beginning\naccounts receivable balance in the current year is taken from the ending accounts receivable\nbalance in the prior year.\nWhen computing the accounts receivable turnover for Banyan Goods, let’s assume net credit sales\nmake up $100,000 of the $120,000 of the net sales found on the income statement in the current\nyear.\nAn accounts receivable turnover of four times per year may be low for Banyan Goods."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Given this\noutcome, they may want to consider stricter credit lending practices to make sure credit customers\nare of a higher quality. They may also need to be more aggressive with collecting any outstanding\naccounts.\nTotal Asset Turnover\nTotal asset turnover measures the ability of a company to use their assets to generate revenues. A\ncompany would like to use as few assets as possible to generate the most net sales. Therefore, a\nhigher total asset turnover means the company is using their assets very efficiently to produce net\nsales. The formula for total asset turnover is:\nAverage total assets are found by dividing the sum of beginning and ending total assets balances\nfound on the balance sheet."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 8,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "The beginning total assets balance in the current year is taken from the\nending total assets balance in the prior year.\nBanyan Goods’ total asset turnover is:\nThe outcome of 0.53 means that for every $1 of assets, $0.53 of net sales are generated. Over time,\nBanyan Goods would like to see this turnover ratio increase.\nInventory Turnover\nAverage accounts receivable\nAccounts receivable turnover\n=\n=\n= $25,000\n$20,000+$30,000\n2\n= 4 times\n$100,000\n$25,000\nAverage total assets\nTotal assets turnover\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.53 times (rounded)\n$120,000\n$225,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Inventory turnover measures how many times during the year a company has sold and replaced\ninventory. This can tell a company how well inventory is managed. A higher ratio is preferable;\nhowever, an extremely high turnover may mean that the company does not have enough inventory\navailable to meet demand. A low turnover may mean the company has too much supply of inventory\non hand. The formula for inventory turnover is:\nCost of goods sold for the current year is found on the income statement. Average inventory is found\nby dividing the sum of beginning and ending inventory balances found on the balance sheet. The\nbeginning inventory balance in the current year is taken from the ending inventory balance in the\nprior year."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Banyan Goods’ inventory turnover is:\n1.6 times is a very low turnover rate for Banyan Goods. This may mean the company is maintaining\ntoo high an inventory supply to meet a low demand from customers. They may want to decrease\ntheir on-hand inventory to free up more liquid assets to use in other ways.\nDays’ Sales in Inventory\nDays’ sales in inventory expresses the number of days it takes a company to turn inventory into\nsales. This assumes that no new purchase of inventory occurred within that time period. The fewer\nthe number of days, the more quickly the company can sell its inventory. The higher the number of\ndays, the longer it takes to sell its inventory. The formula for days’ sales in inventory is:\nBanyan Goods’ days’ sales in inventory is:\n243 days is a long time to sell inventory."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 9,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "While industry dictates what is an acceptable number of\ndays to sell inventory, 243 days is unsustainable long-term. Banyan Goods will need to better\nmanage their inventory and sales strategies to move inventory more quickly.\nThe last category of financial measurement examines profitability ratios.\nProfitability Ratios\nProfitability considers how well a company produces returns given their operational performance.\nThe company needs to leverage its operations to increase profit. To assist with profit goal\nAverage inventory\nInventory turnover\n=\n=\n= $37,500\n$35,000+$40,000\n2\n= 1.6 times\n$60,000\n$37,500\nDays' sales in inventory = (\n) × 365 = 243 days (rounded)\n$40,000\n$60,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "attainment, company revenues need to outweigh expenses. Let’s consider three profitability\nmeasurements and ratios: profit margin, return on total assets, and return on equity.\nProfit Margin\nProfit margin represents how much of sales revenue has translated into income. This ratio shows\nhow much of each $1 of sales is returned as profit. The larger the ratio figure (the closer it gets to 1),\nthe more of each sales dollar is returned as profit. The portion of the sales dollar not returned as\nprofit goes toward expenses. The formula for profit margin is:\nFor Banyan Goods, the profit margin in the current year is:\nThis means that for every dollar of sales, $0.29 returns as profit."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "If Banyan Goods thinks this is too\nlow, the company would try and find ways to reduce expenses and increase sales.\nReturn on Total Assets\nThe return on total assets measures the company’s ability to use its assets successfully to generate\na profit. The higher the return (ratio outcome), the more profit is created from asset use. Average\ntotal assets are found by dividing the sum of beginning and ending total assets balances found on\nthe balance sheet. The beginning total assets balance in the current year is taken from the ending\ntotal assets balance in the prior year. The formula for return on total assets is:\nFor Banyan Goods, the return on total assets for the current year is:\nThe higher the figure, the better the company is using its assets to create a profit."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 10,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Industry standards\ncan dictate what is an acceptable return.\nReturn on Equity\nReturn on equity measures the company’s ability to use its invested capital to generate income. The\ninvested capital comes from stockholders investments in the company’s stock and its retained\nearnings and is leveraged to create profit. The higher the return, the better the company is doing at\nusing its investments to yield a profit. The formula for return on equity is:\nProfit margin = (\n) = 0.29 (rounded) or 29%\n$35,000\n$120,000\nAverage total assets\nReturn on total assets\n=\n=\n= $225,000\n$200,000+$250,000\n2\n= 0.16 (rounded) or 16%\n$35,000\n$225,000"
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholders’ equity is found by dividing the sum of beginning and ending stockholders’\nequity balances found on the balance sheet. The beginning stockholders’ equity balance in the\ncurrent year is taken from the ending stockholders’ equity balance in the prior year. Keep in mind\nthat the net income is calculated after preferred dividends have been paid.\nFor Banyan Goods, we will use the net income figure and assume no preferred dividends have been\npaid. The return on equity for the current year is:\nThe higher the figure, the better the company is using its investments to create a profit. Industry\nstandards can dictate what is an acceptable return."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Advantages and Disadvantages of Financial\nStatement Analysis\nThere are several advantages and disadvantages to financial statement analysis. Financial\nstatement analysis can show trends over time, which can be helpful in making future business\ndecisions. Converting information to percentages or ratios eliminates some of the disparity between\ncompetitor sizes and operating abilities, making it easier for stakeholders to make informed\ndecisions. It can assist with understanding the makeup of current operations within the business,\nand which shifts need to occur internally to increase productivity.\nA stakeholder needs to keep in mind that past performance does not always dictate future\nperformance."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Attention must be given to possible economic influences that could skew the numbers\nbeing analyzed, such as inflation or a recession. Additionally, the way a company reports information\nwithin accounts may change over time. For example, where and when certain transactions are\nrecorded may shift, which may not be readily evident in the financial statements.\nA company that wants to budget properly, control costs, increase revenues, and make long-term\nexpenditure decisions may want to use financial statement analysis to guide future operations. As\nlong as the company understands the limitations of the information provided, financial statement\nanalysis is a good tool to predict growth and company financial strength."
  },
  {
    "book_id": "c5821b08-1607-4c2c-99b7-4f3e19a88bc5",
    "book_title": "NDLI Financial Statement Analysis.pdf",
    "page": 11,
    "source": "NDLI Financial Statement Analysis.pdf",
    "chunk_text": "Average stockholder equity\nReturn on equity\n=\n=\n= $95,000\n$90,000+$100,000\n2\n= 0.37 (rounded) or 37%\n$35,000\n$95,000"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 1,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "1 \n \nSET 1 (Answer Key) \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nConsider a single neuron as a linear unit. Train the model with 'sugars' (grams of sugars \nper serving) as input and 'calories' (calories per serving) as output. Let the bias is b=90, and \nthe weight is w=2.5. Estimate the calorie content of cereal with 5 grams of sugar per serving \nwith a network diagram. \nAnswer: \nGiven Values: Input: x=5 grams sugar, Weight: w=2.5, Bias: b=90 \nA linear neuron computes the output as: y = w ⋅ x + b. = =2.5×5+90=102.5 \nPredicted calories = 102.5. \nCO1 \n2."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 1,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "How do the complexity of a model and the size of the training dataset contribute to \noverfitting, and what techniques can be employed to mitigate its effects? \nAnswer : \nOverfitting happens when the model is too complex (many parameters) or the training dataset is \ntoo small, leading to memorization instead of generalization. It can be mitigated using \nregularization (L1/L2, dropout), early stopping, data augmentation, cross-validation, simpler \nmodels, or by collecting more data. \nCO1 \n3. \nDetermine the number of parameters if the input image shape is 1024x1024x3 and the \nconvolution layer uses 32 filters of size 7x7 with a stride of 1 and padding of 2."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 1,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Answer: Given Values : Input shape = 1024×1024×3, Filter size = 7×7, Input channels = 3, \nNumber of filters = 32, Stride & padding affect output size, but not parameter count. \nFormula for Parameters in a Convolutional Layer: \ntotal parameters = parameters per filter × number of filters \nparameters per filter = (filter height × filter width × input channels) + 1 (bias) \nNow, Filter weights = 7 × 7 × 3 = 1477 × 7 × 3=147, Add bias = 147 + 1 = 148147 + 1 = 148 per \nfilter, Then For 32 filters: 148 × 32 = 4736148 × 32 = 4736, Total parameters = 4736 \nCO2 \n4. \nWhy are convolutions useful in deep learning? \nAnswer: \nConvolutions are useful in deep learning because they reduce the number of parameters compared \nto fully connected networks, making the model more efficient."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 1,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "They can automatically extract \nlocal features such as edges, corners, and textures, which are essential for understanding images. \nConvolutions also provide translation invariance, meaning the same feature can be detected \nanywhere in the image. By stacking multiple convolutional layers, the network can learn \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 2,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "2 \n \nhierarchical representations, starting from low-level patterns and progressing to high-level object \nfeatures. \n5. \nDistinguish between the phenomena of exploding gradients and vanishing gradients. \n \nExploding Gradients \n \nVanishing Gradients \n● Gradients become very large during \nbackprop. \n● Gradients become very small (near \nzero). \n● Unstable training, weights diverge. \n● Learning slows or stops, early layers \ndon’t learn. \n● Repeated multiplication → large \nvalues grow. \n● Repeated multiplication → values \nshrink. \n● Very deep networks, RNNs with long \nsequences \n● Very deep networks, RNNs with \nsigmoid/tanh. \n● Gradient clipping, small learning rate. \n● ReLU/variants, batch norm, skip \nconnections. \n \nCO3 \n6."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 2,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "How do LSTM and GRU models differ in terms of the number of gates and their \nfunctionalities? \nLSTM (Long Short-Term Memory) \nGRU (Gated Recurrent Unit) \n● 3 gates – Input gate, Forget gate, \nOutput gate. \n● 2 gates – Update gate, Reset gate. \n● Has a separate cell state and hidden \nstate → better for long dependencies. \n● Combines cell state and hidden state \ninto one → simpler. \n● More parameters → computationally \nheavier. \n● Fewer parameters → faster, less \nmemory. \n● Controls what to write, keep, and \noutput explicitly. \n● Controls how much of the past to \nkeep or reset. \n \nCO3 \n7. \nDefine Autoencoder and offer concrete examples of its applications in real-world scenarios?"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 2,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Answer: \nAn Autoencoder is a type of neural network used for unsupervised learning that learns to encode \ninput data into a lower-dimensional representation (latent space) and then reconstruct it back to \nthe original form. It consists of two parts Encoder and decoder. Encoder compresses the input \ninto latent features. Decoder reconstructs the input from latent features.The goal is to learn \nefficient representations by minimizing reconstruction error. \nApplications : Image Denoising, Dimensionality Reduction, Anomaly Detection, Image \nCompression, Recommendation Systems \n \nCO4 \n8. \nWhy do RBMs use Gibbs sampling during training? \nAnswers: \n RBMs use Gibbs sampling because computing exact probabilities over all visible–hidden \nconfigurations is intractable."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 2,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Gibbs sampling alternately samples hidden units given visible units \nand visible units given hidden units, approximating the model’s distribution. This enables \nefficient training using algorithms like Contrastive Divergence. \nCO4 \n9. \nDefine Intersection over union(IoU) in bounding box prediction. \nAnswer: Intersection over Union (IoU) is a metric used to evaluate object detection models. It \nCO5"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 3,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "3 \n \nmeasures the overlap between the predicted bounding box and the ground-truth bounding box. \n \n \n10. \nWhat makes the CamVid dataset suitable for urban scene understanding in semantic \nsegmentation? \nAnswer: The CamVid dataset is suitable for urban scene semantic segmentation because it \ncontains video frames of real-world urban driving, with pixel-level annotations for multiple \nobject classes like roads, vehicles, pedestrians, and buildings. Its diverse classes and temporal \ninformation make it ideal for training and evaluating segmentation models in city environments. \nCO5 \n \n \n PART B (5 x 13 = 65 marks) \n \n \n \nCO \n11. \n(a) \nIn a practical deep learning project, how do you determine the most suitable \nactivation function for different layers in a neural network and why?"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 3,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Additionally, \nexplain these activation functions and provide PyTorch implementations? \nActivation function: \nWe want to set boundaries for the overall output value, x*w+b is passed through the activation \nfunction. Let us define the total input as z, where z= x*w+b. Then pass z through some activation \nfunction to limit its value. \n● The most simple networks rely on basic step function that outputs 0 or 1. \n● Regardless of the values it outputs 0 or 1. \n● This is a very strong function since small changes aren’t reflected. \n● There is just an immediate cutoff that splits between 0 and 1. \n \nIt would be nice if we could have a more dynamic function, for example the red line."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 3,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Sigmoid (Logistic) \n● The Sigmoid function (also known as the Logistic function) is one of the most widely \nused activation functions. The function is defined as: \nCO1"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 4,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "4 \n \n \n● The function is a common S-shaped curve. \n● The output of the function is centered at 0.5 with a range from 0 to 1. \n● The function is differentiable. That means we can find the slope of the sigmoid curve at \nany two points. \n● The function is monotonic but the function’s derivative is not. \nProblems with Sigmoid activation function \n● Vanishing gradient: looking at the function plot, you can see that when inputs become \nsmall or large, the function saturates at 0 or 1, with a derivative extremely close to 0. \nThus it has almost no gradient to propagate back through the network, so there is almost \nnothing left for lower layers. \n● Computationally expensive: the function has an exponential operation. \n● The output is not zero centered."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 4,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Implementation \n \nHyperbolic Tangent (Tanh) \nAnother very popular and widely used activation function is the Hyperbolic Tangent, also known \nas Tanh. It is defined as: \n \n● The function is a common S-shaped curve as well. \n● The difference is that the output of Tanh is zero centered with a range from -1 to 1 \n(instead of 0 to 1 in the case of the Sigmoid function) \n● The same as the Sigmoid, this function is differentiable \n● The same as the Sigmoid, the function is monotonic, but the function’s derivative is not. \nProblems with Tanh activation function"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 5,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "5 \n \n● Vanishing gradient: looking at the function plot, you can see that when inputs become \nsmall or large, the function saturates at -1 or 1, with a derivative extremely close to 0. \nThus it has almost no gradient to propagate back through the network, so there is almost \nnothing left for lower layers. \n● Computationally expensive: the function has an exponential operation. \n Implementation \n \nRectified Linear Unit (ReLU) \n● The Rectified Linear Unit (ReLU) is the most commonly used activation function in deep \nlearning. \n● The function returns 0 if the input is negative, but for any positive input, it returns that \nvalue back. The function is defined as: \n \n● Graphically, the ReLU function is composed of two linear pieces to account for non \nlinearities."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 5,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "A function is non-linear if the slope isn’t constant. So, the ReLU function is \nnon-linear around 0, but the slope is always either 0 (for negative inputs) or 1 (for \npositive inputs). \n● The ReLU function is continuous, but it is not differentiable at the origin. \n● The output of ReLU does not have a maximum value (It is not saturated) and this helps \nGradient Descent \n● The function is very fast to compute (Compare to Sigmoid and Tanh) \nProblem with ReLU \n● ReLU works great in most applications, but it is not perfect. It suffers from a problem \nknown as the dying ReLU. \nDying ReLU \nDuring training, some neurons effectively die, meaning they stop outputting anything other than 0."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 5,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "In some cases, you may find that half of your network’s neurons are dead, especially if you used a \nlarge learning rate. A neuron dies when its weights get tweaked in such a way that the weighted \nsum of its inputs are negative for all instances in the training set. When this happens, it just keeps"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 6,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "6 \n \noutputting 0s, and gradient descent does not affect it anymore since the gradient of the ReLU \nfunction is 0 when its input is negative. \n Implementation \n \nSoftmax Activation Function \n● Softmax function calculates the probability distribution of the event over K different \nevents. \n● This function will calculate the probabilities of each target class over all possible target \nclasses. \n● The range will be 0 to 1, and the sum of all the probabilities will be equal to one. \n● The model returns the probabilities of each class and the target class chosen will have the \nhighest probability. \n \nImplementation \n \n \n \n \n(OR) \n \n \n(b) \n(i) Create the appropriate PyTorch implementation of the following neural network."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 6,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "It should include model creation, model instantiation, loss function selection and \noptimizer selection.(6 Marks) \nCO1"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 7,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "7 \n \n \n \n(ii) Explain the confusion matrix for binary class and multiclass prediction. (7 Marks) \nClassification Metrics \n● Classification is about predicting the class labels given input data. \n● In binary classification, there are only two possible output classes. \n● A very common example of binary classification is spam detection, where the \ninput data could include the email text and metadata (sender, sending time), and \nthe output label is either “spam” or “not spam.” \nConfusion Matrix \n● Confusion Matrix is a performance measurement for the machine learning \nclassification problems where the output can be two or more classes. \n● It is a table with combinations of predicted and actual values."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 7,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● A confusion matrix is defined as the table that is often used to describe the \nperformance of a classification model on a set of the test data for which the true \nvalues are known."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 8,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "8 \n \n \nTrue Positive: We predicted positive and it’s true. \nTrue Negative: We predicted negative and it’s true. \nFalse Positive (Type 1 Error)- We predicted positive and it’s false. \nFalse Negative (Type 2 Error)- We predicted negative and it’s false. \nAccuracy: \nAccuracy simply measures how often the classifier correctly predicts. We can define \naccuracy as the ratio of the number of correct predictions and the total number of \npredictions. \n \nMulticlass Confusion Matrix \n \nImplementation"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 9,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "9 \n \n12. \n(a) \nThe CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes, with 6000 \nimages per class. There are 50000 training images and 10000 test images. Develop the \nPyTorch implementation for the following. \ni) Load the CIFAR-10 dataset (3 Marks) \nii) Do the necessary Preprocessing (3 Marks) \niii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function (3 Marks) \niv) Train the model with training data and validation data. (4 Marks) \n \nAnswer: \n \n \n \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 10,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "10 \n \n \n \n \n(OR) \n \n \n(b) \nExplain the following with examples \n(i) Padding (4 Marks) \n(ii) Striding (4 Marks) \n(iii) Pooling (5 Marks) \ni) Padding \nThere are two problems arise with convolution: \n1. Every time after convolution operation, original image size getting shrinks \nIn image classification task there are multiple convolution layers so after multiple \nconvolution operation, our original image will really get small but we don’t want \nthe image to shrink every time. \n2. When kernel moves over original images, it touches the edge of the image less \nnumber of times and touches the middle of the image more number of times and \nit overlaps also in the middle. So, the corner features of any image or on the \nedges aren’t used much in the output."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 10,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "In order to solve these two issues, a new concept is introduced called padding. Padding preserves \nthe size of the original image. \n \nPadded image convolved with 2*2 kernel \nSo if a 𝑛∗𝑛 matrix convolved with an f*f matrix the with padding p then the size of the output \nimage will be (n - f + 2p + 1) * (n - f + 2p + 1) where p =1 in this case. \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 11,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "11 \n \n \n(ii) Striding \n \n● Stride is the number of pixels shifts over the input matrix. \n● When the stride is 1 then we move the filters one pixel at a time. \n● When the stride is 2 then the filters jump 2 pixels at a time as we slide them around. \n● This will produce smaller output volumes spatially. \n \nleft image: stride =0, middle image: stride = 1, right image: stride =2 \nOutput Dimension \n● We can compute the spatial size of the output volume as a function of the input \nvolume size (W), the receptive field size of the Conv Layer neurons (F), the stride \nwith which they are applied (S), and the amount of zero padding used (P) on the \nborder. \n● Formula for calculating how many neurons “fit” is given by ((W−F+2P)/S)+1."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 11,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get \na 5x5 output. With stride 2 we would get a 3x3 output. \n(iii) Pooling \n● A pooling layer is another building block of a CNN. \n● Pooling function is to progressively reduce the spatial size of the representation to reduce \nthe network complexity and computational cost. \nThere are two types of widely used pooling in CNN layer: \n● Max Pooling \n● Average Pooling \nMax Pooling \n● Max pooling is simply a rule to take the maximum of a region and it helps to proceed \nwith the most important features from the image. \n● Max pooling selects the brighter pixels from the image. \n● It is useful when the background of the image is dark and we are interested in only the \nlighter pixels of the image."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 12,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "12 \n \n \nAverage Pooling \n● Average Pooling is different from Max Pooling in the sense that it retains much \ninformation about the “less important” elements of a block, or pool. \n● Whereas Max Pooling simply throws them away by picking the maximum value, \n● Average Pooling blends them in. This can be useful in a variety of situations, where such \ninformation is useful. \n \nPooling Dimension \n● The pooling layer downsamples the volume spatially, independently in each depth slice \nof the input volume. \n● In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride \n2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. \n \n \n \n \n \n13."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 12,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "(a) \nUsing GloVe word embeddings, implement the following tasks: \n(i) Develop a function that retrieves the embedding vector of a given word from pretrained GloVe embeddings. (5 Marks) \n(ii) Implement a function that computes and returns the most similar words to a given \nword by comparing their embedding vectors using cosine similarity. (8 Marks) \n \nimport numpy as np \n \ndef load_glove_embeddings(glove_file_path): \nCO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 13,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "13 \n \n embeddings = {} \n with open(glove_file_path, 'r', encoding='utf8') as f: \n for line in f: \n parts = line.strip().split() \n word = parts[0] \n vector = np.array(parts[1:], dtype=np.float32) \n embeddings[word] = vector \n return embeddings \n \ndef get_word_embedding(word, glove_embeddings): \n \"\"\" \n Retrieves the embedding vector for a given word. \n \n Args: \n word (str): The word to retrieve. \n glove_embeddings (dict): A dictionary mapping words to vectors. \n \n Returns: \n np.ndarray or None: Embedding vector if the word exists, otherwise None."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 13,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "\"\"\" \n return glove_embeddings.get(word) \n \n \nfrom sklearn.metrics.pairwise import cosine_similarity \n \ndef find_most_similar_words(word, glove_embeddings, top_n=5): \n \"\"\" \n Finds the most similar words to the input word using cosine similarity. \n \n Args: \n word (str): The word to find similarities for. \n glove_embeddings (dict): Preloaded GloVe embeddings. \n top_n (int): Number of similar words to return. \n \n Returns: \n List[Tuple[str, float]]: List of (word, similarity_score) tuples. \n \"\"\" \n if word not in glove_embeddings: \n return [] \n \n word_vec = glove_embeddings[word].reshape(1, -1) \n similarities = {} \n \n for other_word, vec in glove_embeddings.items(): \n if other_word == word: \n continue \n sim = cosine_similarity(word_vec, vec.reshape(1, -1))[0][0] \n similarities[other_word] = sim"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 14,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "14 \n \n # Sort by similarity and return the top N \n sorted_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True) \n return sorted_similar[:top_n] \n \n \n \n(OR) \n \n \n(b) \nDescribe the forward and backward pass mechanisms in bidirectional RNNs. Why are they \nparticularly useful in NLP tasks like machine translation? \nAnswer: \nA Bidirectional RNN consists of two RNNs running in opposite directions: \n· One processes the sequence forward (left to right). \n· The other processes it backward (right to left). \n· Their outputs are concatenated at each timestep to form the final representation. \n \nA Bidirectional RNN consists of two RNNs running in opposite directions: \n· One processes the sequence forward (left to right). \n· The other processes it backward (right to left)."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 14,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "· Their outputs are concatenated at each time step to form the final representation. \n \nCO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 16,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "16 \n \n \n \n \n \n14. \n(a) \ni) Enumerate and explain the various types of Generative Adversarial Network (GAN) \narchitectures. (7 Marks) \nGenerative Adversarial Networks (GANs) operate on an adversarial principle, pitting a \nGenerator (G) against a Discriminator (D) to learn to produce realistic data. Over time, \nnumerous architectures have emerged to improve training stability, output quality, and \ncontrol over generation. Here are some of the most significant types: \n1. Vanilla GAN (Original GAN): \n○ Architecture: The foundational model, typically using Multi-Layer \nPerceptrons (MLPs) for both G and D.The Generator maps a random noise \nvector to a data sample, and the Discriminator outputs a probability of the \ninput being real."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 16,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Working Principle: G aims to produce samples that fool D into \nclassifying them as real. D's goal is to accurately distinguish real data from \nG's fakes. This creates a minimax game. \n○ Contribution: Introduced the core adversarial training concept, laying the \ngroundwork for all subsequent GAN research. \n○ Limitations: Suffers from training instability, often leading to mode \ncollapse (G produces a limited variety of outputs) and difficulty generating \nhigh-resolution, diverse samples. \n2. Deep Convolutional GAN (DCGAN): \n○ Architecture: Replaced the MLPs of vanilla GANs with Convolutional \nNeural Networks (CNNs) in both G and D. The Generator uses \n\"transposed convolutions\" (deconvolutions) for upsampling, and the \nDiscriminator uses strided convolutions for downsampling."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 16,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Batch \nNormalization is extensively used, and specific activation functions (e.g., \nReLU in G, Leaky ReLU in D) are common. \n○ Working Principle: Applies the power of CNNs to learn hierarchical \nfeatures in images, improving the quality and stability of image generation \ncompared to vanilla GANs. \n○ Contribution: Established architectural guidelines and best practices for \nbuilding stable and effective GANs for visual data. \n○ Improvement Over: Significantly enhanced the stability and visual \nquality of generated images from vanilla GANs. \n3. Conditional GAN (cGAN): \n○ Architecture: Extends any base GAN (like Vanilla or DCGAN) by \nincorporating auxiliary \"conditional\" information (e.g., class labels, text \ndescriptions, or another image) into both the Generator and the \nDiscriminator."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 16,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "This conditioning is typically concatenated with the input \nnoise vector for G and with the data input for D. \n○ Working Principle: G learns to generate specific types of data \nconditioned on the provided input, while D learns to discriminate the \nauthenticity of the data given that condition. \n○ Contribution: Enables targeted and controlled data generation, allowing \nusers to specify characteristics of the desired output. \n○ Example Use: Generating an image of a \"cat\" when given the label \"cat,\" \nor transforming a sketch into a photo. \n4. Wasserstein GAN (WGAN) and WGAN-GP (Gradient Penalty): \nCO4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 17,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "17 \n \n○ Architecture: Primarily modifies the loss function and the \nDiscriminator's role. It replaces the original GAN's Jensen-Shannon \ndivergence-based loss with the Wasserstein distance (Earth Mover's \nDistance).The Discriminator is re-termed a \"Critic\" and outputs a raw \nscore (no sigmoid activation). WGAN-GP further refines this by adding a \n\"gradient penalty\" term to the Critic's loss, enforcing the Lipschitz \nconstraint more robustly than WGAN's weight clipping. \n○ Working Principle: The Wasserstein distance provides a more stable and \nmeaningful gradient, even when the real and fake data distributions are \nnon-overlapping, which was a major problem for earlier GANs."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 17,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Contribution: Drastically improved training stability and reduced mode \ncollapse, providing a reliable metric for generator convergence. \n○ Improvement Over: Addressed the core training instability and mode \ncollapse issues prevalent in Vanilla and DCGANs. \n5. CycleGAN: \n○ Architecture: Composed of two pairs of Generators and Discriminators, \ndesigned for unpaired image-to-image translation between two domains \n(e.g., Domain A and Domain B). It features a critical \"cycle consistency \nloss\" which ensures that translating an image from A to B and then back to \nA yields the original image A. \n○ Working Principle: The cycle consistency loss acts as a powerful \nregularization, allowing the model to learn mappings between domains \nwithout requiring perfectly aligned (paired) training data."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 17,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Contribution: Revolutionized image-to-image translation by removing \nthe stringent requirement for paired datasets, opening up new applications. \n○ Example Use: Transforming horses into zebras, changing summer scenes \nto winter scenes, or converting photos into specific artistic styles. \nii) Describe in detail the practical applications of GANs across different domains (6 \nMarks) \nGANs have transcended theoretical research to become powerful tools across a wide \narray of practical domains, fundamentally changing how we approach data generation and \nmanipulation. \n1. Image and Video Synthesis & Editing: \n○ Photorealistic Image Generation: GANs can create highly realistic images \nof objects, landscapes, and even human faces that are indistinguishable \nfrom real photos."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 17,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "This is used in generating synthetic data for AI training, \nvirtual photography for e-commerce, and creating unique visual assets for \nart and advertising.Examples include StyleGAN-generated faces used for \ndiverse online profiles. \n○ Image-to-Image Translation: This is a major application, enabling \nconversion between different image domains. \n■ Style Transfer: Applying artistic styles (e.g., Van Gogh's style) to \nregular photos (e.g., CycleGAN). \n■ Semantic Image Synthesis: Generating photorealistic images from \nsemantic layouts or sketches (e.g., turning a rough drawing of a cat \ninto a realistic cat photo). \n■ Domain Adaptation: Transforming images between different \nconditions, like converting day scenes to night scenes, or summer \nlandscapes to winter ones."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 17,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Photo Inpainting/Outpainting: Filling in missing or corrupted parts of an \n2. image with realistic content, or extending an image beyond its original"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 18,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "18 \n \nboundaries, widely used in photo restoration and creative content generation. \n3. Super-Resolution: Enhancing the resolution and detail of low-resolution images, \nvaluable in forensics, medical imaging, and improving video quality. \n4. Deepfakes (Ethical Concern): While demonstrating powerful generative \ncapabilities, the ability to create highly realistic fake videos or audio of \nindividuals has severe ethical implications concerning misinformation and \nimpersonation. \n2. Data Augmentation and Synthetic Data Generation: \n○ Medical Imaging: Generating synthetic medical scans (e.g., X-rays, MRIs) \nto augment limited datasets for training diagnostic AI models, especially \nfor rare diseases, thus improving model robustness and ensuring patient \nprivacy."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 18,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Autonomous Driving: Creating diverse synthetic road scenes, including \nrare events, adverse weather conditions, and various lighting scenarios, to \ntrain self-driving car algorithms more safely and comprehensively, \nreducing the need for costly and risky real-world data collection. \n○ Fraud Detection: Generating synthetic fraudulent transaction data to train \nmodels, particularly when real fraud cases are scarce but crucial for \neffective detection. \n○ Privacy Preservation: Generating synthetic versions of sensitive datasets \nthat retain statistical properties of the original data but contain no real \nindividual information, useful for sharing data with privacy concerns. \n3."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 18,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Entertainment and Creative Industries: \n○ Video Game Development: Automatically generating realistic textures, \ncharacters, landscapes, and entire game levels, significantly speeding up \ncontent creation pipelines. \n○ Film and Animation: Assisting in character design, generating background \nelements, creating special effects, and even synthesizing realistic crowd \nscenes. \n○ Generative Art: Creating unique and novel artworks, music, and even \narchitectural designs that explore new aesthetic possibilities, blurring the \nlines between human and AI creativity. \n4. Scientific Research and Design: \n○ Drug Discovery: Generating novel molecular structures with desired \nchemical properties, accelerating the search for new drugs and treatments."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 18,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Material Science: Designing new materials with specific characteristics by \nexploring the vast space of possible atomic and molecular configurations. \n○ Fashion Design: Generating new clothing designs, patterns, and virtual \ntry-on experiences, providing designers with a tool to rapidly prototype \nideas. \n5. Telecommunications and Networking: \n○ Network Anomaly Detection: Generating synthetic anomalous network \ntraffic to train intrusion detection systems, improving their ability to spot \nnovel threats. \nChannel Modeling: Simulating complex wireless communication channels \nto test and optimize communication protocols. \n \n \n(OR)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 19,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "19 \n \n \n(b) \nDescribe the different types of autoencoders and explain their architectures, objectives, \nand typical applications with suitable diagrams. \nDifferent Types of Autoencoders: \n1. Undercomplete Autoencoder (Basic Autoencoder) \n \n· Objective: To learn a low-dimensional representation (Z) of the input data such that \nthe reconstruction error between Xand X^ is minimized. The primary goal is \ndimensionality reduction and feature learning. \n· Typical Applications: \n● Dimensionality Reduction: Similar to Principal Component Analysis (PCA) but \ncan capture non-linear relationships. \n● Feature Learning: Extracting salient features from data. \n● Data Compression: Compressing data by storing its latent representation \n2."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 19,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Sparse Autoencoder ( Over complex ) \n \nObjective: To learn representations where only a small subset of hidden units are active \n(non-zero) for any given input. This encourages the network to learn specialized feature \ndetectors. \n● The loss function includes the reconstruction error plus a sparsity penalty. \nCommon sparsity penalties include: \n○ L1 Regularization: \n \n■ Where aj is the activation of the j-th hidden unit, and λ is the \nCO4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 20,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "20 \n \nsparsity regularization parameter. \n○ KL Divergence: \n \n■ Where ρ is the desired average activation (sparsity target), ρ^j is \nthe actual average activation of hidden unit j over a batch, and β is \nthe weight of the sparsity penalty. \nApplications: \n● Feature Learning: Learning more interpretable and distributed features. \n● Denoising: Can indirectly help with denoising as it focuses on salient features. \n● Representation Learning: Useful when a higher-dimensional but sparse \nrepresentation is desired. \n3. Denoising Autoencoder (DAE) \n \nApplications: \n● Denoising: Directly removing noise from images, audio, or other data. \n● Robust Feature Learning: Learning features that are more resilient to real-world \ndata imperfections."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 20,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● Data Imputation: Filling in missing values by treating missing data as noise. \n4. Variational Autoencoder (VAE)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 21,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "21 \n \n \nTypical Applications: \n● Generative Modeling: Generating new, realistic data samples (e.g., images, text, \nmusic). \n● Dimensionality Reduction: Learning a meaningful latent representation. \n● Anomaly Detection: Anomalous data points tend to have high reconstruction \nerrors or fall into low-density regions of the learned latent space. \nLatent Space Interpolation: Smoothly transitioning between different generated \nsamples by interpolating in the latent space \n \n \n \n \n15. \n(a) \nExplain bounding box prediction in YOLO algorithm with necessary diagrams. \nAnswer: \n \n \nCO5"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 26,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "26 \n \n \n \n \n \n(OR) \n \n \n(b) \n \nDiscuss the various region proposal methods used in object detection. Compare their \nefficiency and suitability for integration with modern object detection frameworks. \n \n \n \n \nCO5"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 29,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "29 \n \n \n \n \n \n \n \n PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \n \nCO \n16. \n(a) \nCreate the PyTorch implementation Denoising Autoencoder for the MNIST dataset. \ni) Load the MNIST dataset. ( 3 Mark) \nii) Create a model of Denoising Autoencoder ( 4 Mark) \niii) Compile the model with optimizer and loss function ( 3 Mark) \niv) Fit the model with training data and validation data. ( 5 Mark) \n \nAnswer: \n \n# (i) LOAD THE MNIST DATASET \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader, random_split \n \nCO4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 30,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "30    # 📁 Transforms: Normalize and Convert to Tensors  transform = transforms.Compose([   transforms.ToTensor(), # Converts to [0,1]  ])    # Download MNIST  train_dataset = datasets.MNIST(root='./data', train=True, download=True,  transform=transform)  test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)    # Split training into train/val  train_data, val_data = random_split(train_dataset, [50000, 10000])    # Dataloaders  train_loader = DataLoader(train_data, batch_size=128, shuffle=True)  val_loader = DataLoader(val_data, batch_size=128, shuffle=False)  test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # (ii) DENOISING AUTOENCODER MODEL  class DenoisingAutoencoder(nn.Module):   def __init__(self):"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 30,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "super(DenoisingAutoencoder, self).__init__()   self.encoder = nn.Sequential(   nn.Linear(28*28, 128),   nn.ReLU(),   nn.Linear(128, 64),   )   self.decoder = nn.Sequential(   nn.Linear(64, 128),   nn.ReLU(),   nn.Linear(128, 28*28),   nn.Sigmoid() # keep output in [0, 1]   )     def forward(self, x):   x = x.view(x.size(0), -1) # Flatten input   encoded = self.encoder(x)   decoded = self.decoder(encoded)   return decoded.view(x.size(0), 1, 28, 28)    # (iii) COMPILE THE MODEL  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  model = DenoisingAutoencoder().to(device)  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=0.001)    # Add Gaussian Noise  def add_noise(inputs, noise_factor=0.5):   noisy = inputs + noise_factor * torch.randn_like(inputs)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 30,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "return torch.clip(noisy, 0., 1.)    # (iv) TRAINING FUNCTION"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 31,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "31    def train_model(model, train_loader, val_loader, epochs=10):   for epoch in range(epochs):   model.train()   train_loss = 0   for images, _ in train_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)   optimizer.zero_grad()   loss.backward()   optimizer.step()   train_loss += loss.item()     model.eval()   val_loss = 0   with torch.no_grad():   for images, _ in val_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)   val_loss += loss.item()     print(f\"Epoch [{epoch+1}/{epochs}] ➤ Train Loss:  {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")    # Train the model  train_model(model,"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 31,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "train_loader, val_loader)          # (i) LOAD THE MNIST DATASET  import torch  import torch.nn as nn  import torch.optim as optim  from torchvision import datasets, transforms  from torch.utils.data import DataLoader, random_split    # 📁 Transforms: Normalize and Convert to Tensors  transform = transforms.Compose([   transforms.ToTensor(), # Converts to [0,1]  ])    # Download MNIST  train_dataset = datasets.MNIST(root='./data', train=True, download=True,  transform=transform)  test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 32,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "32    # Split training into train/val  train_data, val_data = random_split(train_dataset, [50000, 10000])    # Dataloaders  train_loader = DataLoader(train_data, batch_size=128, shuffle=True)  val_loader = DataLoader(val_data, batch_size=128, shuffle=False)  test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # (ii) DENOISING AUTOENCODER MODEL  class DenoisingAutoencoder(nn.Module):   def __init__(self):   super(DenoisingAutoencoder, self).__init__()   self.encoder = nn.Sequential(   nn.Linear(28*28, 128),   nn.ReLU(),   nn.Linear(128, 64),   )   self.decoder = nn.Sequential(   nn.Linear(64, 128),   nn.ReLU(),   nn.Linear(128, 28*28),   nn.Sigmoid() # keep output in [0, 1]   )     def forward(self, x):   x = x.view(x.size(0), -1) # Flatten input   encoded ="
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 32,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "self.encoder(x)   decoded = self.decoder(encoded)   return decoded.view(x.size(0), 1, 28, 28)    # (iii) COMPILE THE MODEL  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  model = DenoisingAutoencoder().to(device)  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=0.001)    # Add Gaussian Noise  def add_noise(inputs, noise_factor=0.5):   noisy = inputs + noise_factor * torch.randn_like(inputs)   return torch.clip(noisy, 0., 1.)    # (iv) TRAINING FUNCTION  def train_model(model, train_loader, val_loader, epochs=10):   for epoch in range(epochs):   model.train()   train_loss = 0   for images, _ in train_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 32,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "optimizer.zero_grad()   loss.backward()   optimizer.step()"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 33,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "33 \n \n train_loss += loss.item() \n \n model.eval() \n val_loss = 0 \n with torch.no_grad(): \n for images, _ in val_loader: \n images = images.to(device) \n noisy_imgs = add_noise(images) \n outputs = model(noisy_imgs) \n loss = criterion(outputs, images) \n val_loss += loss.item() \n \n print(f\"Epoch [{epoch+1}/{epochs}] ➤ Train Loss: \n{train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\") \n \n# Train the model \ntrain_model(model, train_loader, val_loader) \n \n \n \n \n \n \n(OR) \n \n \n(b) \nUsing the PyTorch implementation of a Restricted Boltzmann Machine (RBM), develop a \ncollaborative filtering-based recommender system on the MovieLens dataset."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 33,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Answers: \n# Imports \nimport numpy as np \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torch.utils.data import DataLoader \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split \n \n# (1) LOAD AND PREPROCESS MOVIELENS DATA \n# Download from: https://files.grouplens.org/datasets/movielens/ml-100k.zip and extract \nit. \n \nratings = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \n\"timestamp\"]) \nn_users = ratings.user_id.nunique() \nn_items = ratings.item_id.nunique() \n \n# Create user-item matrix \ndef create_user_item_matrix(df, n_users, n_items): \n data_matrix = np.zeros((n_users, n_items), dtype=np.float32) \n for row in df.itertuples(): \n data_matrix[row.user_id - 1, row.item_id - 1] = row.rating \nCO4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 34,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "34     return data_matrix    data = create_user_item_matrix(ratings, n_users, n_items)  train_data, test_data = train_test_split(data, test_size=0.2)    train_tensor = torch.FloatTensor(train_data)  test_tensor = torch.FloatTensor(test_data)    # (2) DEFINE RESTRICTED BOLTZMANN MACHINE (RBM)  class RBM(nn.Module):   def __init__(self, n_visible, n_hidden):   super(RBM, self).__init__()   self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)   self.h_bias = nn.Parameter(torch.zeros(n_hidden))   self.v_bias = nn.Parameter(torch.zeros(n_visible))     def sample_h(self, v):   wx = torch.matmul(v, self.W.t()) + self.h_bias   prob = torch.sigmoid(wx)   return prob, torch.bernoulli(prob)     def sample_v(self, h):   wx = torch.matmul(h, self.W) + self.v_bias   prob = torch.sigmoid(wx)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 34,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "return prob, torch.bernoulli(prob)     def forward(self, v):   h_prob, h_sample = self.sample_h(v)   v_prob, v_sample = self.sample_v(h_sample)   return v_prob     def contrastive_divergence(self, v0, k=1, lr=0.01):   v = v0.clone()   for step in range(k):   h_prob0, h0 = self.sample_h(v)   v_prob, v = self.sample_v(h0)   h_prob1, _ = self.sample_h(v)     # Update weights   self.W.data += lr * (torch.matmul(h_prob0.t(), v0) - torch.matmul(h_prob1.t(), v)) /  v0.size(0)   self.v_bias.data += lr * torch.sum(v0 - v, dim=0) / v0.size(0)   self.h_bias.data += lr * torch.sum(h_prob0 - h_prob1, dim=0) / v0.size(0)     loss = torch.mean((v0 - v) ** 2)   return loss    # ⚙ (3) COMPILE MODEL  n_visible = n_items  n_hidden = 64  rbm = RBM(n_visible, n_hidden)    # (4) TRAIN RBM MODEL  def"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 34,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "train_rbm(rbm, train_tensor, epochs=10, batch_size=64, lr=0.01):"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 35,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "35     train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)   for epoch in range(epochs):   total_loss = 0   for batch in train_loader:   batch = batch.clone()   batch[batch == 0] = -1 # mask unrated items   loss = rbm.contrastive_divergence(batch, k=1, lr=lr)   total_loss += loss.item()   print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")    train_rbm(rbm, train_tensor)    # (5) MAKE RECOMMENDATIONS  def recommend(rbm, user_vector, top_k=10):   with torch.no_grad():   input_vec = user_vector.clone()   input_vec[input_vec == 0] = -1 # mask missing ratings   output = rbm(input_vec)   predicted_ratings = output[0]   recommended_items = torch.argsort(predicted_ratings, descending=True)   return recommended_items[:top_k]    # Example: Recommend"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 35,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "for user 0  user_id = 0  user_vector = train_tensor[user_id].unsqueeze(0)  recommendations = recommend(rbm, user_vector)  print(\"Top recommended movie indices for user 0:\", recommendations.tolist())        # Imports  import numpy as np  import torch  import torch.nn as nn  import torch.optim as optim  from torch.utils.data import DataLoader  import pandas as pd  from sklearn.model_selection import train_test_split    # (1) LOAD AND PREPROCESS MOVIELENS DATA  # Download from: https://files.grouplens.org/datasets/movielens/ml-100k.zip and extract it   ratings = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\",  \"timestamp\"])  n_users = ratings.user_id.nunique()  n_items = ratings.item_id.nunique()    # Create user-item matrix  def create_user_item_matrix(df,"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 35,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "n_users, n_items):   data_matrix = np.zeros((n_users, n_items), dtype=np.float32)   for row in df.itertuples():   data_matrix[row.user_id - 1, row.item_id - 1] = row.rating"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 36,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "36     return data_matrix    data = create_user_item_matrix(ratings, n_users, n_items)  train_data, test_data = train_test_split(data, test_size=0.2)    train_tensor = torch.FloatTensor(train_data)  test_tensor = torch.FloatTensor(test_data)    # (2) DEFINE RESTRICTED BOLTZMANN MACHINE (RBM)  class RBM(nn.Module):   def __init__(self, n_visible, n_hidden):   super(RBM, self).__init__()   self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)   self.h_bias = nn.Parameter(torch.zeros(n_hidden))   self.v_bias = nn.Parameter(torch.zeros(n_visible))     def sample_h(self, v):   wx = torch.matmul(v, self.W.t()) + self.h_bias   prob = torch.sigmoid(wx)   return prob, torch.bernoulli(prob)     def sample_v(self, h):   wx = torch.matmul(h, self.W) + self.v_bias   prob = torch.sigmoid(wx)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 36,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "return prob, torch.bernoulli(prob)     def forward(self, v):   h_prob, h_sample = self.sample_h(v)   v_prob, v_sample = self.sample_v(h_sample)   return v_prob     def contrastive_divergence(self, v0, k=1, lr=0.01):   v = v0.clone()   for step in range(k):   h_prob0, h0 = self.sample_h(v)   v_prob, v = self.sample_v(h0)   h_prob1, _ = self.sample_h(v)     # Update weights   self.W.data += lr * (torch.matmul(h_prob0.t(), v0) - torch.matmul(h_prob1.t(), v)) /  v0.size(0)   self.v_bias.data += lr * torch.sum(v0 - v, dim=0) / v0.size(0)   self.h_bias.data += lr * torch.sum(h_prob0 - h_prob1, dim=0) / v0.size(0)     loss = torch.mean((v0 - v) ** 2)   return loss    # ⚙ (3) COMPILE MODEL  n_visible = n_items  n_hidden = 64  rbm = RBM(n_visible, n_hidden)    # (4) TRAIN RBM MODEL"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 37,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "37    def train_rbm(rbm, train_tensor, epochs=10, batch_size=64, lr=0.01):   train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)   for epoch in range(epochs):   total_loss = 0   for batch in train_loader:   batch = batch.clone()   batch[batch == 0] = -1 # mask unrated items   loss = rbm.contrastive_divergence(batch, k=1, lr=lr)   total_loss += loss.item()   print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")    train_rbm(rbm, train_tensor)    # (5) MAKE RECOMMENDATIONS  def recommend(rbm, user_vector, top_k=10):   with torch.no_grad():   input_vec = user_vector.clone()   input_vec[input_vec == 0] = -1 # mask missing ratings   output = rbm(input_vec)   predicted_ratings = output[0]   recommended_items = torch.argsort(predicted_ratings,"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 37,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "descending=True)   return recommended_items[:top_k]    # Example: Recommend for user 0  user_id = 0  user_vector = train_tensor[user_id].unsqueeze(0)  recommendations = recommend(rbm, user_vector)  print(\"Top recommended movie indices for user 0:\", recommendations.tolist())               ____________________            For Set 1 QP    Part - A  Question No."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 37,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK3 \nK4 \nK3 \nK2 \nK4 \nK4 \nK2 \nK2 \nK2 \nK3 \nDifficulty Level \n3 \n3 \n4 \n2 \n3 \n3 \n3 \n2 \n3 \n2 \n \nPart - B \nPart - C \nQuestion No. \n11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK2 \nK6 \nK6 \nK2 \nK6 \nK3 \nK3 \nK2 \nK2 \nK2 \nK6 \nK4 \nDifficulty Level \n3 \n3 \n4 \n3 \n3 \n3 \n3 \n3 \n3 \n3 \n4 \n3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 38,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "38 \n \nSET 2 (Answer Key) \n \n \n \nCommon to (AI-DS, AI-ML) \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nCreate the model for the following network using PyTorch. \n \nAnswer: \n \n \nCO1 \n2. \nHow does dropout regularization work in neural networks? \nAnswer: \nDropout regularization randomly disables a fraction of neurons during training to prevent \noverfitting. This forces the network to learn more robust and generalized features. \n \nCO1 \n3. \nSummarize the key concepts and principles that drive different strategies in transfer learning. \nAnswer: \nDeep Transfer Learning Strategies \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 39,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "39 \n \n• Direct use of pre-trained models \n• Leveraging feature extraction from pre-trained models \n• Fine-tuning layers of pre-trained models \n4. \nDetermine the convolution layer output shape if the input image shape is 128x128x3 and it uses 32 \nfilters of size 5x5 with a stride of 1 and padding of 2. \nAnswer: \n \nCO2 \n5. \nDistinguish between the phenomena of exploding gradients and vanishing gradients. \nAnswer: \nExploding gradients occur when weights grow too large during backpropagation, making training \nunstable. \n Vanishing gradients happen when gradients shrink, causing very slow or no learning, especially \nin deep RNNs. \nCO3 \n6. \nWhy do we need bidirectional RNNs in sequence modeling?"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 39,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Answer: \nBidirectional RNNs process data in both forward and backward directions, capturing past and future \ncontext, which improves performance in tasks like sentiment analysis and speech recognition. \nCO3 \n7. \nMention one key advantage of using denoising autoencoders for feature learning. \nAnswer: \nOne key advantage of using denoising autoencoders for feature learning is that they learn robust \nand meaningful features by reconstructing the original input from a corrupted version, helping the \nmodel generalize better and resist noise in real-world data. \nCO4 \n8. \nHow do the Generator and Discriminator in GANs engage in a competitive learning process? \nAnswer: \nThe generator tries to produce realistic data, while the discriminator tries to distinguish real \nfrom fake data."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 39,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Each improves by competing against the other in a minimax game. \nCO4 \n9. \nDifferentiate semantic segmentation and instance segmentation. \nAnswer: \nSemantic Segmentation classifies each pixel in an image into a category (e.g., sky, road), but does \nnot differentiate between objects of the same class. \nInstance Segmentation not only classifies each pixel but also distinguishes between individual \ninstances of the same class (e.g., multiple cars). \n \nCO5 \n10. \nDefine non-maximum suppression and explain how it helps refine the output of object detection \nmodels. \nAnswer: \nNon-Maximum Suppression (NMS) removes overlapping bounding boxes by keeping only the \none with the highest confidence score, helping to eliminate duplicate detections. \nCO5 \n \n PART B (5 x 13 = 65 marks)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 40,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "40 \n \n \n \n \nCO \n11. \n(a) \nWrite a PyTorch program to implement a linear regression model using the dataset where \ny=2X+1+e, with X ranging from 1 to 50 and e as random noise. Define a linear regression \nmodel, use Mean Squared Error (MSE) as the loss function, and optimize it with Stochastic \nGradient Descent (SGD). Train the model for 50 epochs, ensuring the inclusion of the \nforward pass, loss computation, backpropagation, and parameter updates. Finally, display \nthe learned parameters (weights and bias) after training \n \n \nCO1"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 41,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "41 \n \n \n \n \n \n \n(OR) \n \n \n(b) \n(i) Differentiate between shallow and deep neural networks. (6 Marks) \nCO1"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 42,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "42 \n \n \n(ii) Explain the concept of loss functions in deep learning. Discuss popular loss functions \nsuch as mean squared error (MSE), categorical cross-entropy, and binary cross-entropy. (7 \nMarks)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 43,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "43 \n \n \n \n \n \n \n \n \n \n12. \n(a) \nImplement transfer learning by using VGG as the base model to classify the CIFAR-100 \ndataset using PyTorch. \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 44,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "44 \n \ni) Load the CIFAR-100 dataset (3 Mark) \nii) Use a pre-trained VGG model as the base model and modify its classifier for \nCIFAR-100. (3 Mark) \niii) Create a sequential model with the appropriate number of neurons in the \noutput layer, activation function, and loss function (3 Mark) \niv) Train the model with training data and validate it using the test dataset, and \nevaluate its accuracy. (4 Mark)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 45,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "45 \n \n \n \n \n \n(OR) \n \n \n(b) \nExplain data augmentation and its various techniques, providing illustrative examples for \neach approach. \nData augmentation \n· Data augmentation is a process of artificially increasing the amount of data by \ngenerating new data points from existing data. \n· Data augmentation includes adding minor alterations to data or using machine \nlearning models to generate new data points in the latent space of original data to \namplify the dataset. \nSynthetic data: When data is generated artificially without using real-world images."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 45,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Synthetic data \nare often produced by Generative Adversarial Networks \nAugmented data: Derived from original images with some sort of minor geometric \ntransformations (such as flipping, translation, rotation, or the addition of noise) in order to increase \nthe diversity of the training set. \nData Augmentation Techniques \n· Flip \n· Rotation \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 46,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "46 \n \n· Scale \n· Crop \n· Translation \n· Gaussian Noise \nFlip \n· Images can be flipped horizontally and vertically. \n· A vertical flip is equivalent to rotating an image by 180 degrees and then performing \na horizontal flip. \n \nRotation \n· One key thing to note about this operation is that image dimensions may not be \npreserved after rotation. \n· If your image is a square, rotating it at right angles will preserve the image size. \n· If it’s a rectangle, rotating it by 180 degrees would preserve the size. \n· Rotating the image by finer angles will also change the final image size. \n \n· The image can be scaled outward or inward. \n· While scaling outward, the final image size will be larger than the original image \nsize. \n· Scaling inward reduces the image size."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 46,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Crop \n· Unlike scaling, we just randomly sample a section from the original image. \n· We then resize this section to the original image size. \n· This method is popularly known as random cropping."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 47,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "47 \n \nTranslation \n· Translation just involves moving the image along the X or Y direction (or both). \n· This method of augmentation is very useful as most objects can be located at almost \nanywhere in the image. \n \nGaussian Noise \nOver-fitting usually happens when your neural network tries to learn high frequency features \n(patterns that occur a lot) that may not be useful. Gaussian noise, which has zero mean, essentially \nhas data points in all frequencies, effectively distorting the high frequency features. This also means \nthat lower frequency components (usually, your intended data) are also distorted, but your neural \nnetwork can learn to look past that. Adding just the right amount of noise can enhance the learning \ncapability. \n \n \n \n \n \n \n \n13."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 47,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "(a) \nDiscuss the vanishing and exploding gradient problems in RNNs. How do LSTM and \nGRU architectures mitigate these issues? \nVanishing and Exploding Gradient Problems in RNNs: \n● Vanishing Gradient Problem: \n○ Description: During backpropagation through time (BPTT), gradients \nshrink exponentially as they are propagated back through many time steps. \nThis is because the derivatives of activation functions (like sigmoid, \ncommonly used in older RNNs) are often small (e.g., between 0 and 0.25 \nfor sigmoid). When these small values are multiplied together over many \nlayers/time steps (due to the chain rule), the gradient quickly approaches \nzero."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 47,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Impact: Small or vanishing gradients make it difficult for the network to \nlearn long-term dependencies, meaning information from earlier parts of a \nsequence has little influence on updates to weights that affect later parts. \nThis prevents the RNN from capturing relationships between distant \nelements in a sequence, such as words far apart in a long sentence. \n● Exploding Gradient Problem: \n○ Description: While less emphasized in the provided text, the chain rule \ncan also lead to gradients becoming extremely large. This happens when \nthe values being multiplied in the chain rule are consistently large, causing \nthe gradient to grow exponentially. \n○ Impact: Exploding gradients can lead to unstable training, where the \nCO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 48,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "48 \n \nmodel weights receive very large updates, causing the learning process to \ndiverge and the model to perform poorly. \nHow LSTM and GRU Architectures Mitigate These Issues: \nLSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) architectures were \nspecifically designed to address the vanishing and exploding gradient problems prevalent \nin simple RNNs. They achieve this through their unique gating mechanisms: \n● LSTMs (Long Short-Term Memory RNNs): \n \n○ LSTMs introduce a \"cell state\" and various \"gates\" (input gate, forget gate, \nand output gate) that regulate the flow of information. \n○ The cell state acts as a long-term memory, capable of carrying information \nacross many time steps without significant degradation."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 48,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ The forget gate determines what information to discard from the cell state, \npreventing old, irrelevant information from accumulating. \n○ The input gate controls which new information gets stored in the cell \nstate. \n○ The output gate controls what part of the cell state is outputted as the \nhidden state. \n○ These gates, implemented with sigmoid and tanh activation functions, \nallow LSTMs to selectively remember or forget information, creating a \n\"constant error carousel\" that helps gradients flow more effectively over \nlong distances, thus mitigating the vanishing gradient problem. \n● GRUs (Gated Recurrent Unit RNNs): \n \n○ GRUs are a simplified version of LSTMs, designed to be computationally \nmore efficient while still effectively addressing the gradient problems."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 48,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ They combine the forget and input gates into a single \"update gate\" and \nalso have a \"reset gate.\" \n○ The update gate determines how much of the past information (from the \nprevious hidden state) should be passed on to the current hidden state and"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 49,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "49 \n \nhow much new information should be incorporated. \n○ The reset gate decides how much of the previous hidden state to forget \nbefore computing the new hidden state. \n○ By having fewer gates than LSTMs, GRUs offer a more streamlined \nstructure that still allows for better control over information flow and helps \nto preserve gradients over longer sequences, combating both vanishing and \nexploding gradients. \nBoth LSTMs and GRUs enable RNNs to capture long-term dependencies that simple \nRNNs struggle with, making them highly effective for sequential data tasks like text \ngeneration and speech recognition. \n \n \n(OR) \n \n \n(b) \nExplain BiRNN and the need for bidirectional traversal with the Pytorch implementation. \n· BiRNN stands for Bidirectional Recurrent Neural Network."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 49,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "· It is an extension of a standard RNN where two RNNs are run: \n· One processes the input forward (from beginning to end). \n· The other processes the input backward (from end to beginning). \n· The outputs from both directions are then combined (usually concatenated or \nsummed) at each time step. \n \n· Consider the sentence: \n\"Michael Eats Dosa in Chennai\" \n· To classify words like \"dosa\", the model needs past context (\"eats\") \nand for \"Chennai\", the model needs future context (\"in\"). \nBenefits of BiRNNs: \n· Better context understanding in sequence tasks (e.g., language modeling, \nspeech recognition). \n· Improved accuracy in tasks like: \no Named Entity Recognition \nCO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 50,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "50    o Part-of-Speech Tagging  o Sentiment Analysis  o Machine Translation    import torch  import torch.nn as nn    class BiRNN(nn.Module):   def __init__(self, input_size, hidden_size, output_size, num_layers=1):   super(BiRNN, self).__init__()   self.hidden_size = hidden_size   self.num_layers = num_layers     # Bidirectional RNN   self.rnn = nn.RNN(input_size, hidden_size, num_layers,   batch_first=True, bidirectional=True)     # The output layer uses hidden_size * 2 because of bidirection   self.fc = nn.Linear(hidden_size * 2, output_size)     def forward(self, x):   # Initialize hidden state: (num_layers * 2, batch_size, hidden_size)   h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)     # Forward propagate RNN   out, _ = self.rnn(x, h0)     # Pass the last time"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 50,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "step output from both directions to the FC layer   out = self.fc(out[:, -1, :]) # shape: (batch_size, output_size)   return out      model = BiRNN(input_size, hidden_size, output_size)            14."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 50,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "(a) \nExplain Sparse Autoencoders using L1 regularization and KL divergence. Also, \nimplement a sparse autoencoder with PyTorch implementation. \nAutoencoders are a type of neural network that learns a compressed, distributed \nrepresentation (encoding) of input data in an unsupervised manner. They consist of two \nmain parts: an encoder that maps the input to a hidden (latent) representation, and a \ndecoder that reconstructs the input from this hidden representation. The goal is to minimize \nthe reconstruction error between the input and the output. \nSparse Autoencoders \nWhile standard autoencoders aim to learn an efficient representation, they can sometimes \nCO4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 51,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "51 \n \nlearn the identity function if the hidden layer has enough capacity, leading to poor feature \nextraction and overfitting. Sparse autoencoders address this by introducing a sparsity \nconstraint on the hidden layer activations. This constraint encourages the hidden units to \nbe mostly inactive (their activations are close to zero) for any given input. This forces the \nnetwork to learn a more distributed representation where different hidden units specialize \nin detecting different features in the input data. \nThere are primarily two common ways to enforce sparsity in autoencoders: \n1. L1 Regularization \n2."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 51,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "KL Divergence \nL1 Regularization for Sparsity \nL1 regularization (also known as Lasso regularization) adds a penalty term to the loss \nfunction that is proportional to the absolute value of the hidden unit activations. \nThe total loss function for an L1 sparse autoencoder is given by: \n \nWhere: \n● x is the input data. \n● x′ is the reconstructed output. \n● ∥x−x′∥2 is the reconstruction loss, often Mean Squared \nError (MSE). \n● λ is the regularization parameter, controlling the strength of the sparsity penalty. \nA larger λ promotes more sparsity. \n● aj is the activation of the j-th hidden unit. \n● s is the number of hidden units."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 51,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "By penalizing the sum of absolute activations, L1 regularization encourages many of the \naj values to become exactly zero, effectively \"turning off\" a significant portion of the \nhidden units for a given input. This directly promotes sparsity. \nKL Divergence for Sparsity \nInstead of directly penalizing the absolute activations, KL divergence enforces sparsity by \nencouraging the average activation of each hidden unit to be close to a predefined small \nvalue, ρ (rho), which represents the desired sparsity level (e.g., 0.05 or 0.1). \nThe Kullback-Leibler (KL) divergence measures the difference between two probability \ndistributions."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 51,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "In the context of sparse autoencoders, we treat the average activation of a \nhidden unit as a Bernoulli random variable with a probability of activation ρ^j, and we want \nthis to be close to a target Bernoulli distribution with probability ρ."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 52,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "52 \n \nThe KL divergence term for a single hidden unit j is: \n \nWhere: \n● β is the weight of the sparsity penalty, controlling its influence on the total loss. \n● The other terms are as defined for L1 regularization. \nThe KL divergence term is minimized when ρ^j is close to ρ. If ρ^j is much larger than ρ, \nthe penalty increases, pushing ρ^j down."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 52,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "If ρ^j is much smaller than ρ, the penalty also  increases (though less drastically for very small ρ^j), encouraging some activation    import torch.nn.functional as F  from torchvision import datasets, transforms  from torch.utils.data import DataLoader    # Hyperparameters  input_size = 784 # For MNIST  hidden_size = 128  batch_size = 64  learning_rate = 1e-3  num_epochs = 10  l1_lambda = 1e-5 # L1 regularization coefficient    # Dataset  transform = transforms.ToTensor()  train_loader = DataLoader(   datasets.MNIST(root='./data', train=True, download=True, transform=transform),   batch_size=batch_size,   shuffle=True  )    # Autoencoder with L1 sparsity  class SparseAutoencoder(nn.Module):   def __init__(self):   super(SparseAutoencoder, self).__init__()   self.encoder ="
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 52,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "nn.Linear(input_size, hidden_size)   self.decoder = nn.Linear(hidden_size, input_size)     def forward(self, x):   x = x.view(-1, input_size)   hidden = torch.sigmoid(self.encoder(x))   output = torch.sigmoid(self.decoder(hidden))   return output, hidden    model = SparseAutoencoder()  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 53,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "53 \n \n# Training \nfor epoch in range(num_epochs): \n total_loss = 0 \n for batch, _ in train_loader: \n batch = batch.view(-1, input_size) \n optimizer.zero_grad() \n outputs, hidden = model(batch) \n mse_loss = criterion(outputs, batch) \n l1_loss = l1_lambda * torch.sum(torch.abs(hidden)) \n loss = mse_loss + l1_loss \n loss.backward() \n optimizer.step() \n total_loss += loss.item() \n \n print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\") \n \n \n \n \n(OR) \n \n \n(b) \nExplain the architecture and working of Generative Adversarial Networks (GANs). \n \nA GAN consists of two primary neural networks: \n1. Generator (G): This network is responsible for creating new data samples (e.g., \nimages, text, audio) that are as realistic as possible, mimicking the real data \ndistribution."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 53,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "It takes a random noise vector as input, often sampled from a \nsimple distribution like a Gaussian or uniform distribution. This noise vector \nacts as the \"creative seed\" for the generator. The generator's architecture \ntypically involves layers that upsample this noise into the desired data format \n(e.g., transposed convolutional layers for image generation). \n2. Discriminator (D): This network acts as a \"critic\" or \"authenticator.\" It takes \nan input data sample and outputs a probability (a value between 0 and 1) \nindicating whether it believes the sample is real (from the actual training \ndataset) or fake (generated by the generator)."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 53,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "The discriminator is typically a \nstandard classifier, often using convolutional layers for image data, that learns \nto distinguish between real and fake inputs. \nHow GANs Work (The Adversarial Process) \nThe working principle of GANs can be likened to a game between a forger (the \nGenerator) and a detective (the Discriminator): \n● The Forger (Generator): Tries to create fake banknotes (data) that are so \nconvincing the detective can't tell them apart from real ones. \n● The Detective (Discriminator): Tries to become an expert at telling the \ndifference between real banknotes and fake ones. \nThis competition drives both networks to improve: \n1. Generator's Goal: To produce data that is indistinguishable from real data, \nthereby \"fooling\" the discriminator."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 53,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Its objective is to maximize the \nprobability of the discriminator making a mistake (classifying generated data \nCO4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 54,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "54 \n \nas real). \n2. Discriminator's Goal: To accurately distinguish between real and fake data. \nIts objective is to minimize the probability of making mistakes (correctly \nclassifying real data as real and generated data as fake) \n \n \n \n \n \n \n \n15. \n(a) \nExplain the architecture of Fast R-CNN for object detection. Describe the role of Selective \nSearch, ROI Pooling, and the two output branches in the detection pipeline. \n \n \nCO5"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 55,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "55 \n \n \n \n \n \n \n \n \n \n(OR) \n \n \n(b) \nExplain the working of a Single Shot Detector (SSD) and discuss how it effectively \ndetects objects of varying sizes in an image. \nCO5"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 56,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "56 \n \n \n \n \n \n \n \n PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \nCO \n16. \n(a) \nCreate the PyTorch implementation for the stock price prediction using RNN. \n(i) Implement the input preprocessing . (4 Marks) \n(ii) Implement a stock price prediction class with an appropriate loss function and optimizer \nfor multi-class classification. (6 Marks) \n(iii) Implement a function to train the model using the training dataset. (5 Marks) \n \n \n \n \n \nCO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 57,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "57        # Import Libraries  import numpy as np  import torch  import torch.nn as nn  from sklearn.preprocessing import MinMaxScaler  from sklearn.model_selection import train_test_split    # (i) INPUT PREPROCESSING — 4 Marks  np.random.seed(42)  prices = np.cumsum(np.random.randn(1000) * 2 + 0.1) + 100 # Random walk    def label_movement(prices):   movement = []   for i in range(1, len(prices)):   diff = prices[i] - prices[i - 1]   if diff > 0.5:   movement.append(2) # Up   elif diff < -0.5:   movement.append(0) # Down   else:   movement.append(1) # No Change   return np.array(movement)    labels = label_movement(prices)  scaler = MinMaxScaler()  scaled_prices = scaler.fit_transform(prices[:-1].reshape(-1, 1)) # exclude last price    def create_sequences(data, labels, seq_len=10):   X, y"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 57,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "= [], []   for i in range(len(data) - seq_len):   X.append(data[i:i+seq_len])   y.append(labels[i+seq_len])   return np.array(X), np.array(y)    X, y = create_sequences(scaled_prices, labels)  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)  X_train = torch.tensor(X_train, dtype=torch.float32)  X_test = torch.tensor(X_test, dtype=torch.float32)  y_train = torch.tensor(y_train, dtype=torch.long)  y_test = torch.tensor(y_test, dtype=torch.long)    # (ii) MODEL DEFINITION + LOSS + OPTIMIZER — 6 Marks  class StockRNN(nn.Module):   def __init__(self, input_size=1, hidden_size=64, num_layers=1, num_classes=3):   super(StockRNN, self).__init__()   self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)   self.fc = nn.Linear(hidden_size,"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 57,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "num_classes)   def forward(self, x):   out, _ = self.rnn(x)   out = out[:, -1, :] # Last timestep   return self.fc(out)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 58,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "58    model = StockRNN()  criterion = nn.CrossEntropyLoss()  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)    # (iii) TRAINING FUNCTION — 5 Marks  def train_model(model, X_train, y_train, epochs=20):   model.train()   for epoch in range(epochs):   optimizer.zero_grad()   output = model(X_train)   loss = criterion(output, y_train)   loss.backward()   optimizer.step()   _, preds = torch.max(output, 1)   acc = (preds == y_train).float().mean()   print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Accuracy:  {acc.item():.4f}\")    train_model(model, X_train, y_train)    # (Optional) EVALUATION  def evaluate(model, X_test, y_test):   model.eval()   with torch.no_grad():   output = model(X_test)   _, preds = torch.max(output, 1)   acc = (preds == y_test).float().mean()"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 58,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "print(f\"Test Accuracy: {acc.item():.4f}\")    evaluate(model, X_test, y_test)          (OR)      (b)    Develop a Named Entity Recognition (NER) system using PyTorch with the following  requirements:  (i) Implement an LSTM-based neural network architecture  (ii) Include proper data preprocessing for sequence labeling  (iii) Compile the model with appropriate loss function and optimizer  (iv) Train the model and evaluate its performance    # Imports  import torch  import torch.nn as nn  from collections import defaultdict  from sklearn.metrics import classification_report  CO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 59,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "59      # Sample Data (Tiny Dataset for Demo)  train_data = [   [(\"John\", \"B-PER\"), (\"lives\", \"O\"), (\"in\", \"O\"), (\"New\", \"B-LOC\"), (\"York\", \"ILOC\")],   [(\"Mary\", \"B-PER\"), (\"is\", \"O\"), (\"from\", \"O\"), (\"Paris\", \"B-LOC\")],  ]  val_data = [   [(\"Steve\", \"B-PER\"), (\"works\", \"O\"), (\"in\", \"O\"), (\"London\", \"B-LOC\")]  ]    # Preprocessing - Build Vocabulary  word2idx = defaultdict(lambda: len(word2idx)); tag2idx = defaultdict(lambda:  len(tag2idx))  PAD, UNK = \"<PAD>\", \"<UNK>\"; word2idx[PAD]; word2idx[UNK]; tag2idx[PAD]  for sentence in train_data:   for word, tag in sentence:   word2idx[word]; tag2idx[tag]  max_len = max(len(s) for s in train_data + val_data)    # Encode Sentences  def encode(sentence, word2idx, tag2idx, max_len):   words = [word for word, tag in sentence]   tags = [tag for word,"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 59,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "tag in sentence]   word_ids = [word2idx.get(w, word2idx[UNK]) for w in words]   tag_ids = [tag2idx[t] for t in tags]   while len(word_ids) < max_len:   word_ids.append(word2idx[PAD])   tag_ids.append(tag2idx[PAD])   return word_ids, tag_ids    X_train = torch.tensor([encode(s, word2idx, tag2idx, max_len)[0] for s in train_data])  y_train = torch.tensor([encode(s, word2idx, tag2idx, max_len)[1] for s in train_data])  X_val = torch.tensor([encode(s, word2idx, tag2idx, max_len)[0] for s in val_data])  y_val = torch.tensor([encode(s, word2idx, tag2idx, max_len)[1] for s in val_data])    # Define LSTM-based NER Model  class NERLSTM(nn.Module):   def __init__(self, vocab_size, tagset_size, embedding_dim=64, hidden_dim=128):   super(NERLSTM, self).__init__()   self.embedding ="
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 59,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "nn.Embedding(vocab_size, embedding_dim,  padding_idx=word2idx[PAD])   self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,  bidirectional=True)   self.fc = nn.Linear(hidden_dim * 2, tagset_size)   def forward(self, x):   x = self.embedding(x)   x, _ = self.lstm(x)   return self.fc(x)    # Compile Model: Loss + Optimizer  model = NERLSTM(len(word2idx), len(tag2idx))  criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[PAD])  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 60,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "60      # Train Model  def train(model, X, y, epochs=10):   model.train()   for epoch in range(epochs):   optimizer.zero_grad()   out = model(X).view(-1, len(tag2idx)) # (batch*seq_len, num_tags)   loss = criterion(out, y.view(-1))   loss.backward()   optimizer.step()   print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")  train(model, X_train, y_train)    # Evaluate Model  def evaluate(model, X, y_true):   model.eval()   with torch.no_grad():   y_pred = torch.argmax(model(X), dim=2)   true_labels, pred_labels = [], []   for true, pred in zip(y_true, y_pred):   for t, p in zip(true, pred):   if t != tag2idx[PAD]:   true_labels.append(t.item())   pred_labels.append(p.item())   idx2tag = {v: k for k, v in tag2idx.items()}   print(classification_report(   [idx2tag[i] for i in true_labels],"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 60,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "[idx2tag[i] for i in pred_labels],   target_names=[k for k in tag2idx if k != PAD]   ))  evaluate(model, X_val, y_val)       _____________________              For Set 2 QP    Part - A  Question No."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 60,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK6 \nK2 \nK2 \nK3 \nK4 \nK2 \nK2 \nK4 \nK4 \nK2 \nDifficulty Level \n3 \n3 \n2 \n4 \n3 \n2 \n2 \n3 \n3 \n3 \n \n Part - B \nPart - C \nQuestion No. \n11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK3 \nK4 \nK6 \nK2 \nK3 \nK3 \nK3 \nK2 \nK4 \nK2 \nK6 \nK6 \nDifficulty Level \n3 \n3 \n4 \n3 \n3 \n3 \n3 \n3 \n3 \n3 \n4 \n4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 61,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "61 \n \nSET 3 (Answer Key) \n \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nHow does the choice of a loss function impact the training of a neural network? \nAnswer: \nSelecting the right loss function is crucial in machine learning as it directly impacts the model's \ntraining and performance.The loss function measures the difference between the predicted and \nactual values, guiding the optimization process to improve the model \nCO1 \n2. \nHow does batch normalization help in accelerating the training of deep neural networks? \nAnswer: \nDropout regularization randomly disables a fraction of neurons during training to prevent \noverfitting."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 61,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "This forces the network to learn more robust and generalized features. \nCO1 \n3. \nHow does data augmentation enhance the robustness of machine learning models? \nAnswer: \n• It reduces the cost of collection of data. \n• It reduces the cost of labelling data. \n• It improves the model prediction accuracy. \n• It prevents data scarcity. \n• It frames better data models. \n• It reduces data overfitting. \n• It creates variability and flexibility in data models. \n• It increases the generalization ability of the data models. \n• It helps in resolving the class imbalance issue in the classification \nCO2 \n4. \nDepict and explain the multiple filter processing for a three-channel image. \nAnswer: \n \nCO2 \n5. \nName the three gates in LSTM and state one function of each."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 61,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Answer: \n• Input gate: Controls what new information is stored. \n• Forget gate: Decides which old information to discard. \n• Output gate: Regulates the output from the current cell state. \nCO3 \n6. \nDifferentiate between one-to-many and many-to-many RNN architectures with examples. \nAnswer: \nLSTM has 3 gates: input, forget, and output; separates memory cell and hidden state. \nCO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 62,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "62 \n \n \n \n \n PART B (5 x 13 = 65 marks) \n \n \n \n \n \nCO \n11. \n(a) \nDiscuss the significance of selecting appropriate loss functions for regression, binary \nclassification, and multiclass classification tasks, and demonstrate their application using \nPyTorch. \nAnswer: \n● Selecting the right loss function is crucial in machine learning as it directly impacts the \nmodel's training and performance. The loss function measures the difference between the \npredicted and actual values, guiding the optimization process to improve the model. \n1. Loss Functions for Regression Tasks \n \nIn regression problems, the output is a continuous numerical value. The most commonly \nused loss functions are: \nCO1 \nGRU has 2 gates: update and reset; combines memory and hidden state, making it faster. \n7."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 62,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Define Minimax Loss and Wasserstein Loss \nAnswer: \nMinimax Loss is used in standard GANs, where the generator tries to minimize and the \ndiscriminator tries to maximize the probability of correctly classifying real vs. fake data. \nWasserstein Loss, used in WGANs, replaces probability-based loss with the Earth Mover’s \nDistance, providing more stable training and better gradient flow. \nCO4 \n8. \nWhat is the function of the discriminator in a Generative Adversarial Network (GAN)? \nAnswer: \n• The function of the discriminator in a Generative Adversarial Network (GAN) is to \ndistinguish between real and fake data."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 62,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "• It evaluates whether a given sample is from the real dataset or generated by the generator, \nand provides feedback to help both networks improve — encouraging the generator to \nproduce more realistic data. \nCO4 \n9. \nDifferentiate between R-CNN and SPPNet. \nAnswer: \n \nCO5 \n10. \nDefine panoptic segmentation. How does it combine the strengths of semantic and instance \nsegmentation? \nAnswer: \nPanoptic segmentation assigns both a class label and an instance ID to each pixel. It combines \nsemantic segmentation (classifying every pixel) with instance segmentation (differentiating \nobject instances). \nCO5"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 63,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "63 \n \n· Mean Squared Error (MSE): Penalizes large errors more due to squaring. It is \nsensitive to outliers. \n \nWhere: n: number of examples or samples. y: true values. ^y: predicted values. \n● · \nsmaller value indicates that the ground truth and predicted values are closer \nto each other. \nImplementation : (using Class Based Approach) \n \nOr Implementation: (using API) \n \n· Mean Absolute Error (MAE): Less sensitive to outliers as it considers absolute \ndifferences. \n \nWhere: n: number of examples or samples. y: true values. ^y: predicted values. \n· Here also, a lower value indicates a better model \nImplementation : (using Class Based Approach)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 64,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "64 \n \nImplementation: (using API) \n \n2. Loss Function for Binary Classification Tasks \nBinary classification problems involve two classes (e.g., spam vs. not spam). The most \ncommonly used loss function is: \n● Binary Cross-Entropy (BCE) Loss: Measures the difference between the predicted \nprobability and the actual class. \nImplementation : (using Class Based Approach) \n \nImplementation: (using API) \n \n3. Loss Function for Multiclass Classification Tasks \nFor multiclass classification problems where an instance belongs to one of multiple classes, the \ncommonly used loss function is: \n● Categorical Cross-Entropy (CCE) / Cross-Entropy Loss: Used for one-hot \nencoded labels and softmax outputs. \n \np is the softmax probability vector from the model's logits, and y is the ground truth label."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 65,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "65 \n \nImplementation: (using API) \n \n \n \n(OR) \n \n \n(b) \n(i) Using PyTorch, implement a neural network to predict 'CompressiveStrength' using the \nremaining columns as input features. Create a model with three hidden layers, each having \n512 units and the ReLU activation, an output layer with one unit and no activation, and also \ninput_shape should be specified in the first layer. Apply the necessary preprocessing before \ntraining. (7Marks) \n(ii) Explain the concept of regularization in deep learning. Discuss common regularization \ntechniques such as L1 and L2 regularization, dropout, and batch normalization. (6 Marks) \n \n \nCO1"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 67,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "67 \n \n \n(ii) Explain the concept of regularization in deep learning. Discuss common regularization \ntechniques such as L1 and L2 regularization, dropout, and batch normalization. (6 Marks) \nRegularization is a technique used in deep learning to prevent overfitting — a situation \nwhere a model performs well on training data but poorly on unseen data. Regularization \nintroduces additional information or constraints to the model, helping it generalize better to \nnew data. \nCommon Regularization Techniques: \n1. L1 Regularization (Lasso): \n○ Adds the absolute value of the weights to the loss function\n \n○ Encourages sparsity by driving some weights to zero. \n○ Useful for feature selection. \n2. L2 Regularization (Ridge):"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 68,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "68 \n \n○ Adds the squared value of the weights to the loss function: \n \n○ Prevents large weights, promoting simpler models. \n○ Helps in weight decay and smoother generalization. \n3. Dropout: \n○ Randomly \"drops out\" (sets to zero) a percentage of neurons during training. \n○ Prevents co-adaptation of neurons. \n○ Forces the network to learn redundant, robust features. \n4. Batch Normalization: \n○ Normalizes the output of a layer for each mini-batch. \n○ Reduces internal covariate shift, improving training speed and stability. \nHas a regularizing effect and can sometimes reduce the need for dropout. \n \n \n \n \n12."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 68,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "(a) \n(i) Develop the PyTorch implementation of the model with the input shape of (512,512,1) \n(7 Marks) \n \nimport torch \nimport torch.nn as nn \nclass CustomCNN(nn.Module): \n def __init__(self): \n super(CustomCNN, self).__init__() \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 69,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "69     # Convolutional Layers   self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1,  padding='same') # Output: (32, 512, 512)   self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1,  padding='same') # Output: (64, 512, 512)   # Flatten layer   self.flatten = nn.Flatten() # Output: (64 * 512 * 512 = 16777216)   # Fully Connected Layer   self.dense = nn.Linear(16777216, 6) # Output: (6)   def forward(self, x):   x = torch.relu(self.conv1(x))   x = torch.relu(self.conv2(x))   x = self.flatten(x)   x = self.dense(x)   return x  # Instantiate the model  model = CustomCNN()  # Print model summary  print(model)  # Print total parameters  total_params = sum(p.numel() for p in model.parameters())  print(f\"Total trainable parameters:"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 69,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "{total_params}\")    (ii) Explain the significance of the convolution layer in deep learning models, particularly  its role in feature extraction and pattern recognition, and provide insights into its impact?"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 69,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "(6 Marks) \nThe convolutional layer plays a pivotal role in deep learning models, especially in tasks \nrelated to computer vision and image analysis."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 70,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "70 \n \nFeature Extraction: \nConvolutional layers are designed to automatically learn and extract meaningful features \nfrom input data. In computer vision, these features could be edges, textures, shapes, or \nother higher-level patterns. \nThey use small kernels (also known as filters) that slide across the input data, capturing \nlocal information and producing feature maps. Each filter is responsible for detecting a \nspecific pattern or feature. \nThe hierarchical structure of convolutional layers allows the network to learn complex \nfeatures by combining simple ones, which is crucial for understanding and representing the \nunderlying structure of the data. \nPattern Recognition: \nConvolutional layers excel at pattern recognition in images."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 70,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "By learning hierarchical \nfeatures, they can recognize intricate patterns, such as objects, shapes, or textures, at \nvarious levels of abstraction. \nThrough the use of multiple convolutional layers followed by pooling layers, deep learning \nmodels can gradually build up a hierarchy of features, enabling them to recognize \nincreasingly complex patterns and objects. \nThe ability to automatically learn these patterns from data makes convolutional layers \nhighly adaptable and capable of generalizing well to new, unseen examples. \n \n \n(OR) \n \n \n(b) \nDescribe the concept of Convolutional Neural Networks (CNNs) and their role in computer \nvision tasks. Explain the key components of CNNs, including convolutional layers, pooling \nlayers, and fully connected layers."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 70,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Discuss how CNNs automatically learn hierarchical \npatterns and extract meaningful features from images. \nDefinition: \nA Convolutional Neural Network (CNN) is a specialized deep learning architecture \ndesigned for processing grid-like data (e.g., images, videos). CNNs automatically learn \nhierarchical features through convolutional operations, making them the backbone of \nmodern computer vision. \nRole in Computer Vision: \nCNNs excel at tasks like: \n● Image Classification (e.g., identifying objects in photos). \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 71,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "71 \n \n● Object Detection (e.g., localizing multiple objects in an image). \n● Semantic Segmentation (e.g., pixel-wise labeling of images). \n● Face Recognition, Medical Imaging, and more. \nKey Components of CNNs \n1. Convolutional Layers \n● Purpose: Extract local features (e.g., edges, textures) via learnable filters (kernels). \n● Operation: \n○ Slides a kernel (e.g., 3×3) across the input, computing dot products. \n○ Outputs a feature map highlighting detected patterns. \n● Parameters: \n○ kernel_size: Spatial dimensions of the filter (e.g., 3×3). \n○ stride: Step size of the kernel (default=1). \n○ padding: Preserves spatial dimensions (e.g., padding='same'). \n● Activation: ReLU introduces nonlinearity (e.g., ReLU(conv(x))). \n2."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 71,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Pooling Layers \n● Purpose: Reduce spatial dimensions (downsampling) to: \n○ Decrease computational cost. \n○ Introduce translation invariance. \n● Types: \n○ Max Pooling: Selects the maximum value in a window (e.g., 2×2). \n○ Average Pooling: Takes the mean value in a window. \n● Output: Smaller but deeper feature maps. \n3. Fully Connected (Dense) Layers \n● Purpose: Classify features extracted by convolutional/pooling layers. \n● Operation: Flattens the 3D feature maps into a 1D vector for traditional MLP-like \nclassification. \n● Use Case: Final layers in CNNs (e.g., for ImageNet’s 1000-class output)."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 72,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "72 \n \nHow CNNs Learn Hierarchical Patterns \n1. Low-Level Features: \n○ Early layers detect edges, colors, and textures (via small kernels). \n2. Mid-Level Features: \n○ Deeper layers combine edges into shapes (e.g., circles, corners). \n3. High-Level Features: \n○ Final layers recognize complex objects (e.g., \"cat ears\" or \"car wheels\"). \nExample: \n● Input Image → Conv1 (Edges) → Conv2 (Textures) → Conv3 (Object Parts) → \nOutput (Full Object). \n \nWhy CNNs Outperform Traditional Methods \n● Parameter Sharing: Kernels are reused across the image, reducing parameters. \n● Local Connectivity: Focuses on local regions, not the entire image. \n● Hierarchical Learning: Mimics human visual perception (simple → complex)."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 72,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Applications of CNNs \nTask \nExample \nImage Classification \nResNet on ImageNet \nObject Detection \nYOLO, Faster R-CNN \nMedical Imaging \nTumor detection in MRI scans \nAutonomous Driving \nLane and pedestrian detection \n \n \n \n \n \n13. \n(a) \nExplain the necessity of word embedding in Natural Language Processing (NLP) and \nCO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 73,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "73 \n \noutline its various methodologies. \n· Natural Language Processing (NLP) aims to enable computers to understand, \ninterpret, and generate human language.However, computers fundamentally operate on \nnumbers, not words. This is where word embeddings become indispensable. \nTraditional methods of representing words, like one-hot encoding or Bag-of-Words \n(BoW), have significant limitations: \n● Sparsity and High Dimensionality: One-hot encoding creates a vector where each \nword is represented by a unique dimension, resulting in extremely sparse (mostly \nzeros) and high-dimensional vectors, especially for large vocabularies. This is \ncomputationally expensive and inefficient."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 73,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● Lack of Semantic Meaning: Traditional methods treat each word as an \nindependent entity, failing to capture the semantic relationships between words. For \nexample, \"king\" and \"queen\" are just different words, with no inherent connection \nunderstood by the machine. Similarly, \"cat\" and \"feline\" would be seen as distinct, \neven though they are semantically similar. \n● No Contextual Understanding: These methods disregard the context in which a \nword appears. The same word can have different meanings depending on its \nsurrounding words (e.g., \"bank\" of a river vs. a financial \"bank\"), but traditional \nmethods assign a single representation."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 73,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● Inability to Generalize: If a model is trained on \"fish\" but encounters \"cod\" during \ntesting, it won't understand the semantic similarity if it hasn't seen \"cod\" before, \nleading to poor generalization. \nWord embeddings address these issues by providing dense, low-dimensional, and \nsemantically rich numerical representations of words. They bridge the gap between \nhuman language and machine understanding by: \n● Capturing Semantic and Syntactic Relationships: Words with similar meanings \nor that appear in similar contexts are mapped to vectors that are close to each other \nin the vector space. This allows for arithmetic operations (e.g., \"king\" - \"man\" + \n\"woman\" ≈ \"queen\"), demonstrating an understanding of analogies and \nrelationships."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 73,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● Dimensionality Reduction: Instead of sparse, high-dimensional vectors, word \nembeddings represent words as dense vectors of much lower dimensions (typically \ntens to hundreds). This significantly reduces computational complexity and"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 74,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "74 \n \nmemory requirements. \n● Contextual Understanding (in advanced embeddings): Newer embedding \ntechniques can generate different vectors for the same word based on its context, \neffectively handling polysemy (words with multiple meanings). \n● Improved Performance in Downstream NLP Tasks: By providing a richer and \nmore efficient representation of words, embeddings significantly boost the \nperformance of various NLP tasks, including: \no Text Classification: Sentiment analysis, spam detection, topic categorization. \n○ Named Entity Recognition (NER): Identifying people, organizations, \nlocations. \n○ Machine Translation: Understanding word relationships across languages. \n○ Information Retrieval and Search Engines: More accurate matching of \nqueries to relevant documents."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 74,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Question Answering: Better understanding of questions and answers. \n○ Text Generation: Creating coherent and contextually relevant text. \nVarious Methodologies of Word Embedding \n· Word embedding methodologies can be broadly categorized into: \n1. Frequency-Based Methods \nThese methods rely on statistical measures of word co-occurrence within a corpus. \n● Bag-of-Words (BoW): \n○ Concept: Represents a document as a multiset of its words, disregarding \ngrammar and word order. It simply counts the frequency of each word in a \ndocument. \n○ Limitations: High dimensionality, sparsity, and no semantic meaning or \ncontextual understanding. It treats \"good\" and \"bad\" as equally distinct as \n\"cat\" and \"dog\"."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 74,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● Term Frequency-Inverse Document Frequency (TF-IDF): \n○ Concept: A statistical measure that evaluates how relevant a word is to a \ndocument in a collection of documents. It increases with the number of \ntimes a word appears in the document but is offset by the frequency of the"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 75,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "75 \n \nword in the corpus. This helps to down-weight common words (like \"the\", \n\"a\") that provide little unique information. \n○ Limitations: Still results in sparse vectors and doesn't capture semantic \nrelationships between words directly. \n2. Prediction-Based (Neural Network-based) Methods \n· These methods learn word embeddings by training neural networks to predict words \nbased on their context, or vice-versa. \n● Word2Vec: \n○ Concept: Developed by Google, Word2Vec is a group of shallow, twolayer neural network models trained to reconstruct the linguistic context of \nwords. It learns to map semantically similar words to geometrically close \nembedding vectors."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 75,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "It has two main architectures: \n■ Continuous Bag-of-Words (CBOW): Predicts the current word \nbased on its surrounding context words. It's generally faster and \nperforms well for frequent words. \n■ Skip-gram: Predicts the surrounding context words given a target \nword. It's better at capturing rare words and phrases. \n○ Key Feature: Captures semantic relationships (e.g., King - Man + Woman \n≈ Queen). \n● GloVe (Global Vectors for Word Representation): \n○ Concept: Developed by Stanford, GloVe combines the advantages of both \nglobal matrix factorization (like Latent Semantic Analysis) and local \ncontext window methods (like Word2Vec). It constructs word embeddings \nby analyzing global word-word co-occurrence statistics from a large corpus."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 75,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "It explicitly models the ratio of co-occurrence probabilities. \n○ Key Feature: Captures both global statistical information and local \ncontextual information. \n● FastText: \n○ Concept: Developed by Facebook AI Research, FastText extends \nWord2Vec by treating each word as a \"bag of character n-grams.\" This \nmeans it considers subword information."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 76,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "76 \n \n○ Key Feature: Effective for morphologically rich languages (where words \nhave many forms due to prefixes/suffixes) and can handle out-of-vocabulary \n(OOV) words by inferring their embeddings from their character n-grams. \n3. Contextualized Embeddings \nThese are more advanced methods that generate dynamic word embeddings, meaning the \nvector representation of a word changes based on its surrounding context within a \nsentence. This effectively addresses the issue of polysemy. \n● ELMo (Embeddings from Language Models): \n○ Concept: ELMo is a deep contextualized word representation that models \nboth complex characteristics of word use (e.g., syntax and semantics) and \nhow these uses vary across linguistic contexts (e.g., to model polysemy)."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 76,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "It \nuses a bidirectional LSTM to generate word embeddings. \n○ Key Feature: Provides different vectors for a word depending on its usage \nin a sentence. \n● BERT (Bidirectional Encoder Representations from Transformers): \n○ Concept: Developed by Google, BERT is a transformer-based model that \ngenerates context-aware embeddings by considering the context of words \nfrom both left and right sides simultaneously. It's pre-trained on large text \ncorpora using tasks like Masked Language Modeling (predicting masked \nwords) and Next Sentence Prediction. \n○ Key Feature: Bidirectional context understanding, leading to highly \nnuanced word representations. It has revolutionized many NLP tasks."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 76,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● GPT (Generative Pre-trained Transformer) series (GPT-2, GPT-3, GPT-4, \netc.): \n○ Concept: While primarily known for text generation, GPT models also \nproduce powerful contextualized embeddings. They are transformer-based \nand pre-trained to predict the next word in a sequence. Although primarily \nunidirectional in their original form, their vast pre-training on diverse data \nenables them to learn rich representations. \n○ Key Feature: Excellent for language generation and various downstream \nNLP tasks through fine-tuning."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 77,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "77 \n \nWord Method \nExample Vectors (Illustrative) \nOne-hot \nhappy = [1,0,0]; excited = [0,1,0]; angry=[0,0,1] \nBag of Words \nSentence: \"happy happy angry\" → [2,0,1] \nTF-IDF \neights words based on rarity \nWord2Vec (CBOW/Skip-Gram) \nhappy = [0.8,0.6]; excited=[0.75,0.7]; angry=[-0.6,-\n0.8] \n \n \n \n(OR) \n \n \n(b) \nExplain the Long Short-Term Memory (LSTM) cell, utilizing a diagram and PyTorch \nimplementation to illustrate its workings. \nLSTM (Long Short-Term Memory) is a special type of Recurrent Neural Network (RNN) \ncell designed to remember information over long sequences and avoid the vanishing \ngradient problem in standard RNNs. \n· It does so by maintaining a cell state that runs through the sequence with minimal \nchanges, controlled by gates that decide what to keep, update, or forget."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 77,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "An LSTM cell has 3 main gates: \n· Forget Gate (fₜ): Decides what information to discard from the cell state. \n· Input Gate (iₜ): Decides what new information to add to the cell state. \n· Output Gate (oₜ): Decides what to output based on the updated cell state. \nAnd an internal candidate cell state (𝒞̃ₜ) that suggests new info to add. \n \nAn LSTM's core strength lies in its \"gates,\" which regulate the flow of information into \nand out of the cell state, allowing it to remember or forget information over long \nsequences. \n1. Forget Gate Layer (First Step): \nCO3"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 78,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "78 \n \n \n \no Purpose: To decide what information from the previous cell state (Ct−1) should be \ndiscarded. \no Mechanism: A sigmoid layer looks at the previous hidden state (ht−1) and the current \ninput (xt). \no Output: Generates a number between 0 and 1 for each number in Ct−1. \n§ '1' means \"completely keep this.\" \n§ '0' means \"completely get rid of this.\" \no Example (Language Model): If the cell state contains the gender of the previous \nsubject, and a new subject is encountered, the forget gate will output '0' for the old subject's \ngender, effectively forgetting it. \n2. Input Gate Layer & Tanh Layer (Second Step): \n \no Purpose: To decide what new information will be stored in the current cell state (Ct)."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 78,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "This step has two parts: \no Input Gate Layer: \n§ Mechanism: A sigmoid layer examines ht−1 and xt. \n§ Output: Decides which new values will be updated (represented by it). \no Tanh Layer (Candidate Values):"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 79,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "79 \n \n§ Mechanism: A tanh layer also looks at ht−1 and xt. \n§ Output: Creates a vector of new candidate values (C~t) that could potentially be added \nto the state. These values are scaled between -1 and 1. \no Combined Goal: These two components work together to determine what new \ninformation is relevant to incorporate. \no Example (Language Model): This is where the LSTM identifies and prepares to add \nthe gender of the new subject to the cell state. \n2. Updating the Cell State (Third Step): \n \no Purpose: To transform the old cell state (Ct−1) into the new cell state (Ct) by \nincorporating the decisions made by the forget and input gates. \no Mechanism: \n§ The old cell state (Ct−1) is multiplied by the forget gate's output (ft), effectively \ndiscarding the unwanted information."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 79,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "§ The new candidate values (C~t) are scaled by the input gate's output (it) (meaning only \nthe selected new information is considered). \n§ These two results are then added together to form the new cell state (Ct=ft∗Ct−1+it∗C~t\n). \no Example (Language Model): This is the moment where the old subject's gender \ninformation is actually removed, and the new subject's gender information is actively \nincorporated into the cell state. \n2. Output Gate Layer (Fourth Step):"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 80,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "80 \n \n \no Purpose: To decide what information from the current cell state (Ct) will be output as \nthe hidden state (ht). \no Mechanism: \n§ An output sigmoid layer looks at ht−1 and xt to determine which parts of the cell state \nare relevant for output. \n§ The new cell state (Ct) is passed through a tanh function (to push values between -1 and \n1). \n§ The tanh output of Ct is then multiplied element-wise by the output of the sigmoid \noutput gate. This filters the cell state, only outputting the relevant parts. \no Example (Language Model): After processing a subject, the LSTM might output \ninformation relevant to a verb (e.g., whether the subject is singular or plural), which would \nguide the conjugation of the next verb in the sentence."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 80,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "import torch \nimport torch.nn as nn \n \nclass LSTMCell(nn.Module): \n def __init__(self, input_size, hidden_size): \n super(LSTMCell, self).__init__() \n self.input_size = input_size \n self.hidden_size = hidden_size \n \n # Linear layers for input, forget, cell, and output gates \n self.i2h = nn.Linear(input_size, 4 * hidden_size) \n self.h2h = nn.Linear(hidden_size, 4 * hidden_size) \n \n def forward(self, x, hidden): \n h_prev, c_prev = hidden # hidden state and cell state"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 81,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "81 \n \n # Concatenate x and h_prev then apply linear transformations \n gates = self.i2h(x) + self.h2h(h_prev) \n i_gate, f_gate, c_gate, o_gate = gates.chunk(4, dim=1) \n \n i = torch.sigmoid(i_gate) \n f = torch.sigmoid(f_gate) \n o = torch.sigmoid(o_gate) \n g = torch.tanh(c_gate) \n \n c_next = f * c_prev + i * g \n h_next = o * torch.tanh(c_next) \n \n return h_next, c_next \n \n \n \n \n \n \n14. \n(a) \nExplain the fundamental architectural components of Autoencoders in neural networks, \noutlining their roles and functions, and demonstrate these concepts through a PyTorch \nimplementation. \n \nCO4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 82,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "82 \n \n \n \n \n \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \n \n# 1. Define the Autoencoder Architecture \nclass Autoencoder(nn.Module): \n def __init__(self, input_dim, hidden_dim): \n super(Autoencoder, self).__init__()"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 83,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "83 \n \n \n # Encoder \n self.encoder = nn.Sequential( \n nn.Linear(input_dim, hidden_dim), # Input layer to hidden layer \n nn.ReLU(True), # Non-linear activation \n # You can add more layers here for a deeper encoder \n ) \n \n # Decoder \n self.decoder = nn.Sequential( \n nn.Linear(hidden_dim, input_dim), # Hidden layer to output layer \n nn.Sigmoid() # Sigmoid for output (pixel values between 0 and 1) \n ) \n \n def forward(self, x): \n encoded = self.encoder(x) \n decoded = self.decoder(encoded) \n return decoded \n \n# 2."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 83,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Hyperparameters and Data Loading \nINPUT_DIM = 28 * 28 # For MNIST images (28x28 pixels) \nHIDDEN_DIM = 128 # Dimensionality of the latent space (bottleneck) \nBATCH_SIZE = 64 \nNUM_EPOCHS = 20 \nLEARNING_RATE = 1e-3 \n \n# MNIST Dataset loading \ntransform = transforms.Compose([ \n transforms.ToTensor(), # Convert PIL Image to PyTorch Tensor \n transforms.Normalize((0.5,), (0.5,)) # Normalize pixel values to [-1, 1] \n]) \n \n# Download and load training data \ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, \ntransform=transform) \ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n \n# 3."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 83,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Model, Loss, and Optimizer Initialization \nmodel = Autoencoder(INPUT_DIM, HIDDEN_DIM) \ncriterion = nn.MSELoss() # Mean Squared Error for reconstruction loss \noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) \n \n# Move model to GPU if available \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \nmodel.to(device) \n \nprint(f\"Using device: {device}\") \nprint(model) # Print model architecture \n \n# 4. Training Loop \nprint(\"Starting training...\") \nfor epoch in range(NUM_EPOCHS): \n total_loss = 0 \n for batch_idx, (data, _) in enumerate(train_loader):"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 84,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "84 \n \n # Flatten the images from (batch_size, 1, 28, 28) to (batch_size, 784) \n data = data.view(-1, INPUT_DIM).to(device) \n \n # Zero the gradients \n optimizer.zero_grad() \n \n # Forward pass \n reconstructed_data = model(data) \n \n # Calculate loss \n loss = criterion(reconstructed_data, data) \n \n # Backward pass and optimize \n loss.backward() \n optimizer.step() \n \n total_loss += loss.item() \n \n avg_loss = total_loss / len(train_loader) \n print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\") \n \nprint(\"Training finished.\") \n \n# 5."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 84,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Visualization of Reconstruction (Optional)  model.eval() # Set model to evaluation mode  with torch.no_grad():   # Get a batch of test images   test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)   test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)     data_iter = iter(test_loader)   images, _ = next(data_iter)     # Flatten images and move to device   original_images = images.view(-1, INPUT_DIM).to(device)     # Reconstruct images   reconstructed_images = model(original_images).cpu() # Move back to CPU for  plotting   original_images = original_images.cpu()     # Unnormalize images for plotting (assuming original normalization was (0.5, 0.5))   # reconstructed_images = reconstructed_images * 0.5 + 0.5   # original_images ="
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 84,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "original_images * 0.5 + 0.5     # Plot original and reconstructed images   fig, axes = plt.subplots(2, 10, figsize=(20, 4))   for i in range(10):   # Original Images   axes[0, i].imshow(original_images[i].reshape(28, 28), cmap='gray')   axes[0, i].axis('off')   # Reconstructed Images   axes[1, i].imshow(reconstructed_images[i].reshape(28, 28), cmap='gray')"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 85,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "85 \n \n axes[1, i].axis('off') \n \naxes[0, 0].set_title(\"Original\") \n axes[1, 0].set_title(\"Reconstructed\") \n plt.show() \n \n \n \n \n(OR) \n \n \n(b) \nDevelop a PyTorch-based implementation to train a Generative Adversarial Network (GAN) \non the MNIST dataset. Include an appropriate training loop, loss functions, and optimizer \nsetup."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 85,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "# PyTorch GAN on MNIST - Full Implementation \n \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \n \n# Generator Network \nclass Generator(nn.Module): \n def __init__(self, latent_dim): \n super(Generator, self).__init__() \n self.model = nn.Sequential( \n nn.Linear(latent_dim, 128), \n nn.ReLU(True), \n nn.Linear(128, 256), \n nn.BatchNorm1d(256), \n nn.ReLU(True), \n nn.Linear(256, 512), \n nn.BatchNorm1d(512), \n nn.ReLU(True), \n nn.Linear(512, 784), \n nn.Tanh() \nCO4"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 86,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "86     )   def forward(self, z):   img = self.model(z)   return img.view(z.size(0), 1, 28, 28)    # Discriminator Network  class Discriminator(nn.Module):   def __init__(self):   super(Discriminator, self).__init__()   self.model = nn.Sequential(   nn.Linear(784, 512),   nn.LeakyReLU(0.2, inplace=True),   nn.Linear(512, 256),   nn.LeakyReLU(0.2, inplace=True),   nn.Linear(256, 1),   nn.Sigmoid()   )   def forward(self, img):   img_flat = img.view(img.size(0), -1)   return self.model(img_flat)    # Hyperparameters and DataLoader  latent_dim = 100  batch_size = 128  lr = 0.0002  epochs = 50  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    transform = transforms.Compose([   transforms.ToTensor(),   transforms.Normalize([0.5], [0.5])  ])  mnist ="
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 86,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 87,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "87    dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True)    # Initialize networks and optimizers  generator = Generator(latent_dim).to(device)  discriminator = Discriminator().to(device)  optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))  optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))  adversarial_loss = nn.BCELoss()    # Training Loop  for epoch in range(epochs):   for i, (imgs, _) in enumerate(dataloader):   real_imgs = imgs.to(device)   valid = torch.ones(imgs.size(0), 1, device=device)   fake = torch.zeros(imgs.size(0), 1, device=device)     # Train Generator   optimizer_G.zero_grad()   z = torch.randn(imgs.size(0), latent_dim, device=device)   gen_imgs = generator(z)   g_loss ="
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 87,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "adversarial_loss(discriminator(gen_imgs), valid)   g_loss.backward()   optimizer_G.step()     # Train Discriminator   optimizer_D.zero_grad()   real_loss = adversarial_loss(discriminator(real_imgs), valid)   fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)   d_loss = (real_loss + fake_loss) / 2   d_loss.backward()   optimizer_D.step()     if i % 100 == 0:"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 88,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "88 \n \n print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(dataloader)} \\ \n Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\") \n \n# Visualize generated images \ndef generate_images(generator, n=25): \n z = torch.randn(n, latent_dim).to(device) \n gen_imgs = generator(z).cpu().detach() \n gen_imgs = gen_imgs.view(n, 1, 28, 28) \n fig, axes = plt.subplots(5, 5, figsize=(5, 5)) \n for i, ax in enumerate(axes.flatten()): \n ax.imshow(gen_imgs[i].squeeze(), cmap=\"gray\") \n ax.axis(\"off\") \n plt.tight_layout() \n plt.show() \n \ngenerate_images(generator) \n \n \n \n \n \n15. \n(a) \nDescribe object classification and localization. How do modern object detection models \nperform both tasks simultaneously? Support your answer with suitable architectural \nexamples. \n \nCO5"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 89,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "89 \n \n \n \n \nobject detection models can be categorized into two types: \n1. Two-Stage Detectors: These models typically perform the object detection \nprocess in two distinct steps: \n○ Region Proposal: In the first stage, the model generates a set of \"region \nproposals\" or \"regions of interest (RoIs)\" that are likely to contain objects. \nThis is essentially the localization part of the initial guess. \n○ Classification and Bounding Box Regression: In the second stage, these \nproposed regions are then fed into a classification network to determine the \nobject's class and a regression network to refine the bounding box \ncoordinates, making them more precise. \n2. One-Stage Detectors: These models perform both tasks (classification and \nlocalization) in a single forward pass through the network."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 89,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "They directly predict \nbounding boxes and class probabilities for various locations across the image. \nThis approach generally leads to faster inference times, making them suitable for \nreal-time applications. \nHere's how they achieve simultaneous operation: \n· Shared Feature Extraction Backbone: Both two-stage and one-stage detectors \ntypically start with a convolutional neural network (CNN) backbone (e.g., \nResNet, VGG, Darknet) that extracts rich, hierarchical features from the input \nimage. These features are then shared by both the classification and localization \nbranches. The early layers capture low-level features (edges, textures), while \ndeeper layers capture high-level semantic features."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 89,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "· Anchor Boxes / Priors: Many models utilize \"anchor boxes\" (also known as prior \nboxes or default boxes). These are predefined bounding box shapes and sizes at \nvarious locations across the image. The model then predicts offsets from these \nanchor boxes (for localization) and class probabilities for each anchor box (for \nclassification).This allows the model to predict multiple objects at different scales"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 90,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "90 \n \nand aspect ratios simultaneously. \n· Grid-based Prediction (in One-Stage Detectors): Models like YOLO divide the \ninput image into a grid. Each grid cell is responsible for predicting objects whose \ncenter falls within that cell. For each cell, it predicts a fixed number of bounding \nboxes, their confidence scores (objectness: whether an object is present in that \nbox), and class probabilities. This directly links spatial information (grid cell) to \nthe object's presence and class. \n· Multi-task Loss Function: The training of these models involves a composite \nloss function that simultaneously optimizes for both classification accuracy and \nbounding box regression accuracy."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 90,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "○ Classification Loss: Typically cross-entropy loss, measures how well the \nmodel predicts the correct class label for each object. \n○ Localization (Regression) Loss: Often L1 or L2 loss (or more advanced \nforms like IoU loss), measures the difference between the predicted bounding \nbox coordinates and the ground truth bounding box coordinates. \n· Non-Maximum Suppression (NMS): After the model generates numerous \nbounding box predictions (many overlapping), NMS is applied as a postprocessing step. It removes redundant and less confident bounding boxes, \nkeeping only the most confident and representative ones for each detected object. \nSuitable Architectural Examples: \n1."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 90,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "YOLO (You Only Look Once) - A prominent One-Stage Detector \n \n● Architecture: YOLO models (YOLOv1, YOLOv2, YOLOv3, YOLOv4, YOLOv5, \nYOLOv7, YOLOv8, etc.) are known for their speed. They treat object detection as \na regression problem. \n○ The input image is divided into an S×S grid. \n○ Each grid cell predicts B bounding boxes, a confidence score for each box \n(indicating the probability of an object being present in that box), and C \nconditional class probabilities (probability of an object being a specific \nclass, given that an object is present). \n○ The final output is a tensor of size S×S×(B×5+C). The 5 for each bounding \nbox corresponds to (x,y,w,h) coordinates and objectness score."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 90,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● Simultaneous Operation: YOLO directly predicts both the bounding box \ncoordinates and class probabilities for each grid cell in a single forward pass."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 91,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "91 \n \nThere's no separate region proposal stage. The network learns to directly map \nimage pixels to bounding box parameters and class scores. \n2. Faster R-CNN - A classic Two-Stage Detector \n \n● Architecture: Faster R-CNN significantly improved upon its predecessors (R-CNN, \nFast R-CNN) by introducing a Region Proposal Network (RPN). \n○ Backbone CNN: Extracts a feature map from the input image. \n○ Region Proposal Network (RPN): This is a small convolutional network that \nslides over the feature map generated by the backbone. At each slidingwindow location, it predicts two things: \n■ Objectness Score: Whether a region contains an object or not (binary \nclassification)."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 91,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "■ Bounding Box Refinements: Adjustments to a set of predefined \n\"anchor boxes\" at that location to better fit potential objects. \n○ RoI Pooling / RoI Align: The proposed regions (RoIs) from the RPN are \nthen pooled (or aligned, for more precise results) to a fixed size feature \nvector. \n○ Classification and Regression Head: These fixed-size feature vectors are fed \ninto a fully connected layer (or small CNNs) with two output branches: \n■ Classification Head: Predicts the actual class probabilities for each \nRoI. \n■ Bounding Box Regression Head: Further refines the bounding box \ncoordinates for each classified object."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 91,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "● Simultaneous Operation: Faster R-CNN processes the image in two stages, but the \nRPN and the detection network share the same convolutional features, making it an \nend-to-end trainable system. The RPN provides the initial \"localization\" proposals, \nand the subsequent stage refines these localizations and performs the final \n\"classification.\" \n3. SSD (Single Shot MultiBox Detector) - Another One-Stage Detector"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 92,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "92 \n \n \n● Architecture: SSD is also a one-stage detector that aims for speed and accuracy. \n○ It uses a VGG or ResNet-like backbone for feature extraction. \n○ It predicts bounding boxes and class probabilities at multiple scales (from \ndifferent convolutional layers) of the feature map. This is achieved by using \na set of default (anchor) boxes with varying scales and aspect ratios at each \nfeature map location. \n● Simultaneous Operation: Similar to YOLO, SSD performs classification and \nlocalization simultaneously by directly predicting offsets to default boxes and their \ncorresponding class scores from feature maps at various resolutions. This multiscale prediction helps in detecting objects of different sizes effectively."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 92,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "(OR) \n \n \n(b) \nExplain how Region Proposal Networks (RPN) are integrated into Faster R-CNN, and \nevaluate its performance in terms of speed and accuracy compared to previous models. \nA Region Proposal Network (RPN) is a specialized neural network that predicts object \nregions or \"proposals\" in an image.Its integration into the Faster R-CNN (Region-based \nConvolutional Neural Network) architecture marked a pivotal moment in object detection, \ncreating a unified, end-to-end deep learning model that significantly outpaced its \npredecessors in both speed and accuracy. \nThe core innovation of the RPN is that it shares the powerful, deep convolutional features of \nthe main object detection network, making the region proposal step nearly instantaneous."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 92,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "This resolved the major computational bottleneck present in earlier models like R-CNN and \nFast R-CNN, which relied on slow, external algorithms like Selective Search to generate \npotential object locations. \nIntegration into Faster R-CNN Architecture \nCO5"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 93,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "93 \n \n \nThe Faster R-CNN model is composed of two main modules that share a common set of \nconvolutional layers (the \"backbone,\" e.g., VGG or ResNet): \n1. Region Proposal Network (RPN): This network's sole job is to identify a set of highquality rectangular region proposals that are likely to contain objects. \n2. Fast R-CNN Detector: This network takes the proposed regions from the RPN and \nperforms final classification (e.g., \"person,\" \"car\") and refines the bounding box \ncoordinates. \nThe integration works as follows: \n1. Shared Feature Map: An input image is first processed by the backbone CNN, producing \na single, high-level feature map. This feature map is the crucial link, as it's used by both the \nRPN and the final detector. \n2."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 93,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "Anchor Boxes: The RPN slides a small network over this feature map. At each location, \nit considers multiple predefined bounding boxes of different scales and aspect ratios, known \nas anchor boxes. \n3. Dual Outputs: For each anchor box, the RPN outputs two predictions: \n○ An objectness score: The probability that an anchor contains any object \nversus being background. \n○ Bounding box regression: Adjustments to the anchor's coordinates to make it \nfit a potential object more tightly. \n4. Proposal Hand-off: The highest-scoring region proposals from the RPN are then passed \nto the second stage. An RoI (Region of Interest) Pooling layer extracts a fixed-size feature"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 94,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "94 \n \nvector for each proposal from the same shared feature map. \n5. Final Detection: These feature vectors are fed into the Fast R-CNN module, which \nmakes the final class prediction and further refines the bounding box for each object. \nThis unified architecture allows the RPN to be trained jointly with the detector, enabling it \nto learn how to generate proposals that are specifically tailored for the main detection \nnetwork, thereby improving overall accuracy. \n \n· The most significant impact of the RPN was on speed. By replacing the slow CPUbased Selective Search algorithm (which took ~2 seconds per image) with a lightweight \nneural network running on the GPU, the time spent on generating proposals plummeted to \nabout 10 milliseconds."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 94,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "This elimination of the primary bottleneck allowed the entire \ndetection pipeline to be nearly real-time, making it over 10 times faster than Fast R-CNN \nand about 250 times faster than the original R-CNN. \n· While speed was the headline improvement, accuracy also saw a notable boost. Unlike \nSelective Search, which is a fixed, hand-engineered algorithm, the RPN is a trainable \nnetwork. By training it jointly with the detection network, the RPN learns to generate \nproposals that are optimized for the detector and the specific classes in the dataset. This \nability to learn high-quality, data-specific proposals helps the detector perform better, \nleading to a higher mean Average Precision (mAP) compared to models that use static \nproposal methods."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 94,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \n \nCO"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 95,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "95 \n \n16. \n(a) \nImplement transfer learning by using VGG as the base model to classify the CIFAR-10 \ndataset using PyTorch. \ni) Load the CIFAR-10 dataset. (3 Mark) \nii) Use a pre-trained VGG model as the base model (4 Mark) \niii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function. (4 Mark) \niv) Train the model with training data and validation data. (4 Mark) \n \n \n \n \nCO2 \n \n \n(OR)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 96,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "96 \n \n \n(b) \nThe MNIST dataset consists of 70000 28x28 grayscale digit images in 10 classes. There \nare 60000 training images and 10000 test images. Develop a PyTorch implementation for \nthe following. \ni) Load the MNIST dataset. (4 Mark) \nii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function. (5 Mark) \niii) Train the model with training data and validate it using the test dataset, and \nevaluate its accuracy."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 96,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "(6 Mark) \n#Import Libraries \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nimport torchvision \nimport torchvision.transforms as transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \nimport numpy as np \nfrom sklearn.metrics import confusion_matrix, classification_report \nimport seaborn as sns \n \n# Data Preprocessing \ntransform = transforms.Compose([ \n transforms.ToTensor(), \n transforms.Normalize((0.5,), (0.5,)) # Normalize images \n]) \n \n# Load MNIST dataset \nCO2"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 97,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "97    train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True,  transform=transform, download=True)  test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False,  transform=transform, download=True)    # Check dataset  image, label = train_dataset[0]  print(\"Image shape:\", image.shape)  print(\"Number of training samples:\", len(train_dataset))    image, label = test_dataset[0]  print(\"Image shape:\", image.shape)  print(\"Number of testing samples:\", len(test_dataset))    # Create DataLoaders  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)    # Define the CNN Model  class CNNClassifier(nn.Module):   def __init__(self):   super(CNNClassifier, self).__init__()   self.conv1 ="
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 97,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3,  padding=1)   self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,  padding=1)   self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,  padding=1)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 98,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "98     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)   self.fc1 = nn.Linear(128 * 3 * 3, 128) # Adjusted for MNIST image size   self.fc2 = nn.Linear(128, 64)   self.fc3 = nn.Linear(64, 10) # 10 classes in MNIST     def forward(self, x):   x = self.pool(torch.relu(self.conv1(x)))   x = self.pool(torch.relu(self.conv2(x)))   x = self.pool(torch.relu(self.conv3(x)))   x = x.view(x.size(0), -1) # Flatten the tensor   x = torch.relu(self.fc1(x))   x = torch.relu(self.fc2(x))   x = self.fc3(x)   return x    # Print model summary  from torchsummary import summary  model = CNNClassifier()  if torch.cuda.is_available():   device = torch.device(\"cuda\")   model.to(device)  print('Name: ')  print('Register Number: ')  summary(model, input_size=(1, 28, 28)) # MNIST images are 28x28 with 1  channel"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 99,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "99 \n \ncriterion = nn.CrossEntropyLoss() \noptimizer = optim.Adam(model.parameters(), lr=0.001) \n \n# Training Function \ndef train_model(model, train_loader, num_epochs=10): \n for epoch in range(num_epochs): \n model.train() \n running_loss = 0.0 \n for images, labels in train_loader: \n if torch.cuda.is_available(): \n images, labels = images.to(device), labels.to(device) \n \n optimizer.zero_grad() \n outputs = model(images) \n loss = criterion(outputs, labels) \n loss.backward() \n optimizer.step() \n running_loss += loss.item() \n \n print('Name: ') \n print('Register Number: ') \n print(f'Epoch [{epoch+1}/{num_epochs}], Loss: \n{running_loss/len(train_loader):.4f}') \n \n# Train the model \ntrain_model(model, train_loader, num_epochs=10)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 100,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "100 \n \n# Testing Function \ndef test_model(model, test_loader): \n model.eval() \n correct = 0 \n total = 0 \n all_preds = [] \n all_labels = [] \n \n with torch.no_grad(): \n for images, labels in test_loader: \n if torch.cuda.is_available(): \n images, labels = images.to(device), labels.to(device) \n \n outputs = model(images) \n _, predicted = torch.max(outputs, 1) \n total += labels.size(0) \n correct += (predicted == labels).sum().item() \n all_preds.extend(predicted.cpu().numpy()) \n all_labels.extend(labels.cpu().numpy()) \n \n accuracy = correct / total \n print('Name: ') \n print('Register Number: ') \n print(f'Test Accuracy: {accuracy:.4f}') \n # Compute confusion matrix \n cm = confusion_matrix(all_labels, all_preds)"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 101,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "101     plt.figure(figsize=(8, 6))   print('Name: ')   print('Register Number: ')   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',  xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)   plt.xlabel('Predicted')   plt.ylabel('Actual')   plt.title('Confusion Matrix')   plt.show()   # Print classification report   print('Name: ')   print('Register Number: ')   print(\"Classification Report:\")   print(classification_report(all_labels, all_preds, target_names=[str(i) for i in  range(10)]))  # Test the model  test_model(model, test_loader)    # Function to predict and visualize an image  def predict_image(model, image_index, dataset):   model.eval()   image, label = dataset[image_index]   if torch.cuda.is_available():   image = image.to(device)     with torch.no_grad():"
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 102,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "102 \n \n _, predicted = torch.max(output, 1) \n \n class_names = [str(i) for i in range(10)] \n \n print('Name: ') \n print('Register Number: ') \n plt.imshow(image.cpu().squeeze(), cmap=\"gray\") \n plt.title(f'Actual: {class_names[label]}\\nPredicted: \n{class_names[predicted.item()]}') \n plt.axis(\"off\") \n plt.show() \n print(f'Actual: {class_names[label]}, Predicted: \n{class_names[predicted.item()]}') \n \n# Predict and visualize an image \npredict_image(model, image_index=80, dataset=test_dataset) \n \n \n _____________________ \n \n \n \nFor Set 3 QP \n \nPart - A \nQuestion No. \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK3 \nK2 \nK2 \nK2 \nK2 \nK2 \nK2 \nK2 \nK3 \nK3 \nDifficulty Level \n3 \n3 \n2 \n2 \n2 \n2 \n3 \n2 \n3 \n2 \n \n Part - B \nPart - C \nQuestion No."
  },
  {
    "book_id": "90034a66-998e-4c29-9756-2923ddc43298",
    "book_title": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "page": 102,
    "source": "AK- 19AI413-2025 (3 Sets) -10.09.2025 -MM.pdf",
    "chunk_text": "11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK2 \nK6 \nK3 \nK3 \nK2 \nK3 \nK3 \nK6 \nK3 \nK2 \nK6 \nK6 \nDifficulty Level \n3 \n3 \n3 \n3 \n3 \n4 \n3 \n4 \n3 \n3 \n4 \n4"
  },
  {
    "book_id": "9f3e624d-f053-4f60-8c3a-68d3270d97c3",
    "book_title": "Screenshot 2025-11-17 182246.png",
    "page": 1,
    "source": "Screenshot 2025-11-17 182246.png",
    "chunk_text": "Covariance hypothesis. One way of overcoming the limitation of Hebb’s hypothesis\nis to use the covariance hypothesis introduced in Sejnowski (1977a, b). In this hypothesis, the presynaptic and postsynaptic signals in Eq. (2.9) are replaced by the departure\nof presynaptic and postsynaptic signals from their respective average values over a certain time interval. Let ¥ and y denote the time-averaged values of the presynaptic signal"
  },
  {
    "book_id": "9f3e624d-f053-4f60-8c3a-68d3270d97c3",
    "book_title": "Screenshot 2025-11-17 182246.png",
    "page": 1,
    "source": "Screenshot 2025-11-17 182246.png",
    "chunk_text": "x, and postsynaptic signal y,, respectively. According to the covariance hypothesis, the\nadjustment applied to the synaptic weight w,, is defined by"
  },
  {
    "book_id": "9f3e624d-f053-4f60-8c3a-68d3270d97c3",
    "book_title": "Screenshot 2025-11-17 182246.png",
    "page": 1,
    "source": "Screenshot 2025-11-17 182246.png",
    "chunk_text": "where 1 is the learning-rate parameter. The average values x and y constitute presynaptic and postsynaptic thresholds, which determine the sign of synaptic modification."
  },
  {
    "book_id": "9f3e624d-f053-4f60-8c3a-68d3270d97c3",
    "book_title": "Screenshot 2025-11-17 182246.png",
    "page": 1,
    "source": "Screenshot 2025-11-17 182246.png",
    "chunk_text": "In particular, the covariance hypothesis allows for the following:"
  },
  {
    "book_id": "9b79c0eb-8f65-4527-85a9-9dde32e468c4",
    "book_title": "Screenshot 2025-12-17 165904.png",
    "page": 1,
    "source": "Screenshot 2025-12-17 165904.png",
    "chunk_text": "Data augmentation is a process of artificially increasing the amount of data by\ngenerating new data points from existing data."
  },
  {
    "book_id": "9b79c0eb-8f65-4527-85a9-9dde32e468c4",
    "book_title": "Screenshot 2025-12-17 165904.png",
    "page": 1,
    "source": "Screenshot 2025-12-17 165904.png",
    "chunk_text": "Data augmentation includes adding minor alterations to data or using machine\nlearning models to generate new data points in the latent space of original data to\namplify the dataset."
  },
  {
    "book_id": "9b79c0eb-8f65-4527-85a9-9dde32e468c4",
    "book_title": "Screenshot 2025-12-17 165904.png",
    "page": 1,
    "source": "Screenshot 2025-12-17 165904.png",
    "chunk_text": "Synthetic data: When data is generated artificially without using real-world images. Synthetic data"
  },
  {
    "book_id": "9b79c0eb-8f65-4527-85a9-9dde32e468c4",
    "book_title": "Screenshot 2025-12-17 165904.png",
    "page": 1,
    "source": "Screenshot 2025-12-17 165904.png",
    "chunk_text": "are often produced by Generative Adversarial Networks"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 1,
    "source": "deep learning.pdf",
    "chunk_text": "1 \n \nSET 1 (Answer Key) \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nConsider a single neuron as a linear unit. Train the model with 'sugars' (grams of sugars \nper serving) as input and 'calories' (calories per serving) as output. Let the bias is b=90, and \nthe weight is w=2.5. Estimate the calorie content of cereal with 5 grams of sugar per serving \nwith a network diagram. \nAnswer: \nGiven Values: Input: x=5 grams sugar, Weight: w=2.5, Bias: b=90 \nA linear neuron computes the output as: y = w ⋅ x + b. = =2.5×5+90=102.5 \nPredicted calories = 102.5. \nCO1 \n2."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 1,
    "source": "deep learning.pdf",
    "chunk_text": "How do the complexity of a model and the size of the training dataset contribute to \noverfitting, and what techniques can be employed to mitigate its effects? \nAnswer : \nOverfitting happens when the model is too complex (many parameters) or the training dataset is \ntoo small, leading to memorization instead of generalization. It can be mitigated using \nregularization (L1/L2, dropout), early stopping, data augmentation, cross-validation, simpler \nmodels, or by collecting more data. \nCO1 \n3. \nDetermine the number of parameters if the input image shape is 1024x1024x3 and the \nconvolution layer uses 32 filters of size 7x7 with a stride of 1 and padding of 2."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 1,
    "source": "deep learning.pdf",
    "chunk_text": "Answer: Given Values : Input shape = 1024×1024×3, Filter size = 7×7, Input channels = 3, \nNumber of filters = 32, Stride & padding affect output size, but not parameter count. \nFormula for Parameters in a Convolutional Layer: \ntotal parameters = parameters per filter × number of filters \nparameters per filter = (filter height × filter width × input channels) + 1 (bias) \nNow, Filter weights = 7 × 7 × 3 = 1477 × 7 × 3=147, Add bias = 147 + 1 = 148147 + 1 = 148 per \nfilter, Then For 32 filters: 148 × 32 = 4736148 × 32 = 4736, Total parameters = 4736 \nCO2 \n4. \nWhy are convolutions useful in deep learning? \nAnswer: \nConvolutions are useful in deep learning because they reduce the number of parameters compared \nto fully connected networks, making the model more efficient."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 1,
    "source": "deep learning.pdf",
    "chunk_text": "They can automatically extract \nlocal features such as edges, corners, and textures, which are essential for understanding images. \nConvolutions also provide translation invariance, meaning the same feature can be detected \nanywhere in the image. By stacking multiple convolutional layers, the network can learn \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 2,
    "source": "deep learning.pdf",
    "chunk_text": "2 \n \nhierarchical representations, starting from low-level patterns and progressing to high-level object \nfeatures. \n5. \nDistinguish between the phenomena of exploding gradients and vanishing gradients. \n \nExploding Gradients \n \nVanishing Gradients \n● Gradients become very large during \nbackprop. \n● Gradients become very small (near \nzero). \n● Unstable training, weights diverge. \n● Learning slows or stops, early layers \ndon’t learn. \n● Repeated multiplication → large \nvalues grow. \n● Repeated multiplication → values \nshrink. \n● Very deep networks, RNNs with long \nsequences \n● Very deep networks, RNNs with \nsigmoid/tanh. \n● Gradient clipping, small learning rate. \n● ReLU/variants, batch norm, skip \nconnections. \n \nCO3 \n6."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 2,
    "source": "deep learning.pdf",
    "chunk_text": "How do LSTM and GRU models differ in terms of the number of gates and their \nfunctionalities? \nLSTM (Long Short-Term Memory) \nGRU (Gated Recurrent Unit) \n● 3 gates – Input gate, Forget gate, \nOutput gate. \n● 2 gates – Update gate, Reset gate. \n● Has a separate cell state and hidden \nstate → better for long dependencies. \n● Combines cell state and hidden state \ninto one → simpler. \n● More parameters → computationally \nheavier. \n● Fewer parameters → faster, less \nmemory. \n● Controls what to write, keep, and \noutput explicitly. \n● Controls how much of the past to \nkeep or reset. \n \nCO3 \n7. \nDefine Autoencoder and offer concrete examples of its applications in real-world scenarios?"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 2,
    "source": "deep learning.pdf",
    "chunk_text": "Answer: \nAn Autoencoder is a type of neural network used for unsupervised learning that learns to encode \ninput data into a lower-dimensional representation (latent space) and then reconstruct it back to \nthe original form. It consists of two parts Encoder and decoder. Encoder compresses the input \ninto latent features. Decoder reconstructs the input from latent features.The goal is to learn \nefficient representations by minimizing reconstruction error. \nApplications : Image Denoising, Dimensionality Reduction, Anomaly Detection, Image \nCompression, Recommendation Systems \n \nCO4 \n8. \nWhy do RBMs use Gibbs sampling during training? \nAnswers: \n RBMs use Gibbs sampling because computing exact probabilities over all visible–hidden \nconfigurations is intractable."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 2,
    "source": "deep learning.pdf",
    "chunk_text": "Gibbs sampling alternately samples hidden units given visible units \nand visible units given hidden units, approximating the model’s distribution. This enables \nefficient training using algorithms like Contrastive Divergence. \nCO4 \n9. \nDefine Intersection over union(IoU) in bounding box prediction. \nAnswer: Intersection over Union (IoU) is a metric used to evaluate object detection models. It \nCO5"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 3,
    "source": "deep learning.pdf",
    "chunk_text": "3 \n \nmeasures the overlap between the predicted bounding box and the ground-truth bounding box. \n \n \n10. \nWhat makes the CamVid dataset suitable for urban scene understanding in semantic \nsegmentation? \nAnswer: The CamVid dataset is suitable for urban scene semantic segmentation because it \ncontains video frames of real-world urban driving, with pixel-level annotations for multiple \nobject classes like roads, vehicles, pedestrians, and buildings. Its diverse classes and temporal \ninformation make it ideal for training and evaluating segmentation models in city environments. \nCO5 \n \n \n PART B (5 x 13 = 65 marks) \n \n \n \nCO \n11. \n(a) \nIn a practical deep learning project, how do you determine the most suitable \nactivation function for different layers in a neural network and why?"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 3,
    "source": "deep learning.pdf",
    "chunk_text": "Additionally, \nexplain these activation functions and provide PyTorch implementations? \nActivation function: \nWe want to set boundaries for the overall output value, x*w+b is passed through the activation \nfunction. Let us define the total input as z, where z= x*w+b. Then pass z through some activation \nfunction to limit its value. \n● The most simple networks rely on basic step function that outputs 0 or 1. \n● Regardless of the values it outputs 0 or 1. \n● This is a very strong function since small changes aren’t reflected. \n● There is just an immediate cutoff that splits between 0 and 1. \n \nIt would be nice if we could have a more dynamic function, for example the red line."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 3,
    "source": "deep learning.pdf",
    "chunk_text": "Sigmoid (Logistic) \n● The Sigmoid function (also known as the Logistic function) is one of the most widely \nused activation functions. The function is defined as: \nCO1"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 4,
    "source": "deep learning.pdf",
    "chunk_text": "4 \n \n \n● The function is a common S-shaped curve. \n● The output of the function is centered at 0.5 with a range from 0 to 1. \n● The function is differentiable. That means we can find the slope of the sigmoid curve at \nany two points. \n● The function is monotonic but the function’s derivative is not. \nProblems with Sigmoid activation function \n● Vanishing gradient: looking at the function plot, you can see that when inputs become \nsmall or large, the function saturates at 0 or 1, with a derivative extremely close to 0. \nThus it has almost no gradient to propagate back through the network, so there is almost \nnothing left for lower layers. \n● Computationally expensive: the function has an exponential operation. \n● The output is not zero centered."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 4,
    "source": "deep learning.pdf",
    "chunk_text": "Implementation \n \nHyperbolic Tangent (Tanh) \nAnother very popular and widely used activation function is the Hyperbolic Tangent, also known \nas Tanh. It is defined as: \n \n● The function is a common S-shaped curve as well. \n● The difference is that the output of Tanh is zero centered with a range from -1 to 1 \n(instead of 0 to 1 in the case of the Sigmoid function) \n● The same as the Sigmoid, this function is differentiable \n● The same as the Sigmoid, the function is monotonic, but the function’s derivative is not. \nProblems with Tanh activation function"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 5,
    "source": "deep learning.pdf",
    "chunk_text": "5 \n \n● Vanishing gradient: looking at the function plot, you can see that when inputs become \nsmall or large, the function saturates at -1 or 1, with a derivative extremely close to 0. \nThus it has almost no gradient to propagate back through the network, so there is almost \nnothing left for lower layers. \n● Computationally expensive: the function has an exponential operation. \n Implementation \n \nRectified Linear Unit (ReLU) \n● The Rectified Linear Unit (ReLU) is the most commonly used activation function in deep \nlearning. \n● The function returns 0 if the input is negative, but for any positive input, it returns that \nvalue back. The function is defined as: \n \n● Graphically, the ReLU function is composed of two linear pieces to account for non \nlinearities."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 5,
    "source": "deep learning.pdf",
    "chunk_text": "A function is non-linear if the slope isn’t constant. So, the ReLU function is \nnon-linear around 0, but the slope is always either 0 (for negative inputs) or 1 (for \npositive inputs). \n● The ReLU function is continuous, but it is not differentiable at the origin. \n● The output of ReLU does not have a maximum value (It is not saturated) and this helps \nGradient Descent \n● The function is very fast to compute (Compare to Sigmoid and Tanh) \nProblem with ReLU \n● ReLU works great in most applications, but it is not perfect. It suffers from a problem \nknown as the dying ReLU. \nDying ReLU \nDuring training, some neurons effectively die, meaning they stop outputting anything other than 0."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 5,
    "source": "deep learning.pdf",
    "chunk_text": "In some cases, you may find that half of your network’s neurons are dead, especially if you used a \nlarge learning rate. A neuron dies when its weights get tweaked in such a way that the weighted \nsum of its inputs are negative for all instances in the training set. When this happens, it just keeps"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 6,
    "source": "deep learning.pdf",
    "chunk_text": "6 \n \noutputting 0s, and gradient descent does not affect it anymore since the gradient of the ReLU \nfunction is 0 when its input is negative. \n Implementation \n \nSoftmax Activation Function \n● Softmax function calculates the probability distribution of the event over K different \nevents. \n● This function will calculate the probabilities of each target class over all possible target \nclasses. \n● The range will be 0 to 1, and the sum of all the probabilities will be equal to one. \n● The model returns the probabilities of each class and the target class chosen will have the \nhighest probability. \n \nImplementation \n \n \n \n \n(OR) \n \n \n(b) \n(i) Create the appropriate PyTorch implementation of the following neural network."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 6,
    "source": "deep learning.pdf",
    "chunk_text": "It should include model creation, model instantiation, loss function selection and \noptimizer selection.(6 Marks) \nCO1"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 7,
    "source": "deep learning.pdf",
    "chunk_text": "7 \n \n \n \n(ii) Explain the confusion matrix for binary class and multiclass prediction. (7 Marks) \nClassification Metrics \n● Classification is about predicting the class labels given input data. \n● In binary classification, there are only two possible output classes. \n● A very common example of binary classification is spam detection, where the \ninput data could include the email text and metadata (sender, sending time), and \nthe output label is either “spam” or “not spam.” \nConfusion Matrix \n● Confusion Matrix is a performance measurement for the machine learning \nclassification problems where the output can be two or more classes. \n● It is a table with combinations of predicted and actual values."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 7,
    "source": "deep learning.pdf",
    "chunk_text": "● A confusion matrix is defined as the table that is often used to describe the \nperformance of a classification model on a set of the test data for which the true \nvalues are known."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 8,
    "source": "deep learning.pdf",
    "chunk_text": "8 \n \n \nTrue Positive: We predicted positive and it’s true. \nTrue Negative: We predicted negative and it’s true. \nFalse Positive (Type 1 Error)- We predicted positive and it’s false. \nFalse Negative (Type 2 Error)- We predicted negative and it’s false. \nAccuracy: \nAccuracy simply measures how often the classifier correctly predicts. We can define \naccuracy as the ratio of the number of correct predictions and the total number of \npredictions. \n \nMulticlass Confusion Matrix \n \nImplementation"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 9,
    "source": "deep learning.pdf",
    "chunk_text": "9 \n \n12. \n(a) \nThe CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes, with 6000 \nimages per class. There are 50000 training images and 10000 test images. Develop the \nPyTorch implementation for the following. \ni) Load the CIFAR-10 dataset (3 Marks) \nii) Do the necessary Preprocessing (3 Marks) \niii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function (3 Marks) \niv) Train the model with training data and validation data. (4 Marks) \n \nAnswer: \n \n \n \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 10,
    "source": "deep learning.pdf",
    "chunk_text": "10 \n \n \n \n \n(OR) \n \n \n(b) \nExplain the following with examples \n(i) Padding (4 Marks) \n(ii) Striding (4 Marks) \n(iii) Pooling (5 Marks) \ni) Padding \nThere are two problems arise with convolution: \n1. Every time after convolution operation, original image size getting shrinks \nIn image classification task there are multiple convolution layers so after multiple \nconvolution operation, our original image will really get small but we don’t want \nthe image to shrink every time. \n2. When kernel moves over original images, it touches the edge of the image less \nnumber of times and touches the middle of the image more number of times and \nit overlaps also in the middle. So, the corner features of any image or on the \nedges aren’t used much in the output."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 10,
    "source": "deep learning.pdf",
    "chunk_text": "In order to solve these two issues, a new concept is introduced called padding. Padding preserves \nthe size of the original image. \n \nPadded image convolved with 2*2 kernel \nSo if a 𝑛∗𝑛 matrix convolved with an f*f matrix the with padding p then the size of the output \nimage will be (n - f + 2p + 1) * (n - f + 2p + 1) where p =1 in this case. \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 11,
    "source": "deep learning.pdf",
    "chunk_text": "11 \n \n \n(ii) Striding \n \n● Stride is the number of pixels shifts over the input matrix. \n● When the stride is 1 then we move the filters one pixel at a time. \n● When the stride is 2 then the filters jump 2 pixels at a time as we slide them around. \n● This will produce smaller output volumes spatially. \n \nleft image: stride =0, middle image: stride = 1, right image: stride =2 \nOutput Dimension \n● We can compute the spatial size of the output volume as a function of the input \nvolume size (W), the receptive field size of the Conv Layer neurons (F), the stride \nwith which they are applied (S), and the amount of zero padding used (P) on the \nborder. \n● Formula for calculating how many neurons “fit” is given by ((W−F+2P)/S)+1."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 11,
    "source": "deep learning.pdf",
    "chunk_text": "● For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get \na 5x5 output. With stride 2 we would get a 3x3 output. \n(iii) Pooling \n● A pooling layer is another building block of a CNN. \n● Pooling function is to progressively reduce the spatial size of the representation to reduce \nthe network complexity and computational cost. \nThere are two types of widely used pooling in CNN layer: \n● Max Pooling \n● Average Pooling \nMax Pooling \n● Max pooling is simply a rule to take the maximum of a region and it helps to proceed \nwith the most important features from the image. \n● Max pooling selects the brighter pixels from the image. \n● It is useful when the background of the image is dark and we are interested in only the \nlighter pixels of the image."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 12,
    "source": "deep learning.pdf",
    "chunk_text": "12 \n \n \nAverage Pooling \n● Average Pooling is different from Max Pooling in the sense that it retains much \ninformation about the “less important” elements of a block, or pool. \n● Whereas Max Pooling simply throws them away by picking the maximum value, \n● Average Pooling blends them in. This can be useful in a variety of situations, where such \ninformation is useful. \n \nPooling Dimension \n● The pooling layer downsamples the volume spatially, independently in each depth slice \nof the input volume. \n● In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride \n2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. \n \n \n \n \n \n13."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 12,
    "source": "deep learning.pdf",
    "chunk_text": "(a) \nUsing GloVe word embeddings, implement the following tasks: \n(i) Develop a function that retrieves the embedding vector of a given word from pretrained GloVe embeddings. (5 Marks) \n(ii) Implement a function that computes and returns the most similar words to a given \nword by comparing their embedding vectors using cosine similarity. (8 Marks) \n \nimport numpy as np \n \ndef load_glove_embeddings(glove_file_path): \nCO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 13,
    "source": "deep learning.pdf",
    "chunk_text": "13 \n \n embeddings = {} \n with open(glove_file_path, 'r', encoding='utf8') as f: \n for line in f: \n parts = line.strip().split() \n word = parts[0] \n vector = np.array(parts[1:], dtype=np.float32) \n embeddings[word] = vector \n return embeddings \n \ndef get_word_embedding(word, glove_embeddings): \n \"\"\" \n Retrieves the embedding vector for a given word. \n \n Args: \n word (str): The word to retrieve. \n glove_embeddings (dict): A dictionary mapping words to vectors. \n \n Returns: \n np.ndarray or None: Embedding vector if the word exists, otherwise None."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 13,
    "source": "deep learning.pdf",
    "chunk_text": "\"\"\" \n return glove_embeddings.get(word) \n \n \nfrom sklearn.metrics.pairwise import cosine_similarity \n \ndef find_most_similar_words(word, glove_embeddings, top_n=5): \n \"\"\" \n Finds the most similar words to the input word using cosine similarity. \n \n Args: \n word (str): The word to find similarities for. \n glove_embeddings (dict): Preloaded GloVe embeddings. \n top_n (int): Number of similar words to return. \n \n Returns: \n List[Tuple[str, float]]: List of (word, similarity_score) tuples. \n \"\"\" \n if word not in glove_embeddings: \n return [] \n \n word_vec = glove_embeddings[word].reshape(1, -1) \n similarities = {} \n \n for other_word, vec in glove_embeddings.items(): \n if other_word == word: \n continue \n sim = cosine_similarity(word_vec, vec.reshape(1, -1))[0][0] \n similarities[other_word] = sim"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 14,
    "source": "deep learning.pdf",
    "chunk_text": "14 \n \n # Sort by similarity and return the top N \n sorted_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True) \n return sorted_similar[:top_n] \n \n \n \n(OR) \n \n \n(b) \nDescribe the forward and backward pass mechanisms in bidirectional RNNs. Why are they \nparticularly useful in NLP tasks like machine translation? \nAnswer: \nA Bidirectional RNN consists of two RNNs running in opposite directions: \n· One processes the sequence forward (left to right). \n· The other processes it backward (right to left). \n· Their outputs are concatenated at each timestep to form the final representation. \n \nA Bidirectional RNN consists of two RNNs running in opposite directions: \n· One processes the sequence forward (left to right). \n· The other processes it backward (right to left)."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 14,
    "source": "deep learning.pdf",
    "chunk_text": "· Their outputs are concatenated at each time step to form the final representation. \n \nCO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 16,
    "source": "deep learning.pdf",
    "chunk_text": "16 \n \n \n \n \n \n14. \n(a) \ni) Enumerate and explain the various types of Generative Adversarial Network (GAN) \narchitectures. (7 Marks) \nGenerative Adversarial Networks (GANs) operate on an adversarial principle, pitting a \nGenerator (G) against a Discriminator (D) to learn to produce realistic data. Over time, \nnumerous architectures have emerged to improve training stability, output quality, and \ncontrol over generation. Here are some of the most significant types: \n1. Vanilla GAN (Original GAN): \n○ Architecture: The foundational model, typically using Multi-Layer \nPerceptrons (MLPs) for both G and D.The Generator maps a random noise \nvector to a data sample, and the Discriminator outputs a probability of the \ninput being real."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 16,
    "source": "deep learning.pdf",
    "chunk_text": "○ Working Principle: G aims to produce samples that fool D into \nclassifying them as real. D's goal is to accurately distinguish real data from \nG's fakes. This creates a minimax game. \n○ Contribution: Introduced the core adversarial training concept, laying the \ngroundwork for all subsequent GAN research. \n○ Limitations: Suffers from training instability, often leading to mode \ncollapse (G produces a limited variety of outputs) and difficulty generating \nhigh-resolution, diverse samples. \n2. Deep Convolutional GAN (DCGAN): \n○ Architecture: Replaced the MLPs of vanilla GANs with Convolutional \nNeural Networks (CNNs) in both G and D. The Generator uses \n\"transposed convolutions\" (deconvolutions) for upsampling, and the \nDiscriminator uses strided convolutions for downsampling."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 16,
    "source": "deep learning.pdf",
    "chunk_text": "Batch \nNormalization is extensively used, and specific activation functions (e.g., \nReLU in G, Leaky ReLU in D) are common. \n○ Working Principle: Applies the power of CNNs to learn hierarchical \nfeatures in images, improving the quality and stability of image generation \ncompared to vanilla GANs. \n○ Contribution: Established architectural guidelines and best practices for \nbuilding stable and effective GANs for visual data. \n○ Improvement Over: Significantly enhanced the stability and visual \nquality of generated images from vanilla GANs. \n3. Conditional GAN (cGAN): \n○ Architecture: Extends any base GAN (like Vanilla or DCGAN) by \nincorporating auxiliary \"conditional\" information (e.g., class labels, text \ndescriptions, or another image) into both the Generator and the \nDiscriminator."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 16,
    "source": "deep learning.pdf",
    "chunk_text": "This conditioning is typically concatenated with the input \nnoise vector for G and with the data input for D. \n○ Working Principle: G learns to generate specific types of data \nconditioned on the provided input, while D learns to discriminate the \nauthenticity of the data given that condition. \n○ Contribution: Enables targeted and controlled data generation, allowing \nusers to specify characteristics of the desired output. \n○ Example Use: Generating an image of a \"cat\" when given the label \"cat,\" \nor transforming a sketch into a photo. \n4. Wasserstein GAN (WGAN) and WGAN-GP (Gradient Penalty): \nCO4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "17 \n \n○ Architecture: Primarily modifies the loss function and the \nDiscriminator's role. It replaces the original GAN's Jensen-Shannon \ndivergence-based loss with the Wasserstein distance (Earth Mover's \nDistance).The Discriminator is re-termed a \"Critic\" and outputs a raw \nscore (no sigmoid activation). WGAN-GP further refines this by adding a \n\"gradient penalty\" term to the Critic's loss, enforcing the Lipschitz \nconstraint more robustly than WGAN's weight clipping. \n○ Working Principle: The Wasserstein distance provides a more stable and \nmeaningful gradient, even when the real and fake data distributions are \nnon-overlapping, which was a major problem for earlier GANs."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "○ Contribution: Drastically improved training stability and reduced mode \ncollapse, providing a reliable metric for generator convergence. \n○ Improvement Over: Addressed the core training instability and mode \ncollapse issues prevalent in Vanilla and DCGANs. \n5. CycleGAN: \n○ Architecture: Composed of two pairs of Generators and Discriminators, \ndesigned for unpaired image-to-image translation between two domains \n(e.g., Domain A and Domain B). It features a critical \"cycle consistency \nloss\" which ensures that translating an image from A to B and then back to \nA yields the original image A. \n○ Working Principle: The cycle consistency loss acts as a powerful \nregularization, allowing the model to learn mappings between domains \nwithout requiring perfectly aligned (paired) training data."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "○ Contribution: Revolutionized image-to-image translation by removing \nthe stringent requirement for paired datasets, opening up new applications. \n○ Example Use: Transforming horses into zebras, changing summer scenes \nto winter scenes, or converting photos into specific artistic styles. \nii) Describe in detail the practical applications of GANs across different domains (6 \nMarks) \nGANs have transcended theoretical research to become powerful tools across a wide \narray of practical domains, fundamentally changing how we approach data generation and \nmanipulation. \n1. Image and Video Synthesis & Editing: \n○ Photorealistic Image Generation: GANs can create highly realistic images \nof objects, landscapes, and even human faces that are indistinguishable \nfrom real photos."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "This is used in generating synthetic data for AI training, \nvirtual photography for e-commerce, and creating unique visual assets for \nart and advertising.Examples include StyleGAN-generated faces used for \ndiverse online profiles. \n○ Image-to-Image Translation: This is a major application, enabling \nconversion between different image domains. \n■ Style Transfer: Applying artistic styles (e.g., Van Gogh's style) to \nregular photos (e.g., CycleGAN). \n■ Semantic Image Synthesis: Generating photorealistic images from \nsemantic layouts or sketches (e.g., turning a rough drawing of a cat \ninto a realistic cat photo). \n■ Domain Adaptation: Transforming images between different \nconditions, like converting day scenes to night scenes, or summer \nlandscapes to winter ones."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "○ Photo Inpainting/Outpainting: Filling in missing or corrupted parts of an \n2. image with realistic content, or extending an image beyond its original"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 18,
    "source": "deep learning.pdf",
    "chunk_text": "18 \n \nboundaries, widely used in photo restoration and creative content generation. \n3. Super-Resolution: Enhancing the resolution and detail of low-resolution images, \nvaluable in forensics, medical imaging, and improving video quality. \n4. Deepfakes (Ethical Concern): While demonstrating powerful generative \ncapabilities, the ability to create highly realistic fake videos or audio of \nindividuals has severe ethical implications concerning misinformation and \nimpersonation. \n2. Data Augmentation and Synthetic Data Generation: \n○ Medical Imaging: Generating synthetic medical scans (e.g., X-rays, MRIs) \nto augment limited datasets for training diagnostic AI models, especially \nfor rare diseases, thus improving model robustness and ensuring patient \nprivacy."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 18,
    "source": "deep learning.pdf",
    "chunk_text": "○ Autonomous Driving: Creating diverse synthetic road scenes, including \nrare events, adverse weather conditions, and various lighting scenarios, to \ntrain self-driving car algorithms more safely and comprehensively, \nreducing the need for costly and risky real-world data collection. \n○ Fraud Detection: Generating synthetic fraudulent transaction data to train \nmodels, particularly when real fraud cases are scarce but crucial for \neffective detection. \n○ Privacy Preservation: Generating synthetic versions of sensitive datasets \nthat retain statistical properties of the original data but contain no real \nindividual information, useful for sharing data with privacy concerns. \n3."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 18,
    "source": "deep learning.pdf",
    "chunk_text": "Entertainment and Creative Industries: \n○ Video Game Development: Automatically generating realistic textures, \ncharacters, landscapes, and entire game levels, significantly speeding up \ncontent creation pipelines. \n○ Film and Animation: Assisting in character design, generating background \nelements, creating special effects, and even synthesizing realistic crowd \nscenes. \n○ Generative Art: Creating unique and novel artworks, music, and even \narchitectural designs that explore new aesthetic possibilities, blurring the \nlines between human and AI creativity. \n4. Scientific Research and Design: \n○ Drug Discovery: Generating novel molecular structures with desired \nchemical properties, accelerating the search for new drugs and treatments."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 18,
    "source": "deep learning.pdf",
    "chunk_text": "○ Material Science: Designing new materials with specific characteristics by \nexploring the vast space of possible atomic and molecular configurations. \n○ Fashion Design: Generating new clothing designs, patterns, and virtual \ntry-on experiences, providing designers with a tool to rapidly prototype \nideas. \n5. Telecommunications and Networking: \n○ Network Anomaly Detection: Generating synthetic anomalous network \ntraffic to train intrusion detection systems, improving their ability to spot \nnovel threats. \nChannel Modeling: Simulating complex wireless communication channels \nto test and optimize communication protocols. \n \n \n(OR)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 19,
    "source": "deep learning.pdf",
    "chunk_text": "19 \n \n \n(b) \nDescribe the different types of autoencoders and explain their architectures, objectives, \nand typical applications with suitable diagrams. \nDifferent Types of Autoencoders: \n1. Undercomplete Autoencoder (Basic Autoencoder) \n \n· Objective: To learn a low-dimensional representation (Z) of the input data such that \nthe reconstruction error between Xand X^ is minimized. The primary goal is \ndimensionality reduction and feature learning. \n· Typical Applications: \n● Dimensionality Reduction: Similar to Principal Component Analysis (PCA) but \ncan capture non-linear relationships. \n● Feature Learning: Extracting salient features from data. \n● Data Compression: Compressing data by storing its latent representation \n2."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 19,
    "source": "deep learning.pdf",
    "chunk_text": "Sparse Autoencoder ( Over complex ) \n \nObjective: To learn representations where only a small subset of hidden units are active \n(non-zero) for any given input. This encourages the network to learn specialized feature \ndetectors. \n● The loss function includes the reconstruction error plus a sparsity penalty. \nCommon sparsity penalties include: \n○ L1 Regularization: \n \n■ Where aj is the activation of the j-th hidden unit, and λ is the \nCO4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 20,
    "source": "deep learning.pdf",
    "chunk_text": "20 \n \nsparsity regularization parameter. \n○ KL Divergence: \n \n■ Where ρ is the desired average activation (sparsity target), ρ^j is \nthe actual average activation of hidden unit j over a batch, and β is \nthe weight of the sparsity penalty. \nApplications: \n● Feature Learning: Learning more interpretable and distributed features. \n● Denoising: Can indirectly help with denoising as it focuses on salient features. \n● Representation Learning: Useful when a higher-dimensional but sparse \nrepresentation is desired. \n3. Denoising Autoencoder (DAE) \n \nApplications: \n● Denoising: Directly removing noise from images, audio, or other data. \n● Robust Feature Learning: Learning features that are more resilient to real-world \ndata imperfections."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 20,
    "source": "deep learning.pdf",
    "chunk_text": "● Data Imputation: Filling in missing values by treating missing data as noise. \n4. Variational Autoencoder (VAE)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 21,
    "source": "deep learning.pdf",
    "chunk_text": "21 \n \n \nTypical Applications: \n● Generative Modeling: Generating new, realistic data samples (e.g., images, text, \nmusic). \n● Dimensionality Reduction: Learning a meaningful latent representation. \n● Anomaly Detection: Anomalous data points tend to have high reconstruction \nerrors or fall into low-density regions of the learned latent space. \nLatent Space Interpolation: Smoothly transitioning between different generated \nsamples by interpolating in the latent space \n \n \n \n \n15. \n(a) \nExplain bounding box prediction in YOLO algorithm with necessary diagrams. \nAnswer: \n \n \nCO5"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 26,
    "source": "deep learning.pdf",
    "chunk_text": "26 \n \n \n \n \n \n(OR) \n \n \n(b) \n \nDiscuss the various region proposal methods used in object detection. Compare their \nefficiency and suitability for integration with modern object detection frameworks. \n \n \n \n \nCO5"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 29,
    "source": "deep learning.pdf",
    "chunk_text": "29 \n \n \n \n \n \n \n \n PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \n \nCO \n16. \n(a) \nCreate the PyTorch implementation Denoising Autoencoder for the MNIST dataset. \ni) Load the MNIST dataset. ( 3 Mark) \nii) Create a model of Denoising Autoencoder ( 4 Mark) \niii) Compile the model with optimizer and loss function ( 3 Mark) \niv) Fit the model with training data and validation data. ( 5 Mark) \n \nAnswer: \n \n# (i) LOAD THE MNIST DATASET \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader, random_split \n \nCO4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 30,
    "source": "deep learning.pdf",
    "chunk_text": "30    # 📁 Transforms: Normalize and Convert to Tensors  transform = transforms.Compose([   transforms.ToTensor(), # Converts to [0,1]  ])    # Download MNIST  train_dataset = datasets.MNIST(root='./data', train=True, download=True,  transform=transform)  test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)    # Split training into train/val  train_data, val_data = random_split(train_dataset, [50000, 10000])    # Dataloaders  train_loader = DataLoader(train_data, batch_size=128, shuffle=True)  val_loader = DataLoader(val_data, batch_size=128, shuffle=False)  test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # (ii) DENOISING AUTOENCODER MODEL  class DenoisingAutoencoder(nn.Module):   def __init__(self):"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 30,
    "source": "deep learning.pdf",
    "chunk_text": "super(DenoisingAutoencoder, self).__init__()   self.encoder = nn.Sequential(   nn.Linear(28*28, 128),   nn.ReLU(),   nn.Linear(128, 64),   )   self.decoder = nn.Sequential(   nn.Linear(64, 128),   nn.ReLU(),   nn.Linear(128, 28*28),   nn.Sigmoid() # keep output in [0, 1]   )     def forward(self, x):   x = x.view(x.size(0), -1) # Flatten input   encoded = self.encoder(x)   decoded = self.decoder(encoded)   return decoded.view(x.size(0), 1, 28, 28)    # (iii) COMPILE THE MODEL  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  model = DenoisingAutoencoder().to(device)  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=0.001)    # Add Gaussian Noise  def add_noise(inputs, noise_factor=0.5):   noisy = inputs + noise_factor * torch.randn_like(inputs)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 30,
    "source": "deep learning.pdf",
    "chunk_text": "return torch.clip(noisy, 0., 1.)    # (iv) TRAINING FUNCTION"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 31,
    "source": "deep learning.pdf",
    "chunk_text": "31    def train_model(model, train_loader, val_loader, epochs=10):   for epoch in range(epochs):   model.train()   train_loss = 0   for images, _ in train_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)   optimizer.zero_grad()   loss.backward()   optimizer.step()   train_loss += loss.item()     model.eval()   val_loss = 0   with torch.no_grad():   for images, _ in val_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)   val_loss += loss.item()     print(f\"Epoch [{epoch+1}/{epochs}] ➤ Train Loss:  {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")    # Train the model  train_model(model,"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 31,
    "source": "deep learning.pdf",
    "chunk_text": "train_loader, val_loader)          # (i) LOAD THE MNIST DATASET  import torch  import torch.nn as nn  import torch.optim as optim  from torchvision import datasets, transforms  from torch.utils.data import DataLoader, random_split    # 📁 Transforms: Normalize and Convert to Tensors  transform = transforms.Compose([   transforms.ToTensor(), # Converts to [0,1]  ])    # Download MNIST  train_dataset = datasets.MNIST(root='./data', train=True, download=True,  transform=transform)  test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 32,
    "source": "deep learning.pdf",
    "chunk_text": "32    # Split training into train/val  train_data, val_data = random_split(train_dataset, [50000, 10000])    # Dataloaders  train_loader = DataLoader(train_data, batch_size=128, shuffle=True)  val_loader = DataLoader(val_data, batch_size=128, shuffle=False)  test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # (ii) DENOISING AUTOENCODER MODEL  class DenoisingAutoencoder(nn.Module):   def __init__(self):   super(DenoisingAutoencoder, self).__init__()   self.encoder = nn.Sequential(   nn.Linear(28*28, 128),   nn.ReLU(),   nn.Linear(128, 64),   )   self.decoder = nn.Sequential(   nn.Linear(64, 128),   nn.ReLU(),   nn.Linear(128, 28*28),   nn.Sigmoid() # keep output in [0, 1]   )     def forward(self, x):   x = x.view(x.size(0), -1) # Flatten input   encoded ="
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 32,
    "source": "deep learning.pdf",
    "chunk_text": "self.encoder(x)   decoded = self.decoder(encoded)   return decoded.view(x.size(0), 1, 28, 28)    # (iii) COMPILE THE MODEL  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  model = DenoisingAutoencoder().to(device)  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=0.001)    # Add Gaussian Noise  def add_noise(inputs, noise_factor=0.5):   noisy = inputs + noise_factor * torch.randn_like(inputs)   return torch.clip(noisy, 0., 1.)    # (iv) TRAINING FUNCTION  def train_model(model, train_loader, val_loader, epochs=10):   for epoch in range(epochs):   model.train()   train_loss = 0   for images, _ in train_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 32,
    "source": "deep learning.pdf",
    "chunk_text": "optimizer.zero_grad()   loss.backward()   optimizer.step()"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 33,
    "source": "deep learning.pdf",
    "chunk_text": "33 \n \n train_loss += loss.item() \n \n model.eval() \n val_loss = 0 \n with torch.no_grad(): \n for images, _ in val_loader: \n images = images.to(device) \n noisy_imgs = add_noise(images) \n outputs = model(noisy_imgs) \n loss = criterion(outputs, images) \n val_loss += loss.item() \n \n print(f\"Epoch [{epoch+1}/{epochs}] ➤ Train Loss: \n{train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\") \n \n# Train the model \ntrain_model(model, train_loader, val_loader) \n \n \n \n \n \n \n(OR) \n \n \n(b) \nUsing the PyTorch implementation of a Restricted Boltzmann Machine (RBM), develop a \ncollaborative filtering-based recommender system on the MovieLens dataset."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 33,
    "source": "deep learning.pdf",
    "chunk_text": "Answers: \n# Imports \nimport numpy as np \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torch.utils.data import DataLoader \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split \n \n# (1) LOAD AND PREPROCESS MOVIELENS DATA \n# Download from: https://files.grouplens.org/datasets/movielens/ml-100k.zip and extract \nit. \n \nratings = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \n\"timestamp\"]) \nn_users = ratings.user_id.nunique() \nn_items = ratings.item_id.nunique() \n \n# Create user-item matrix \ndef create_user_item_matrix(df, n_users, n_items): \n data_matrix = np.zeros((n_users, n_items), dtype=np.float32) \n for row in df.itertuples(): \n data_matrix[row.user_id - 1, row.item_id - 1] = row.rating \nCO4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 34,
    "source": "deep learning.pdf",
    "chunk_text": "34     return data_matrix    data = create_user_item_matrix(ratings, n_users, n_items)  train_data, test_data = train_test_split(data, test_size=0.2)    train_tensor = torch.FloatTensor(train_data)  test_tensor = torch.FloatTensor(test_data)    # (2) DEFINE RESTRICTED BOLTZMANN MACHINE (RBM)  class RBM(nn.Module):   def __init__(self, n_visible, n_hidden):   super(RBM, self).__init__()   self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)   self.h_bias = nn.Parameter(torch.zeros(n_hidden))   self.v_bias = nn.Parameter(torch.zeros(n_visible))     def sample_h(self, v):   wx = torch.matmul(v, self.W.t()) + self.h_bias   prob = torch.sigmoid(wx)   return prob, torch.bernoulli(prob)     def sample_v(self, h):   wx = torch.matmul(h, self.W) + self.v_bias   prob = torch.sigmoid(wx)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 34,
    "source": "deep learning.pdf",
    "chunk_text": "return prob, torch.bernoulli(prob)     def forward(self, v):   h_prob, h_sample = self.sample_h(v)   v_prob, v_sample = self.sample_v(h_sample)   return v_prob     def contrastive_divergence(self, v0, k=1, lr=0.01):   v = v0.clone()   for step in range(k):   h_prob0, h0 = self.sample_h(v)   v_prob, v = self.sample_v(h0)   h_prob1, _ = self.sample_h(v)     # Update weights   self.W.data += lr * (torch.matmul(h_prob0.t(), v0) - torch.matmul(h_prob1.t(), v)) /  v0.size(0)   self.v_bias.data += lr * torch.sum(v0 - v, dim=0) / v0.size(0)   self.h_bias.data += lr * torch.sum(h_prob0 - h_prob1, dim=0) / v0.size(0)     loss = torch.mean((v0 - v) ** 2)   return loss    # ⚙ (3) COMPILE MODEL  n_visible = n_items  n_hidden = 64  rbm = RBM(n_visible, n_hidden)    # (4) TRAIN RBM MODEL  def"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 34,
    "source": "deep learning.pdf",
    "chunk_text": "train_rbm(rbm, train_tensor, epochs=10, batch_size=64, lr=0.01):"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 35,
    "source": "deep learning.pdf",
    "chunk_text": "35     train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)   for epoch in range(epochs):   total_loss = 0   for batch in train_loader:   batch = batch.clone()   batch[batch == 0] = -1 # mask unrated items   loss = rbm.contrastive_divergence(batch, k=1, lr=lr)   total_loss += loss.item()   print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")    train_rbm(rbm, train_tensor)    # (5) MAKE RECOMMENDATIONS  def recommend(rbm, user_vector, top_k=10):   with torch.no_grad():   input_vec = user_vector.clone()   input_vec[input_vec == 0] = -1 # mask missing ratings   output = rbm(input_vec)   predicted_ratings = output[0]   recommended_items = torch.argsort(predicted_ratings, descending=True)   return recommended_items[:top_k]    # Example: Recommend"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 35,
    "source": "deep learning.pdf",
    "chunk_text": "for user 0  user_id = 0  user_vector = train_tensor[user_id].unsqueeze(0)  recommendations = recommend(rbm, user_vector)  print(\"Top recommended movie indices for user 0:\", recommendations.tolist())        # Imports  import numpy as np  import torch  import torch.nn as nn  import torch.optim as optim  from torch.utils.data import DataLoader  import pandas as pd  from sklearn.model_selection import train_test_split    # (1) LOAD AND PREPROCESS MOVIELENS DATA  # Download from: https://files.grouplens.org/datasets/movielens/ml-100k.zip and extract it   ratings = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\",  \"timestamp\"])  n_users = ratings.user_id.nunique()  n_items = ratings.item_id.nunique()    # Create user-item matrix  def create_user_item_matrix(df,"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 35,
    "source": "deep learning.pdf",
    "chunk_text": "n_users, n_items):   data_matrix = np.zeros((n_users, n_items), dtype=np.float32)   for row in df.itertuples():   data_matrix[row.user_id - 1, row.item_id - 1] = row.rating"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 36,
    "source": "deep learning.pdf",
    "chunk_text": "36     return data_matrix    data = create_user_item_matrix(ratings, n_users, n_items)  train_data, test_data = train_test_split(data, test_size=0.2)    train_tensor = torch.FloatTensor(train_data)  test_tensor = torch.FloatTensor(test_data)    # (2) DEFINE RESTRICTED BOLTZMANN MACHINE (RBM)  class RBM(nn.Module):   def __init__(self, n_visible, n_hidden):   super(RBM, self).__init__()   self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)   self.h_bias = nn.Parameter(torch.zeros(n_hidden))   self.v_bias = nn.Parameter(torch.zeros(n_visible))     def sample_h(self, v):   wx = torch.matmul(v, self.W.t()) + self.h_bias   prob = torch.sigmoid(wx)   return prob, torch.bernoulli(prob)     def sample_v(self, h):   wx = torch.matmul(h, self.W) + self.v_bias   prob = torch.sigmoid(wx)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 36,
    "source": "deep learning.pdf",
    "chunk_text": "return prob, torch.bernoulli(prob)     def forward(self, v):   h_prob, h_sample = self.sample_h(v)   v_prob, v_sample = self.sample_v(h_sample)   return v_prob     def contrastive_divergence(self, v0, k=1, lr=0.01):   v = v0.clone()   for step in range(k):   h_prob0, h0 = self.sample_h(v)   v_prob, v = self.sample_v(h0)   h_prob1, _ = self.sample_h(v)     # Update weights   self.W.data += lr * (torch.matmul(h_prob0.t(), v0) - torch.matmul(h_prob1.t(), v)) /  v0.size(0)   self.v_bias.data += lr * torch.sum(v0 - v, dim=0) / v0.size(0)   self.h_bias.data += lr * torch.sum(h_prob0 - h_prob1, dim=0) / v0.size(0)     loss = torch.mean((v0 - v) ** 2)   return loss    # ⚙ (3) COMPILE MODEL  n_visible = n_items  n_hidden = 64  rbm = RBM(n_visible, n_hidden)    # (4) TRAIN RBM MODEL"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 37,
    "source": "deep learning.pdf",
    "chunk_text": "37    def train_rbm(rbm, train_tensor, epochs=10, batch_size=64, lr=0.01):   train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)   for epoch in range(epochs):   total_loss = 0   for batch in train_loader:   batch = batch.clone()   batch[batch == 0] = -1 # mask unrated items   loss = rbm.contrastive_divergence(batch, k=1, lr=lr)   total_loss += loss.item()   print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")    train_rbm(rbm, train_tensor)    # (5) MAKE RECOMMENDATIONS  def recommend(rbm, user_vector, top_k=10):   with torch.no_grad():   input_vec = user_vector.clone()   input_vec[input_vec == 0] = -1 # mask missing ratings   output = rbm(input_vec)   predicted_ratings = output[0]   recommended_items = torch.argsort(predicted_ratings,"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 37,
    "source": "deep learning.pdf",
    "chunk_text": "descending=True)   return recommended_items[:top_k]    # Example: Recommend for user 0  user_id = 0  user_vector = train_tensor[user_id].unsqueeze(0)  recommendations = recommend(rbm, user_vector)  print(\"Top recommended movie indices for user 0:\", recommendations.tolist())               ____________________            For Set 1 QP    Part - A  Question No."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 37,
    "source": "deep learning.pdf",
    "chunk_text": "1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK3 \nK4 \nK3 \nK2 \nK4 \nK4 \nK2 \nK2 \nK2 \nK3 \nDifficulty Level \n3 \n3 \n4 \n2 \n3 \n3 \n3 \n2 \n3 \n2 \n \nPart - B \nPart - C \nQuestion No. \n11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK2 \nK6 \nK6 \nK2 \nK6 \nK3 \nK3 \nK2 \nK2 \nK2 \nK6 \nK4 \nDifficulty Level \n3 \n3 \n4 \n3 \n3 \n3 \n3 \n3 \n3 \n3 \n4 \n3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 38,
    "source": "deep learning.pdf",
    "chunk_text": "38 \n \nSET 2 (Answer Key) \n \n \n \nCommon to (AI-DS, AI-ML) \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nCreate the model for the following network using PyTorch. \n \nAnswer: \n \n \nCO1 \n2. \nHow does dropout regularization work in neural networks? \nAnswer: \nDropout regularization randomly disables a fraction of neurons during training to prevent \noverfitting. This forces the network to learn more robust and generalized features. \n \nCO1 \n3. \nSummarize the key concepts and principles that drive different strategies in transfer learning. \nAnswer: \nDeep Transfer Learning Strategies \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 39,
    "source": "deep learning.pdf",
    "chunk_text": "39 \n \n• Direct use of pre-trained models \n• Leveraging feature extraction from pre-trained models \n• Fine-tuning layers of pre-trained models \n4. \nDetermine the convolution layer output shape if the input image shape is 128x128x3 and it uses 32 \nfilters of size 5x5 with a stride of 1 and padding of 2. \nAnswer: \n \nCO2 \n5. \nDistinguish between the phenomena of exploding gradients and vanishing gradients. \nAnswer: \nExploding gradients occur when weights grow too large during backpropagation, making training \nunstable. \n Vanishing gradients happen when gradients shrink, causing very slow or no learning, especially \nin deep RNNs. \nCO3 \n6. \nWhy do we need bidirectional RNNs in sequence modeling?"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 39,
    "source": "deep learning.pdf",
    "chunk_text": "Answer: \nBidirectional RNNs process data in both forward and backward directions, capturing past and future \ncontext, which improves performance in tasks like sentiment analysis and speech recognition. \nCO3 \n7. \nMention one key advantage of using denoising autoencoders for feature learning. \nAnswer: \nOne key advantage of using denoising autoencoders for feature learning is that they learn robust \nand meaningful features by reconstructing the original input from a corrupted version, helping the \nmodel generalize better and resist noise in real-world data. \nCO4 \n8. \nHow do the Generator and Discriminator in GANs engage in a competitive learning process? \nAnswer: \nThe generator tries to produce realistic data, while the discriminator tries to distinguish real \nfrom fake data."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 39,
    "source": "deep learning.pdf",
    "chunk_text": "Each improves by competing against the other in a minimax game. \nCO4 \n9. \nDifferentiate semantic segmentation and instance segmentation. \nAnswer: \nSemantic Segmentation classifies each pixel in an image into a category (e.g., sky, road), but does \nnot differentiate between objects of the same class. \nInstance Segmentation not only classifies each pixel but also distinguishes between individual \ninstances of the same class (e.g., multiple cars). \n \nCO5 \n10. \nDefine non-maximum suppression and explain how it helps refine the output of object detection \nmodels. \nAnswer: \nNon-Maximum Suppression (NMS) removes overlapping bounding boxes by keeping only the \none with the highest confidence score, helping to eliminate duplicate detections. \nCO5 \n \n PART B (5 x 13 = 65 marks)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 40,
    "source": "deep learning.pdf",
    "chunk_text": "40 \n \n \n \n \nCO \n11. \n(a) \nWrite a PyTorch program to implement a linear regression model using the dataset where \ny=2X+1+e, with X ranging from 1 to 50 and e as random noise. Define a linear regression \nmodel, use Mean Squared Error (MSE) as the loss function, and optimize it with Stochastic \nGradient Descent (SGD). Train the model for 50 epochs, ensuring the inclusion of the \nforward pass, loss computation, backpropagation, and parameter updates. Finally, display \nthe learned parameters (weights and bias) after training \n \n \nCO1"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 41,
    "source": "deep learning.pdf",
    "chunk_text": "41 \n \n \n \n \n \n \n(OR) \n \n \n(b) \n(i) Differentiate between shallow and deep neural networks. (6 Marks) \nCO1"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 42,
    "source": "deep learning.pdf",
    "chunk_text": "42 \n \n \n(ii) Explain the concept of loss functions in deep learning. Discuss popular loss functions \nsuch as mean squared error (MSE), categorical cross-entropy, and binary cross-entropy. (7 \nMarks)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 43,
    "source": "deep learning.pdf",
    "chunk_text": "43 \n \n \n \n \n \n \n \n \n \n12. \n(a) \nImplement transfer learning by using VGG as the base model to classify the CIFAR-100 \ndataset using PyTorch. \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 44,
    "source": "deep learning.pdf",
    "chunk_text": "44 \n \ni) Load the CIFAR-100 dataset (3 Mark) \nii) Use a pre-trained VGG model as the base model and modify its classifier for \nCIFAR-100. (3 Mark) \niii) Create a sequential model with the appropriate number of neurons in the \noutput layer, activation function, and loss function (3 Mark) \niv) Train the model with training data and validate it using the test dataset, and \nevaluate its accuracy. (4 Mark)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 45,
    "source": "deep learning.pdf",
    "chunk_text": "45 \n \n \n \n \n \n(OR) \n \n \n(b) \nExplain data augmentation and its various techniques, providing illustrative examples for \neach approach. \nData augmentation \n· Data augmentation is a process of artificially increasing the amount of data by \ngenerating new data points from existing data. \n· Data augmentation includes adding minor alterations to data or using machine \nlearning models to generate new data points in the latent space of original data to \namplify the dataset. \nSynthetic data: When data is generated artificially without using real-world images."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 45,
    "source": "deep learning.pdf",
    "chunk_text": "Synthetic data \nare often produced by Generative Adversarial Networks \nAugmented data: Derived from original images with some sort of minor geometric \ntransformations (such as flipping, translation, rotation, or the addition of noise) in order to increase \nthe diversity of the training set. \nData Augmentation Techniques \n· Flip \n· Rotation \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 46,
    "source": "deep learning.pdf",
    "chunk_text": "46 \n \n· Scale \n· Crop \n· Translation \n· Gaussian Noise \nFlip \n· Images can be flipped horizontally and vertically. \n· A vertical flip is equivalent to rotating an image by 180 degrees and then performing \na horizontal flip. \n \nRotation \n· One key thing to note about this operation is that image dimensions may not be \npreserved after rotation. \n· If your image is a square, rotating it at right angles will preserve the image size. \n· If it’s a rectangle, rotating it by 180 degrees would preserve the size. \n· Rotating the image by finer angles will also change the final image size. \n \n· The image can be scaled outward or inward. \n· While scaling outward, the final image size will be larger than the original image \nsize. \n· Scaling inward reduces the image size."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 46,
    "source": "deep learning.pdf",
    "chunk_text": "Crop \n· Unlike scaling, we just randomly sample a section from the original image. \n· We then resize this section to the original image size. \n· This method is popularly known as random cropping."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 47,
    "source": "deep learning.pdf",
    "chunk_text": "47 \n \nTranslation \n· Translation just involves moving the image along the X or Y direction (or both). \n· This method of augmentation is very useful as most objects can be located at almost \nanywhere in the image. \n \nGaussian Noise \nOver-fitting usually happens when your neural network tries to learn high frequency features \n(patterns that occur a lot) that may not be useful. Gaussian noise, which has zero mean, essentially \nhas data points in all frequencies, effectively distorting the high frequency features. This also means \nthat lower frequency components (usually, your intended data) are also distorted, but your neural \nnetwork can learn to look past that. Adding just the right amount of noise can enhance the learning \ncapability. \n \n \n \n \n \n \n \n13."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 47,
    "source": "deep learning.pdf",
    "chunk_text": "(a) \nDiscuss the vanishing and exploding gradient problems in RNNs. How do LSTM and \nGRU architectures mitigate these issues? \nVanishing and Exploding Gradient Problems in RNNs: \n● Vanishing Gradient Problem: \n○ Description: During backpropagation through time (BPTT), gradients \nshrink exponentially as they are propagated back through many time steps. \nThis is because the derivatives of activation functions (like sigmoid, \ncommonly used in older RNNs) are often small (e.g., between 0 and 0.25 \nfor sigmoid). When these small values are multiplied together over many \nlayers/time steps (due to the chain rule), the gradient quickly approaches \nzero."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 47,
    "source": "deep learning.pdf",
    "chunk_text": "○ Impact: Small or vanishing gradients make it difficult for the network to \nlearn long-term dependencies, meaning information from earlier parts of a \nsequence has little influence on updates to weights that affect later parts. \nThis prevents the RNN from capturing relationships between distant \nelements in a sequence, such as words far apart in a long sentence. \n● Exploding Gradient Problem: \n○ Description: While less emphasized in the provided text, the chain rule \ncan also lead to gradients becoming extremely large. This happens when \nthe values being multiplied in the chain rule are consistently large, causing \nthe gradient to grow exponentially. \n○ Impact: Exploding gradients can lead to unstable training, where the \nCO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 48,
    "source": "deep learning.pdf",
    "chunk_text": "48 \n \nmodel weights receive very large updates, causing the learning process to \ndiverge and the model to perform poorly. \nHow LSTM and GRU Architectures Mitigate These Issues: \nLSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) architectures were \nspecifically designed to address the vanishing and exploding gradient problems prevalent \nin simple RNNs. They achieve this through their unique gating mechanisms: \n● LSTMs (Long Short-Term Memory RNNs): \n \n○ LSTMs introduce a \"cell state\" and various \"gates\" (input gate, forget gate, \nand output gate) that regulate the flow of information. \n○ The cell state acts as a long-term memory, capable of carrying information \nacross many time steps without significant degradation."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 48,
    "source": "deep learning.pdf",
    "chunk_text": "○ The forget gate determines what information to discard from the cell state, \npreventing old, irrelevant information from accumulating. \n○ The input gate controls which new information gets stored in the cell \nstate. \n○ The output gate controls what part of the cell state is outputted as the \nhidden state. \n○ These gates, implemented with sigmoid and tanh activation functions, \nallow LSTMs to selectively remember or forget information, creating a \n\"constant error carousel\" that helps gradients flow more effectively over \nlong distances, thus mitigating the vanishing gradient problem. \n● GRUs (Gated Recurrent Unit RNNs): \n \n○ GRUs are a simplified version of LSTMs, designed to be computationally \nmore efficient while still effectively addressing the gradient problems."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 48,
    "source": "deep learning.pdf",
    "chunk_text": "○ They combine the forget and input gates into a single \"update gate\" and \nalso have a \"reset gate.\" \n○ The update gate determines how much of the past information (from the \nprevious hidden state) should be passed on to the current hidden state and"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 49,
    "source": "deep learning.pdf",
    "chunk_text": "49 \n \nhow much new information should be incorporated. \n○ The reset gate decides how much of the previous hidden state to forget \nbefore computing the new hidden state. \n○ By having fewer gates than LSTMs, GRUs offer a more streamlined \nstructure that still allows for better control over information flow and helps \nto preserve gradients over longer sequences, combating both vanishing and \nexploding gradients. \nBoth LSTMs and GRUs enable RNNs to capture long-term dependencies that simple \nRNNs struggle with, making them highly effective for sequential data tasks like text \ngeneration and speech recognition. \n \n \n(OR) \n \n \n(b) \nExplain BiRNN and the need for bidirectional traversal with the Pytorch implementation. \n· BiRNN stands for Bidirectional Recurrent Neural Network."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 49,
    "source": "deep learning.pdf",
    "chunk_text": "· It is an extension of a standard RNN where two RNNs are run: \n· One processes the input forward (from beginning to end). \n· The other processes the input backward (from end to beginning). \n· The outputs from both directions are then combined (usually concatenated or \nsummed) at each time step. \n \n· Consider the sentence: \n\"Michael Eats Dosa in Chennai\" \n· To classify words like \"dosa\", the model needs past context (\"eats\") \nand for \"Chennai\", the model needs future context (\"in\"). \nBenefits of BiRNNs: \n· Better context understanding in sequence tasks (e.g., language modeling, \nspeech recognition). \n· Improved accuracy in tasks like: \no Named Entity Recognition \nCO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 50,
    "source": "deep learning.pdf",
    "chunk_text": "50    o Part-of-Speech Tagging  o Sentiment Analysis  o Machine Translation    import torch  import torch.nn as nn    class BiRNN(nn.Module):   def __init__(self, input_size, hidden_size, output_size, num_layers=1):   super(BiRNN, self).__init__()   self.hidden_size = hidden_size   self.num_layers = num_layers     # Bidirectional RNN   self.rnn = nn.RNN(input_size, hidden_size, num_layers,   batch_first=True, bidirectional=True)     # The output layer uses hidden_size * 2 because of bidirection   self.fc = nn.Linear(hidden_size * 2, output_size)     def forward(self, x):   # Initialize hidden state: (num_layers * 2, batch_size, hidden_size)   h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)     # Forward propagate RNN   out, _ = self.rnn(x, h0)     # Pass the last time"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 50,
    "source": "deep learning.pdf",
    "chunk_text": "step output from both directions to the FC layer   out = self.fc(out[:, -1, :]) # shape: (batch_size, output_size)   return out      model = BiRNN(input_size, hidden_size, output_size)            14."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 50,
    "source": "deep learning.pdf",
    "chunk_text": "(a) \nExplain Sparse Autoencoders using L1 regularization and KL divergence. Also, \nimplement a sparse autoencoder with PyTorch implementation. \nAutoencoders are a type of neural network that learns a compressed, distributed \nrepresentation (encoding) of input data in an unsupervised manner. They consist of two \nmain parts: an encoder that maps the input to a hidden (latent) representation, and a \ndecoder that reconstructs the input from this hidden representation. The goal is to minimize \nthe reconstruction error between the input and the output. \nSparse Autoencoders \nWhile standard autoencoders aim to learn an efficient representation, they can sometimes \nCO4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 51,
    "source": "deep learning.pdf",
    "chunk_text": "51 \n \nlearn the identity function if the hidden layer has enough capacity, leading to poor feature \nextraction and overfitting. Sparse autoencoders address this by introducing a sparsity \nconstraint on the hidden layer activations. This constraint encourages the hidden units to \nbe mostly inactive (their activations are close to zero) for any given input. This forces the \nnetwork to learn a more distributed representation where different hidden units specialize \nin detecting different features in the input data. \nThere are primarily two common ways to enforce sparsity in autoencoders: \n1. L1 Regularization \n2."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 51,
    "source": "deep learning.pdf",
    "chunk_text": "KL Divergence \nL1 Regularization for Sparsity \nL1 regularization (also known as Lasso regularization) adds a penalty term to the loss \nfunction that is proportional to the absolute value of the hidden unit activations. \nThe total loss function for an L1 sparse autoencoder is given by: \n \nWhere: \n● x is the input data. \n● x′ is the reconstructed output. \n● ∥x−x′∥2 is the reconstruction loss, often Mean Squared \nError (MSE). \n● λ is the regularization parameter, controlling the strength of the sparsity penalty. \nA larger λ promotes more sparsity. \n● aj is the activation of the j-th hidden unit. \n● s is the number of hidden units."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 51,
    "source": "deep learning.pdf",
    "chunk_text": "By penalizing the sum of absolute activations, L1 regularization encourages many of the \naj values to become exactly zero, effectively \"turning off\" a significant portion of the \nhidden units for a given input. This directly promotes sparsity. \nKL Divergence for Sparsity \nInstead of directly penalizing the absolute activations, KL divergence enforces sparsity by \nencouraging the average activation of each hidden unit to be close to a predefined small \nvalue, ρ (rho), which represents the desired sparsity level (e.g., 0.05 or 0.1). \nThe Kullback-Leibler (KL) divergence measures the difference between two probability \ndistributions."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 51,
    "source": "deep learning.pdf",
    "chunk_text": "In the context of sparse autoencoders, we treat the average activation of a \nhidden unit as a Bernoulli random variable with a probability of activation ρ^j, and we want \nthis to be close to a target Bernoulli distribution with probability ρ."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 52,
    "source": "deep learning.pdf",
    "chunk_text": "52 \n \nThe KL divergence term for a single hidden unit j is: \n \nWhere: \n● β is the weight of the sparsity penalty, controlling its influence on the total loss. \n● The other terms are as defined for L1 regularization. \nThe KL divergence term is minimized when ρ^j is close to ρ. If ρ^j is much larger than ρ, \nthe penalty increases, pushing ρ^j down."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 52,
    "source": "deep learning.pdf",
    "chunk_text": "If ρ^j is much smaller than ρ, the penalty also  increases (though less drastically for very small ρ^j), encouraging some activation    import torch.nn.functional as F  from torchvision import datasets, transforms  from torch.utils.data import DataLoader    # Hyperparameters  input_size = 784 # For MNIST  hidden_size = 128  batch_size = 64  learning_rate = 1e-3  num_epochs = 10  l1_lambda = 1e-5 # L1 regularization coefficient    # Dataset  transform = transforms.ToTensor()  train_loader = DataLoader(   datasets.MNIST(root='./data', train=True, download=True, transform=transform),   batch_size=batch_size,   shuffle=True  )    # Autoencoder with L1 sparsity  class SparseAutoencoder(nn.Module):   def __init__(self):   super(SparseAutoencoder, self).__init__()   self.encoder ="
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 52,
    "source": "deep learning.pdf",
    "chunk_text": "nn.Linear(input_size, hidden_size)   self.decoder = nn.Linear(hidden_size, input_size)     def forward(self, x):   x = x.view(-1, input_size)   hidden = torch.sigmoid(self.encoder(x))   output = torch.sigmoid(self.decoder(hidden))   return output, hidden    model = SparseAutoencoder()  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 53,
    "source": "deep learning.pdf",
    "chunk_text": "53 \n \n# Training \nfor epoch in range(num_epochs): \n total_loss = 0 \n for batch, _ in train_loader: \n batch = batch.view(-1, input_size) \n optimizer.zero_grad() \n outputs, hidden = model(batch) \n mse_loss = criterion(outputs, batch) \n l1_loss = l1_lambda * torch.sum(torch.abs(hidden)) \n loss = mse_loss + l1_loss \n loss.backward() \n optimizer.step() \n total_loss += loss.item() \n \n print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\") \n \n \n \n \n(OR) \n \n \n(b) \nExplain the architecture and working of Generative Adversarial Networks (GANs). \n \nA GAN consists of two primary neural networks: \n1. Generator (G): This network is responsible for creating new data samples (e.g., \nimages, text, audio) that are as realistic as possible, mimicking the real data \ndistribution."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 53,
    "source": "deep learning.pdf",
    "chunk_text": "It takes a random noise vector as input, often sampled from a \nsimple distribution like a Gaussian or uniform distribution. This noise vector \nacts as the \"creative seed\" for the generator. The generator's architecture \ntypically involves layers that upsample this noise into the desired data format \n(e.g., transposed convolutional layers for image generation). \n2. Discriminator (D): This network acts as a \"critic\" or \"authenticator.\" It takes \nan input data sample and outputs a probability (a value between 0 and 1) \nindicating whether it believes the sample is real (from the actual training \ndataset) or fake (generated by the generator)."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 53,
    "source": "deep learning.pdf",
    "chunk_text": "The discriminator is typically a \nstandard classifier, often using convolutional layers for image data, that learns \nto distinguish between real and fake inputs. \nHow GANs Work (The Adversarial Process) \nThe working principle of GANs can be likened to a game between a forger (the \nGenerator) and a detective (the Discriminator): \n● The Forger (Generator): Tries to create fake banknotes (data) that are so \nconvincing the detective can't tell them apart from real ones. \n● The Detective (Discriminator): Tries to become an expert at telling the \ndifference between real banknotes and fake ones. \nThis competition drives both networks to improve: \n1. Generator's Goal: To produce data that is indistinguishable from real data, \nthereby \"fooling\" the discriminator."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 53,
    "source": "deep learning.pdf",
    "chunk_text": "Its objective is to maximize the \nprobability of the discriminator making a mistake (classifying generated data \nCO4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 54,
    "source": "deep learning.pdf",
    "chunk_text": "54 \n \nas real). \n2. Discriminator's Goal: To accurately distinguish between real and fake data. \nIts objective is to minimize the probability of making mistakes (correctly \nclassifying real data as real and generated data as fake) \n \n \n \n \n \n \n \n15. \n(a) \nExplain the architecture of Fast R-CNN for object detection. Describe the role of Selective \nSearch, ROI Pooling, and the two output branches in the detection pipeline. \n \n \nCO5"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 55,
    "source": "deep learning.pdf",
    "chunk_text": "55 \n \n \n \n \n \n \n \n \n \n(OR) \n \n \n(b) \nExplain the working of a Single Shot Detector (SSD) and discuss how it effectively \ndetects objects of varying sizes in an image. \nCO5"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 56,
    "source": "deep learning.pdf",
    "chunk_text": "56 \n \n \n \n \n \n \n \n PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \nCO \n16. \n(a) \nCreate the PyTorch implementation for the stock price prediction using RNN. \n(i) Implement the input preprocessing . (4 Marks) \n(ii) Implement a stock price prediction class with an appropriate loss function and optimizer \nfor multi-class classification. (6 Marks) \n(iii) Implement a function to train the model using the training dataset. (5 Marks) \n \n \n \n \n \nCO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 57,
    "source": "deep learning.pdf",
    "chunk_text": "57        # Import Libraries  import numpy as np  import torch  import torch.nn as nn  from sklearn.preprocessing import MinMaxScaler  from sklearn.model_selection import train_test_split    # (i) INPUT PREPROCESSING — 4 Marks  np.random.seed(42)  prices = np.cumsum(np.random.randn(1000) * 2 + 0.1) + 100 # Random walk    def label_movement(prices):   movement = []   for i in range(1, len(prices)):   diff = prices[i] - prices[i - 1]   if diff > 0.5:   movement.append(2) # Up   elif diff < -0.5:   movement.append(0) # Down   else:   movement.append(1) # No Change   return np.array(movement)    labels = label_movement(prices)  scaler = MinMaxScaler()  scaled_prices = scaler.fit_transform(prices[:-1].reshape(-1, 1)) # exclude last price    def create_sequences(data, labels, seq_len=10):   X, y"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 57,
    "source": "deep learning.pdf",
    "chunk_text": "= [], []   for i in range(len(data) - seq_len):   X.append(data[i:i+seq_len])   y.append(labels[i+seq_len])   return np.array(X), np.array(y)    X, y = create_sequences(scaled_prices, labels)  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)  X_train = torch.tensor(X_train, dtype=torch.float32)  X_test = torch.tensor(X_test, dtype=torch.float32)  y_train = torch.tensor(y_train, dtype=torch.long)  y_test = torch.tensor(y_test, dtype=torch.long)    # (ii) MODEL DEFINITION + LOSS + OPTIMIZER — 6 Marks  class StockRNN(nn.Module):   def __init__(self, input_size=1, hidden_size=64, num_layers=1, num_classes=3):   super(StockRNN, self).__init__()   self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)   self.fc = nn.Linear(hidden_size,"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 57,
    "source": "deep learning.pdf",
    "chunk_text": "num_classes)   def forward(self, x):   out, _ = self.rnn(x)   out = out[:, -1, :] # Last timestep   return self.fc(out)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 58,
    "source": "deep learning.pdf",
    "chunk_text": "58    model = StockRNN()  criterion = nn.CrossEntropyLoss()  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)    # (iii) TRAINING FUNCTION — 5 Marks  def train_model(model, X_train, y_train, epochs=20):   model.train()   for epoch in range(epochs):   optimizer.zero_grad()   output = model(X_train)   loss = criterion(output, y_train)   loss.backward()   optimizer.step()   _, preds = torch.max(output, 1)   acc = (preds == y_train).float().mean()   print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Accuracy:  {acc.item():.4f}\")    train_model(model, X_train, y_train)    # (Optional) EVALUATION  def evaluate(model, X_test, y_test):   model.eval()   with torch.no_grad():   output = model(X_test)   _, preds = torch.max(output, 1)   acc = (preds == y_test).float().mean()"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 58,
    "source": "deep learning.pdf",
    "chunk_text": "print(f\"Test Accuracy: {acc.item():.4f}\")    evaluate(model, X_test, y_test)          (OR)      (b)    Develop a Named Entity Recognition (NER) system using PyTorch with the following  requirements:  (i) Implement an LSTM-based neural network architecture  (ii) Include proper data preprocessing for sequence labeling  (iii) Compile the model with appropriate loss function and optimizer  (iv) Train the model and evaluate its performance    # Imports  import torch  import torch.nn as nn  from collections import defaultdict  from sklearn.metrics import classification_report  CO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 59,
    "source": "deep learning.pdf",
    "chunk_text": "59      # Sample Data (Tiny Dataset for Demo)  train_data = [   [(\"John\", \"B-PER\"), (\"lives\", \"O\"), (\"in\", \"O\"), (\"New\", \"B-LOC\"), (\"York\", \"ILOC\")],   [(\"Mary\", \"B-PER\"), (\"is\", \"O\"), (\"from\", \"O\"), (\"Paris\", \"B-LOC\")],  ]  val_data = [   [(\"Steve\", \"B-PER\"), (\"works\", \"O\"), (\"in\", \"O\"), (\"London\", \"B-LOC\")]  ]    # Preprocessing - Build Vocabulary  word2idx = defaultdict(lambda: len(word2idx)); tag2idx = defaultdict(lambda:  len(tag2idx))  PAD, UNK = \"<PAD>\", \"<UNK>\"; word2idx[PAD]; word2idx[UNK]; tag2idx[PAD]  for sentence in train_data:   for word, tag in sentence:   word2idx[word]; tag2idx[tag]  max_len = max(len(s) for s in train_data + val_data)    # Encode Sentences  def encode(sentence, word2idx, tag2idx, max_len):   words = [word for word, tag in sentence]   tags = [tag for word,"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 59,
    "source": "deep learning.pdf",
    "chunk_text": "tag in sentence]   word_ids = [word2idx.get(w, word2idx[UNK]) for w in words]   tag_ids = [tag2idx[t] for t in tags]   while len(word_ids) < max_len:   word_ids.append(word2idx[PAD])   tag_ids.append(tag2idx[PAD])   return word_ids, tag_ids    X_train = torch.tensor([encode(s, word2idx, tag2idx, max_len)[0] for s in train_data])  y_train = torch.tensor([encode(s, word2idx, tag2idx, max_len)[1] for s in train_data])  X_val = torch.tensor([encode(s, word2idx, tag2idx, max_len)[0] for s in val_data])  y_val = torch.tensor([encode(s, word2idx, tag2idx, max_len)[1] for s in val_data])    # Define LSTM-based NER Model  class NERLSTM(nn.Module):   def __init__(self, vocab_size, tagset_size, embedding_dim=64, hidden_dim=128):   super(NERLSTM, self).__init__()   self.embedding ="
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 59,
    "source": "deep learning.pdf",
    "chunk_text": "nn.Embedding(vocab_size, embedding_dim,  padding_idx=word2idx[PAD])   self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,  bidirectional=True)   self.fc = nn.Linear(hidden_dim * 2, tagset_size)   def forward(self, x):   x = self.embedding(x)   x, _ = self.lstm(x)   return self.fc(x)    # Compile Model: Loss + Optimizer  model = NERLSTM(len(word2idx), len(tag2idx))  criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[PAD])  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 60,
    "source": "deep learning.pdf",
    "chunk_text": "60      # Train Model  def train(model, X, y, epochs=10):   model.train()   for epoch in range(epochs):   optimizer.zero_grad()   out = model(X).view(-1, len(tag2idx)) # (batch*seq_len, num_tags)   loss = criterion(out, y.view(-1))   loss.backward()   optimizer.step()   print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")  train(model, X_train, y_train)    # Evaluate Model  def evaluate(model, X, y_true):   model.eval()   with torch.no_grad():   y_pred = torch.argmax(model(X), dim=2)   true_labels, pred_labels = [], []   for true, pred in zip(y_true, y_pred):   for t, p in zip(true, pred):   if t != tag2idx[PAD]:   true_labels.append(t.item())   pred_labels.append(p.item())   idx2tag = {v: k for k, v in tag2idx.items()}   print(classification_report(   [idx2tag[i] for i in true_labels],"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 60,
    "source": "deep learning.pdf",
    "chunk_text": "[idx2tag[i] for i in pred_labels],   target_names=[k for k in tag2idx if k != PAD]   ))  evaluate(model, X_val, y_val)       _____________________              For Set 2 QP    Part - A  Question No."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 60,
    "source": "deep learning.pdf",
    "chunk_text": "1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK6 \nK2 \nK2 \nK3 \nK4 \nK2 \nK2 \nK4 \nK4 \nK2 \nDifficulty Level \n3 \n3 \n2 \n4 \n3 \n2 \n2 \n3 \n3 \n3 \n \n Part - B \nPart - C \nQuestion No. \n11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK3 \nK4 \nK6 \nK2 \nK3 \nK3 \nK3 \nK2 \nK4 \nK2 \nK6 \nK6 \nDifficulty Level \n3 \n3 \n4 \n3 \n3 \n3 \n3 \n3 \n3 \n3 \n4 \n4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 61,
    "source": "deep learning.pdf",
    "chunk_text": "61 \n \nSET 3 (Answer Key) \n \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nHow does the choice of a loss function impact the training of a neural network? \nAnswer: \nSelecting the right loss function is crucial in machine learning as it directly impacts the model's \ntraining and performance.The loss function measures the difference between the predicted and \nactual values, guiding the optimization process to improve the model \nCO1 \n2. \nHow does batch normalization help in accelerating the training of deep neural networks? \nAnswer: \nDropout regularization randomly disables a fraction of neurons during training to prevent \noverfitting."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 61,
    "source": "deep learning.pdf",
    "chunk_text": "This forces the network to learn more robust and generalized features. \nCO1 \n3. \nHow does data augmentation enhance the robustness of machine learning models? \nAnswer: \n• It reduces the cost of collection of data. \n• It reduces the cost of labelling data. \n• It improves the model prediction accuracy. \n• It prevents data scarcity. \n• It frames better data models. \n• It reduces data overfitting. \n• It creates variability and flexibility in data models. \n• It increases the generalization ability of the data models. \n• It helps in resolving the class imbalance issue in the classification \nCO2 \n4. \nDepict and explain the multiple filter processing for a three-channel image. \nAnswer: \n \nCO2 \n5. \nName the three gates in LSTM and state one function of each."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 61,
    "source": "deep learning.pdf",
    "chunk_text": "Answer: \n• Input gate: Controls what new information is stored. \n• Forget gate: Decides which old information to discard. \n• Output gate: Regulates the output from the current cell state. \nCO3 \n6. \nDifferentiate between one-to-many and many-to-many RNN architectures with examples. \nAnswer: \nLSTM has 3 gates: input, forget, and output; separates memory cell and hidden state. \nCO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 62,
    "source": "deep learning.pdf",
    "chunk_text": "62 \n \n \n \n \n PART B (5 x 13 = 65 marks) \n \n \n \n \n \nCO \n11. \n(a) \nDiscuss the significance of selecting appropriate loss functions for regression, binary \nclassification, and multiclass classification tasks, and demonstrate their application using \nPyTorch. \nAnswer: \n● Selecting the right loss function is crucial in machine learning as it directly impacts the \nmodel's training and performance. The loss function measures the difference between the \npredicted and actual values, guiding the optimization process to improve the model. \n1. Loss Functions for Regression Tasks \n \nIn regression problems, the output is a continuous numerical value. The most commonly \nused loss functions are: \nCO1 \nGRU has 2 gates: update and reset; combines memory and hidden state, making it faster. \n7."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 62,
    "source": "deep learning.pdf",
    "chunk_text": "Define Minimax Loss and Wasserstein Loss \nAnswer: \nMinimax Loss is used in standard GANs, where the generator tries to minimize and the \ndiscriminator tries to maximize the probability of correctly classifying real vs. fake data. \nWasserstein Loss, used in WGANs, replaces probability-based loss with the Earth Mover’s \nDistance, providing more stable training and better gradient flow. \nCO4 \n8. \nWhat is the function of the discriminator in a Generative Adversarial Network (GAN)? \nAnswer: \n• The function of the discriminator in a Generative Adversarial Network (GAN) is to \ndistinguish between real and fake data."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 62,
    "source": "deep learning.pdf",
    "chunk_text": "• It evaluates whether a given sample is from the real dataset or generated by the generator, \nand provides feedback to help both networks improve — encouraging the generator to \nproduce more realistic data. \nCO4 \n9. \nDifferentiate between R-CNN and SPPNet. \nAnswer: \n \nCO5 \n10. \nDefine panoptic segmentation. How does it combine the strengths of semantic and instance \nsegmentation? \nAnswer: \nPanoptic segmentation assigns both a class label and an instance ID to each pixel. It combines \nsemantic segmentation (classifying every pixel) with instance segmentation (differentiating \nobject instances). \nCO5"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 63,
    "source": "deep learning.pdf",
    "chunk_text": "63 \n \n· Mean Squared Error (MSE): Penalizes large errors more due to squaring. It is \nsensitive to outliers. \n \nWhere: n: number of examples or samples. y: true values. ^y: predicted values. \n● · \nsmaller value indicates that the ground truth and predicted values are closer \nto each other. \nImplementation : (using Class Based Approach) \n \nOr Implementation: (using API) \n \n· Mean Absolute Error (MAE): Less sensitive to outliers as it considers absolute \ndifferences. \n \nWhere: n: number of examples or samples. y: true values. ^y: predicted values. \n· Here also, a lower value indicates a better model \nImplementation : (using Class Based Approach)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 64,
    "source": "deep learning.pdf",
    "chunk_text": "64 \n \nImplementation: (using API) \n \n2. Loss Function for Binary Classification Tasks \nBinary classification problems involve two classes (e.g., spam vs. not spam). The most \ncommonly used loss function is: \n● Binary Cross-Entropy (BCE) Loss: Measures the difference between the predicted \nprobability and the actual class. \nImplementation : (using Class Based Approach) \n \nImplementation: (using API) \n \n3. Loss Function for Multiclass Classification Tasks \nFor multiclass classification problems where an instance belongs to one of multiple classes, the \ncommonly used loss function is: \n● Categorical Cross-Entropy (CCE) / Cross-Entropy Loss: Used for one-hot \nencoded labels and softmax outputs. \n \np is the softmax probability vector from the model's logits, and y is the ground truth label."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 65,
    "source": "deep learning.pdf",
    "chunk_text": "65 \n \nImplementation: (using API) \n \n \n \n(OR) \n \n \n(b) \n(i) Using PyTorch, implement a neural network to predict 'CompressiveStrength' using the \nremaining columns as input features. Create a model with three hidden layers, each having \n512 units and the ReLU activation, an output layer with one unit and no activation, and also \ninput_shape should be specified in the first layer. Apply the necessary preprocessing before \ntraining. (7Marks) \n(ii) Explain the concept of regularization in deep learning. Discuss common regularization \ntechniques such as L1 and L2 regularization, dropout, and batch normalization. (6 Marks) \n \n \nCO1"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 67,
    "source": "deep learning.pdf",
    "chunk_text": "67 \n \n \n(ii) Explain the concept of regularization in deep learning. Discuss common regularization \ntechniques such as L1 and L2 regularization, dropout, and batch normalization. (6 Marks) \nRegularization is a technique used in deep learning to prevent overfitting — a situation \nwhere a model performs well on training data but poorly on unseen data. Regularization \nintroduces additional information or constraints to the model, helping it generalize better to \nnew data. \nCommon Regularization Techniques: \n1. L1 Regularization (Lasso): \n○ Adds the absolute value of the weights to the loss function\n \n○ Encourages sparsity by driving some weights to zero. \n○ Useful for feature selection. \n2. L2 Regularization (Ridge):"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 68,
    "source": "deep learning.pdf",
    "chunk_text": "68 \n \n○ Adds the squared value of the weights to the loss function: \n \n○ Prevents large weights, promoting simpler models. \n○ Helps in weight decay and smoother generalization. \n3. Dropout: \n○ Randomly \"drops out\" (sets to zero) a percentage of neurons during training. \n○ Prevents co-adaptation of neurons. \n○ Forces the network to learn redundant, robust features. \n4. Batch Normalization: \n○ Normalizes the output of a layer for each mini-batch. \n○ Reduces internal covariate shift, improving training speed and stability. \nHas a regularizing effect and can sometimes reduce the need for dropout. \n \n \n \n \n12."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 68,
    "source": "deep learning.pdf",
    "chunk_text": "(a) \n(i) Develop the PyTorch implementation of the model with the input shape of (512,512,1) \n(7 Marks) \n \nimport torch \nimport torch.nn as nn \nclass CustomCNN(nn.Module): \n def __init__(self): \n super(CustomCNN, self).__init__() \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 69,
    "source": "deep learning.pdf",
    "chunk_text": "69     # Convolutional Layers   self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1,  padding='same') # Output: (32, 512, 512)   self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1,  padding='same') # Output: (64, 512, 512)   # Flatten layer   self.flatten = nn.Flatten() # Output: (64 * 512 * 512 = 16777216)   # Fully Connected Layer   self.dense = nn.Linear(16777216, 6) # Output: (6)   def forward(self, x):   x = torch.relu(self.conv1(x))   x = torch.relu(self.conv2(x))   x = self.flatten(x)   x = self.dense(x)   return x  # Instantiate the model  model = CustomCNN()  # Print model summary  print(model)  # Print total parameters  total_params = sum(p.numel() for p in model.parameters())  print(f\"Total trainable parameters:"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 69,
    "source": "deep learning.pdf",
    "chunk_text": "{total_params}\")    (ii) Explain the significance of the convolution layer in deep learning models, particularly  its role in feature extraction and pattern recognition, and provide insights into its impact?"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 69,
    "source": "deep learning.pdf",
    "chunk_text": "(6 Marks) \nThe convolutional layer plays a pivotal role in deep learning models, especially in tasks \nrelated to computer vision and image analysis."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 70,
    "source": "deep learning.pdf",
    "chunk_text": "70 \n \nFeature Extraction: \nConvolutional layers are designed to automatically learn and extract meaningful features \nfrom input data. In computer vision, these features could be edges, textures, shapes, or \nother higher-level patterns. \nThey use small kernels (also known as filters) that slide across the input data, capturing \nlocal information and producing feature maps. Each filter is responsible for detecting a \nspecific pattern or feature. \nThe hierarchical structure of convolutional layers allows the network to learn complex \nfeatures by combining simple ones, which is crucial for understanding and representing the \nunderlying structure of the data. \nPattern Recognition: \nConvolutional layers excel at pattern recognition in images."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 70,
    "source": "deep learning.pdf",
    "chunk_text": "By learning hierarchical \nfeatures, they can recognize intricate patterns, such as objects, shapes, or textures, at \nvarious levels of abstraction. \nThrough the use of multiple convolutional layers followed by pooling layers, deep learning \nmodels can gradually build up a hierarchy of features, enabling them to recognize \nincreasingly complex patterns and objects. \nThe ability to automatically learn these patterns from data makes convolutional layers \nhighly adaptable and capable of generalizing well to new, unseen examples. \n \n \n(OR) \n \n \n(b) \nDescribe the concept of Convolutional Neural Networks (CNNs) and their role in computer \nvision tasks. Explain the key components of CNNs, including convolutional layers, pooling \nlayers, and fully connected layers."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 70,
    "source": "deep learning.pdf",
    "chunk_text": "Discuss how CNNs automatically learn hierarchical \npatterns and extract meaningful features from images. \nDefinition: \nA Convolutional Neural Network (CNN) is a specialized deep learning architecture \ndesigned for processing grid-like data (e.g., images, videos). CNNs automatically learn \nhierarchical features through convolutional operations, making them the backbone of \nmodern computer vision. \nRole in Computer Vision: \nCNNs excel at tasks like: \n● Image Classification (e.g., identifying objects in photos). \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 71,
    "source": "deep learning.pdf",
    "chunk_text": "71 \n \n● Object Detection (e.g., localizing multiple objects in an image). \n● Semantic Segmentation (e.g., pixel-wise labeling of images). \n● Face Recognition, Medical Imaging, and more. \nKey Components of CNNs \n1. Convolutional Layers \n● Purpose: Extract local features (e.g., edges, textures) via learnable filters (kernels). \n● Operation: \n○ Slides a kernel (e.g., 3×3) across the input, computing dot products. \n○ Outputs a feature map highlighting detected patterns. \n● Parameters: \n○ kernel_size: Spatial dimensions of the filter (e.g., 3×3). \n○ stride: Step size of the kernel (default=1). \n○ padding: Preserves spatial dimensions (e.g., padding='same'). \n● Activation: ReLU introduces nonlinearity (e.g., ReLU(conv(x))). \n2."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 71,
    "source": "deep learning.pdf",
    "chunk_text": "Pooling Layers \n● Purpose: Reduce spatial dimensions (downsampling) to: \n○ Decrease computational cost. \n○ Introduce translation invariance. \n● Types: \n○ Max Pooling: Selects the maximum value in a window (e.g., 2×2). \n○ Average Pooling: Takes the mean value in a window. \n● Output: Smaller but deeper feature maps. \n3. Fully Connected (Dense) Layers \n● Purpose: Classify features extracted by convolutional/pooling layers. \n● Operation: Flattens the 3D feature maps into a 1D vector for traditional MLP-like \nclassification. \n● Use Case: Final layers in CNNs (e.g., for ImageNet’s 1000-class output)."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 72,
    "source": "deep learning.pdf",
    "chunk_text": "72 \n \nHow CNNs Learn Hierarchical Patterns \n1. Low-Level Features: \n○ Early layers detect edges, colors, and textures (via small kernels). \n2. Mid-Level Features: \n○ Deeper layers combine edges into shapes (e.g., circles, corners). \n3. High-Level Features: \n○ Final layers recognize complex objects (e.g., \"cat ears\" or \"car wheels\"). \nExample: \n● Input Image → Conv1 (Edges) → Conv2 (Textures) → Conv3 (Object Parts) → \nOutput (Full Object). \n \nWhy CNNs Outperform Traditional Methods \n● Parameter Sharing: Kernels are reused across the image, reducing parameters. \n● Local Connectivity: Focuses on local regions, not the entire image. \n● Hierarchical Learning: Mimics human visual perception (simple → complex)."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 72,
    "source": "deep learning.pdf",
    "chunk_text": "Applications of CNNs \nTask \nExample \nImage Classification \nResNet on ImageNet \nObject Detection \nYOLO, Faster R-CNN \nMedical Imaging \nTumor detection in MRI scans \nAutonomous Driving \nLane and pedestrian detection \n \n \n \n \n \n13. \n(a) \nExplain the necessity of word embedding in Natural Language Processing (NLP) and \nCO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 73,
    "source": "deep learning.pdf",
    "chunk_text": "73 \n \noutline its various methodologies. \n· Natural Language Processing (NLP) aims to enable computers to understand, \ninterpret, and generate human language.However, computers fundamentally operate on \nnumbers, not words. This is where word embeddings become indispensable. \nTraditional methods of representing words, like one-hot encoding or Bag-of-Words \n(BoW), have significant limitations: \n● Sparsity and High Dimensionality: One-hot encoding creates a vector where each \nword is represented by a unique dimension, resulting in extremely sparse (mostly \nzeros) and high-dimensional vectors, especially for large vocabularies. This is \ncomputationally expensive and inefficient."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 73,
    "source": "deep learning.pdf",
    "chunk_text": "● Lack of Semantic Meaning: Traditional methods treat each word as an \nindependent entity, failing to capture the semantic relationships between words. For \nexample, \"king\" and \"queen\" are just different words, with no inherent connection \nunderstood by the machine. Similarly, \"cat\" and \"feline\" would be seen as distinct, \neven though they are semantically similar. \n● No Contextual Understanding: These methods disregard the context in which a \nword appears. The same word can have different meanings depending on its \nsurrounding words (e.g., \"bank\" of a river vs. a financial \"bank\"), but traditional \nmethods assign a single representation."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 73,
    "source": "deep learning.pdf",
    "chunk_text": "● Inability to Generalize: If a model is trained on \"fish\" but encounters \"cod\" during \ntesting, it won't understand the semantic similarity if it hasn't seen \"cod\" before, \nleading to poor generalization. \nWord embeddings address these issues by providing dense, low-dimensional, and \nsemantically rich numerical representations of words. They bridge the gap between \nhuman language and machine understanding by: \n● Capturing Semantic and Syntactic Relationships: Words with similar meanings \nor that appear in similar contexts are mapped to vectors that are close to each other \nin the vector space. This allows for arithmetic operations (e.g., \"king\" - \"man\" + \n\"woman\" ≈ \"queen\"), demonstrating an understanding of analogies and \nrelationships."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 73,
    "source": "deep learning.pdf",
    "chunk_text": "● Dimensionality Reduction: Instead of sparse, high-dimensional vectors, word \nembeddings represent words as dense vectors of much lower dimensions (typically \ntens to hundreds). This significantly reduces computational complexity and"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 74,
    "source": "deep learning.pdf",
    "chunk_text": "74 \n \nmemory requirements. \n● Contextual Understanding (in advanced embeddings): Newer embedding \ntechniques can generate different vectors for the same word based on its context, \neffectively handling polysemy (words with multiple meanings). \n● Improved Performance in Downstream NLP Tasks: By providing a richer and \nmore efficient representation of words, embeddings significantly boost the \nperformance of various NLP tasks, including: \no Text Classification: Sentiment analysis, spam detection, topic categorization. \n○ Named Entity Recognition (NER): Identifying people, organizations, \nlocations. \n○ Machine Translation: Understanding word relationships across languages. \n○ Information Retrieval and Search Engines: More accurate matching of \nqueries to relevant documents."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 74,
    "source": "deep learning.pdf",
    "chunk_text": "○ Question Answering: Better understanding of questions and answers. \n○ Text Generation: Creating coherent and contextually relevant text. \nVarious Methodologies of Word Embedding \n· Word embedding methodologies can be broadly categorized into: \n1. Frequency-Based Methods \nThese methods rely on statistical measures of word co-occurrence within a corpus. \n● Bag-of-Words (BoW): \n○ Concept: Represents a document as a multiset of its words, disregarding \ngrammar and word order. It simply counts the frequency of each word in a \ndocument. \n○ Limitations: High dimensionality, sparsity, and no semantic meaning or \ncontextual understanding. It treats \"good\" and \"bad\" as equally distinct as \n\"cat\" and \"dog\"."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 74,
    "source": "deep learning.pdf",
    "chunk_text": "● Term Frequency-Inverse Document Frequency (TF-IDF): \n○ Concept: A statistical measure that evaluates how relevant a word is to a \ndocument in a collection of documents. It increases with the number of \ntimes a word appears in the document but is offset by the frequency of the"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 75,
    "source": "deep learning.pdf",
    "chunk_text": "75 \n \nword in the corpus. This helps to down-weight common words (like \"the\", \n\"a\") that provide little unique information. \n○ Limitations: Still results in sparse vectors and doesn't capture semantic \nrelationships between words directly. \n2. Prediction-Based (Neural Network-based) Methods \n· These methods learn word embeddings by training neural networks to predict words \nbased on their context, or vice-versa. \n● Word2Vec: \n○ Concept: Developed by Google, Word2Vec is a group of shallow, twolayer neural network models trained to reconstruct the linguistic context of \nwords. It learns to map semantically similar words to geometrically close \nembedding vectors."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 75,
    "source": "deep learning.pdf",
    "chunk_text": "It has two main architectures: \n■ Continuous Bag-of-Words (CBOW): Predicts the current word \nbased on its surrounding context words. It's generally faster and \nperforms well for frequent words. \n■ Skip-gram: Predicts the surrounding context words given a target \nword. It's better at capturing rare words and phrases. \n○ Key Feature: Captures semantic relationships (e.g., King - Man + Woman \n≈ Queen). \n● GloVe (Global Vectors for Word Representation): \n○ Concept: Developed by Stanford, GloVe combines the advantages of both \nglobal matrix factorization (like Latent Semantic Analysis) and local \ncontext window methods (like Word2Vec). It constructs word embeddings \nby analyzing global word-word co-occurrence statistics from a large corpus."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 75,
    "source": "deep learning.pdf",
    "chunk_text": "It explicitly models the ratio of co-occurrence probabilities. \n○ Key Feature: Captures both global statistical information and local \ncontextual information. \n● FastText: \n○ Concept: Developed by Facebook AI Research, FastText extends \nWord2Vec by treating each word as a \"bag of character n-grams.\" This \nmeans it considers subword information."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 76,
    "source": "deep learning.pdf",
    "chunk_text": "76 \n \n○ Key Feature: Effective for morphologically rich languages (where words \nhave many forms due to prefixes/suffixes) and can handle out-of-vocabulary \n(OOV) words by inferring their embeddings from their character n-grams. \n3. Contextualized Embeddings \nThese are more advanced methods that generate dynamic word embeddings, meaning the \nvector representation of a word changes based on its surrounding context within a \nsentence. This effectively addresses the issue of polysemy. \n● ELMo (Embeddings from Language Models): \n○ Concept: ELMo is a deep contextualized word representation that models \nboth complex characteristics of word use (e.g., syntax and semantics) and \nhow these uses vary across linguistic contexts (e.g., to model polysemy)."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 76,
    "source": "deep learning.pdf",
    "chunk_text": "It \nuses a bidirectional LSTM to generate word embeddings. \n○ Key Feature: Provides different vectors for a word depending on its usage \nin a sentence. \n● BERT (Bidirectional Encoder Representations from Transformers): \n○ Concept: Developed by Google, BERT is a transformer-based model that \ngenerates context-aware embeddings by considering the context of words \nfrom both left and right sides simultaneously. It's pre-trained on large text \ncorpora using tasks like Masked Language Modeling (predicting masked \nwords) and Next Sentence Prediction. \n○ Key Feature: Bidirectional context understanding, leading to highly \nnuanced word representations. It has revolutionized many NLP tasks."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 76,
    "source": "deep learning.pdf",
    "chunk_text": "● GPT (Generative Pre-trained Transformer) series (GPT-2, GPT-3, GPT-4, \netc.): \n○ Concept: While primarily known for text generation, GPT models also \nproduce powerful contextualized embeddings. They are transformer-based \nand pre-trained to predict the next word in a sequence. Although primarily \nunidirectional in their original form, their vast pre-training on diverse data \nenables them to learn rich representations. \n○ Key Feature: Excellent for language generation and various downstream \nNLP tasks through fine-tuning."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 77,
    "source": "deep learning.pdf",
    "chunk_text": "77 \n \nWord Method \nExample Vectors (Illustrative) \nOne-hot \nhappy = [1,0,0]; excited = [0,1,0]; angry=[0,0,1] \nBag of Words \nSentence: \"happy happy angry\" → [2,0,1] \nTF-IDF \neights words based on rarity \nWord2Vec (CBOW/Skip-Gram) \nhappy = [0.8,0.6]; excited=[0.75,0.7]; angry=[-0.6,-\n0.8] \n \n \n \n(OR) \n \n \n(b) \nExplain the Long Short-Term Memory (LSTM) cell, utilizing a diagram and PyTorch \nimplementation to illustrate its workings. \nLSTM (Long Short-Term Memory) is a special type of Recurrent Neural Network (RNN) \ncell designed to remember information over long sequences and avoid the vanishing \ngradient problem in standard RNNs. \n· It does so by maintaining a cell state that runs through the sequence with minimal \nchanges, controlled by gates that decide what to keep, update, or forget."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 77,
    "source": "deep learning.pdf",
    "chunk_text": "An LSTM cell has 3 main gates: \n· Forget Gate (fₜ): Decides what information to discard from the cell state. \n· Input Gate (iₜ): Decides what new information to add to the cell state. \n· Output Gate (oₜ): Decides what to output based on the updated cell state. \nAnd an internal candidate cell state (𝒞̃ₜ) that suggests new info to add. \n \nAn LSTM's core strength lies in its \"gates,\" which regulate the flow of information into \nand out of the cell state, allowing it to remember or forget information over long \nsequences. \n1. Forget Gate Layer (First Step): \nCO3"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 78,
    "source": "deep learning.pdf",
    "chunk_text": "78 \n \n \n \no Purpose: To decide what information from the previous cell state (Ct−1) should be \ndiscarded. \no Mechanism: A sigmoid layer looks at the previous hidden state (ht−1) and the current \ninput (xt). \no Output: Generates a number between 0 and 1 for each number in Ct−1. \n§ '1' means \"completely keep this.\" \n§ '0' means \"completely get rid of this.\" \no Example (Language Model): If the cell state contains the gender of the previous \nsubject, and a new subject is encountered, the forget gate will output '0' for the old subject's \ngender, effectively forgetting it. \n2. Input Gate Layer & Tanh Layer (Second Step): \n \no Purpose: To decide what new information will be stored in the current cell state (Ct)."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 78,
    "source": "deep learning.pdf",
    "chunk_text": "This step has two parts: \no Input Gate Layer: \n§ Mechanism: A sigmoid layer examines ht−1 and xt. \n§ Output: Decides which new values will be updated (represented by it). \no Tanh Layer (Candidate Values):"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 79,
    "source": "deep learning.pdf",
    "chunk_text": "79 \n \n§ Mechanism: A tanh layer also looks at ht−1 and xt. \n§ Output: Creates a vector of new candidate values (C~t) that could potentially be added \nto the state. These values are scaled between -1 and 1. \no Combined Goal: These two components work together to determine what new \ninformation is relevant to incorporate. \no Example (Language Model): This is where the LSTM identifies and prepares to add \nthe gender of the new subject to the cell state. \n2. Updating the Cell State (Third Step): \n \no Purpose: To transform the old cell state (Ct−1) into the new cell state (Ct) by \nincorporating the decisions made by the forget and input gates. \no Mechanism: \n§ The old cell state (Ct−1) is multiplied by the forget gate's output (ft), effectively \ndiscarding the unwanted information."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 79,
    "source": "deep learning.pdf",
    "chunk_text": "§ The new candidate values (C~t) are scaled by the input gate's output (it) (meaning only \nthe selected new information is considered). \n§ These two results are then added together to form the new cell state (Ct=ft∗Ct−1+it∗C~t\n). \no Example (Language Model): This is the moment where the old subject's gender \ninformation is actually removed, and the new subject's gender information is actively \nincorporated into the cell state. \n2. Output Gate Layer (Fourth Step):"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 80,
    "source": "deep learning.pdf",
    "chunk_text": "80 \n \n \no Purpose: To decide what information from the current cell state (Ct) will be output as \nthe hidden state (ht). \no Mechanism: \n§ An output sigmoid layer looks at ht−1 and xt to determine which parts of the cell state \nare relevant for output. \n§ The new cell state (Ct) is passed through a tanh function (to push values between -1 and \n1). \n§ The tanh output of Ct is then multiplied element-wise by the output of the sigmoid \noutput gate. This filters the cell state, only outputting the relevant parts. \no Example (Language Model): After processing a subject, the LSTM might output \ninformation relevant to a verb (e.g., whether the subject is singular or plural), which would \nguide the conjugation of the next verb in the sentence."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 80,
    "source": "deep learning.pdf",
    "chunk_text": "import torch \nimport torch.nn as nn \n \nclass LSTMCell(nn.Module): \n def __init__(self, input_size, hidden_size): \n super(LSTMCell, self).__init__() \n self.input_size = input_size \n self.hidden_size = hidden_size \n \n # Linear layers for input, forget, cell, and output gates \n self.i2h = nn.Linear(input_size, 4 * hidden_size) \n self.h2h = nn.Linear(hidden_size, 4 * hidden_size) \n \n def forward(self, x, hidden): \n h_prev, c_prev = hidden # hidden state and cell state"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 81,
    "source": "deep learning.pdf",
    "chunk_text": "81 \n \n # Concatenate x and h_prev then apply linear transformations \n gates = self.i2h(x) + self.h2h(h_prev) \n i_gate, f_gate, c_gate, o_gate = gates.chunk(4, dim=1) \n \n i = torch.sigmoid(i_gate) \n f = torch.sigmoid(f_gate) \n o = torch.sigmoid(o_gate) \n g = torch.tanh(c_gate) \n \n c_next = f * c_prev + i * g \n h_next = o * torch.tanh(c_next) \n \n return h_next, c_next \n \n \n \n \n \n \n14. \n(a) \nExplain the fundamental architectural components of Autoencoders in neural networks, \noutlining their roles and functions, and demonstrate these concepts through a PyTorch \nimplementation. \n \nCO4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 82,
    "source": "deep learning.pdf",
    "chunk_text": "82 \n \n \n \n \n \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \n \n# 1. Define the Autoencoder Architecture \nclass Autoencoder(nn.Module): \n def __init__(self, input_dim, hidden_dim): \n super(Autoencoder, self).__init__()"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 83,
    "source": "deep learning.pdf",
    "chunk_text": "83 \n \n \n # Encoder \n self.encoder = nn.Sequential( \n nn.Linear(input_dim, hidden_dim), # Input layer to hidden layer \n nn.ReLU(True), # Non-linear activation \n # You can add more layers here for a deeper encoder \n ) \n \n # Decoder \n self.decoder = nn.Sequential( \n nn.Linear(hidden_dim, input_dim), # Hidden layer to output layer \n nn.Sigmoid() # Sigmoid for output (pixel values between 0 and 1) \n ) \n \n def forward(self, x): \n encoded = self.encoder(x) \n decoded = self.decoder(encoded) \n return decoded \n \n# 2."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 83,
    "source": "deep learning.pdf",
    "chunk_text": "Hyperparameters and Data Loading \nINPUT_DIM = 28 * 28 # For MNIST images (28x28 pixels) \nHIDDEN_DIM = 128 # Dimensionality of the latent space (bottleneck) \nBATCH_SIZE = 64 \nNUM_EPOCHS = 20 \nLEARNING_RATE = 1e-3 \n \n# MNIST Dataset loading \ntransform = transforms.Compose([ \n transforms.ToTensor(), # Convert PIL Image to PyTorch Tensor \n transforms.Normalize((0.5,), (0.5,)) # Normalize pixel values to [-1, 1] \n]) \n \n# Download and load training data \ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, \ntransform=transform) \ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n \n# 3."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 83,
    "source": "deep learning.pdf",
    "chunk_text": "Model, Loss, and Optimizer Initialization \nmodel = Autoencoder(INPUT_DIM, HIDDEN_DIM) \ncriterion = nn.MSELoss() # Mean Squared Error for reconstruction loss \noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) \n \n# Move model to GPU if available \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \nmodel.to(device) \n \nprint(f\"Using device: {device}\") \nprint(model) # Print model architecture \n \n# 4. Training Loop \nprint(\"Starting training...\") \nfor epoch in range(NUM_EPOCHS): \n total_loss = 0 \n for batch_idx, (data, _) in enumerate(train_loader):"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 84,
    "source": "deep learning.pdf",
    "chunk_text": "84 \n \n # Flatten the images from (batch_size, 1, 28, 28) to (batch_size, 784) \n data = data.view(-1, INPUT_DIM).to(device) \n \n # Zero the gradients \n optimizer.zero_grad() \n \n # Forward pass \n reconstructed_data = model(data) \n \n # Calculate loss \n loss = criterion(reconstructed_data, data) \n \n # Backward pass and optimize \n loss.backward() \n optimizer.step() \n \n total_loss += loss.item() \n \n avg_loss = total_loss / len(train_loader) \n print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\") \n \nprint(\"Training finished.\") \n \n# 5."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 84,
    "source": "deep learning.pdf",
    "chunk_text": "Visualization of Reconstruction (Optional)  model.eval() # Set model to evaluation mode  with torch.no_grad():   # Get a batch of test images   test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)   test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)     data_iter = iter(test_loader)   images, _ = next(data_iter)     # Flatten images and move to device   original_images = images.view(-1, INPUT_DIM).to(device)     # Reconstruct images   reconstructed_images = model(original_images).cpu() # Move back to CPU for  plotting   original_images = original_images.cpu()     # Unnormalize images for plotting (assuming original normalization was (0.5, 0.5))   # reconstructed_images = reconstructed_images * 0.5 + 0.5   # original_images ="
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 84,
    "source": "deep learning.pdf",
    "chunk_text": "original_images * 0.5 + 0.5     # Plot original and reconstructed images   fig, axes = plt.subplots(2, 10, figsize=(20, 4))   for i in range(10):   # Original Images   axes[0, i].imshow(original_images[i].reshape(28, 28), cmap='gray')   axes[0, i].axis('off')   # Reconstructed Images   axes[1, i].imshow(reconstructed_images[i].reshape(28, 28), cmap='gray')"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 85,
    "source": "deep learning.pdf",
    "chunk_text": "85 \n \n axes[1, i].axis('off') \n \naxes[0, 0].set_title(\"Original\") \n axes[1, 0].set_title(\"Reconstructed\") \n plt.show() \n \n \n \n \n(OR) \n \n \n(b) \nDevelop a PyTorch-based implementation to train a Generative Adversarial Network (GAN) \non the MNIST dataset. Include an appropriate training loop, loss functions, and optimizer \nsetup."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 85,
    "source": "deep learning.pdf",
    "chunk_text": "# PyTorch GAN on MNIST - Full Implementation \n \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \n \n# Generator Network \nclass Generator(nn.Module): \n def __init__(self, latent_dim): \n super(Generator, self).__init__() \n self.model = nn.Sequential( \n nn.Linear(latent_dim, 128), \n nn.ReLU(True), \n nn.Linear(128, 256), \n nn.BatchNorm1d(256), \n nn.ReLU(True), \n nn.Linear(256, 512), \n nn.BatchNorm1d(512), \n nn.ReLU(True), \n nn.Linear(512, 784), \n nn.Tanh() \nCO4"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 86,
    "source": "deep learning.pdf",
    "chunk_text": "86     )   def forward(self, z):   img = self.model(z)   return img.view(z.size(0), 1, 28, 28)    # Discriminator Network  class Discriminator(nn.Module):   def __init__(self):   super(Discriminator, self).__init__()   self.model = nn.Sequential(   nn.Linear(784, 512),   nn.LeakyReLU(0.2, inplace=True),   nn.Linear(512, 256),   nn.LeakyReLU(0.2, inplace=True),   nn.Linear(256, 1),   nn.Sigmoid()   )   def forward(self, img):   img_flat = img.view(img.size(0), -1)   return self.model(img_flat)    # Hyperparameters and DataLoader  latent_dim = 100  batch_size = 128  lr = 0.0002  epochs = 50  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    transform = transforms.Compose([   transforms.ToTensor(),   transforms.Normalize([0.5], [0.5])  ])  mnist ="
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 86,
    "source": "deep learning.pdf",
    "chunk_text": "datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 87,
    "source": "deep learning.pdf",
    "chunk_text": "87    dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True)    # Initialize networks and optimizers  generator = Generator(latent_dim).to(device)  discriminator = Discriminator().to(device)  optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))  optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))  adversarial_loss = nn.BCELoss()    # Training Loop  for epoch in range(epochs):   for i, (imgs, _) in enumerate(dataloader):   real_imgs = imgs.to(device)   valid = torch.ones(imgs.size(0), 1, device=device)   fake = torch.zeros(imgs.size(0), 1, device=device)     # Train Generator   optimizer_G.zero_grad()   z = torch.randn(imgs.size(0), latent_dim, device=device)   gen_imgs = generator(z)   g_loss ="
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 87,
    "source": "deep learning.pdf",
    "chunk_text": "adversarial_loss(discriminator(gen_imgs), valid)   g_loss.backward()   optimizer_G.step()     # Train Discriminator   optimizer_D.zero_grad()   real_loss = adversarial_loss(discriminator(real_imgs), valid)   fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)   d_loss = (real_loss + fake_loss) / 2   d_loss.backward()   optimizer_D.step()     if i % 100 == 0:"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 88,
    "source": "deep learning.pdf",
    "chunk_text": "88 \n \n print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(dataloader)} \\ \n Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\") \n \n# Visualize generated images \ndef generate_images(generator, n=25): \n z = torch.randn(n, latent_dim).to(device) \n gen_imgs = generator(z).cpu().detach() \n gen_imgs = gen_imgs.view(n, 1, 28, 28) \n fig, axes = plt.subplots(5, 5, figsize=(5, 5)) \n for i, ax in enumerate(axes.flatten()): \n ax.imshow(gen_imgs[i].squeeze(), cmap=\"gray\") \n ax.axis(\"off\") \n plt.tight_layout() \n plt.show() \n \ngenerate_images(generator) \n \n \n \n \n \n15. \n(a) \nDescribe object classification and localization. How do modern object detection models \nperform both tasks simultaneously? Support your answer with suitable architectural \nexamples. \n \nCO5"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 89,
    "source": "deep learning.pdf",
    "chunk_text": "89 \n \n \n \n \nobject detection models can be categorized into two types: \n1. Two-Stage Detectors: These models typically perform the object detection \nprocess in two distinct steps: \n○ Region Proposal: In the first stage, the model generates a set of \"region \nproposals\" or \"regions of interest (RoIs)\" that are likely to contain objects. \nThis is essentially the localization part of the initial guess. \n○ Classification and Bounding Box Regression: In the second stage, these \nproposed regions are then fed into a classification network to determine the \nobject's class and a regression network to refine the bounding box \ncoordinates, making them more precise. \n2. One-Stage Detectors: These models perform both tasks (classification and \nlocalization) in a single forward pass through the network."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 89,
    "source": "deep learning.pdf",
    "chunk_text": "They directly predict \nbounding boxes and class probabilities for various locations across the image. \nThis approach generally leads to faster inference times, making them suitable for \nreal-time applications. \nHere's how they achieve simultaneous operation: \n· Shared Feature Extraction Backbone: Both two-stage and one-stage detectors \ntypically start with a convolutional neural network (CNN) backbone (e.g., \nResNet, VGG, Darknet) that extracts rich, hierarchical features from the input \nimage. These features are then shared by both the classification and localization \nbranches. The early layers capture low-level features (edges, textures), while \ndeeper layers capture high-level semantic features."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 89,
    "source": "deep learning.pdf",
    "chunk_text": "· Anchor Boxes / Priors: Many models utilize \"anchor boxes\" (also known as prior \nboxes or default boxes). These are predefined bounding box shapes and sizes at \nvarious locations across the image. The model then predicts offsets from these \nanchor boxes (for localization) and class probabilities for each anchor box (for \nclassification).This allows the model to predict multiple objects at different scales"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 90,
    "source": "deep learning.pdf",
    "chunk_text": "90 \n \nand aspect ratios simultaneously. \n· Grid-based Prediction (in One-Stage Detectors): Models like YOLO divide the \ninput image into a grid. Each grid cell is responsible for predicting objects whose \ncenter falls within that cell. For each cell, it predicts a fixed number of bounding \nboxes, their confidence scores (objectness: whether an object is present in that \nbox), and class probabilities. This directly links spatial information (grid cell) to \nthe object's presence and class. \n· Multi-task Loss Function: The training of these models involves a composite \nloss function that simultaneously optimizes for both classification accuracy and \nbounding box regression accuracy."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 90,
    "source": "deep learning.pdf",
    "chunk_text": "○ Classification Loss: Typically cross-entropy loss, measures how well the \nmodel predicts the correct class label for each object. \n○ Localization (Regression) Loss: Often L1 or L2 loss (or more advanced \nforms like IoU loss), measures the difference between the predicted bounding \nbox coordinates and the ground truth bounding box coordinates. \n· Non-Maximum Suppression (NMS): After the model generates numerous \nbounding box predictions (many overlapping), NMS is applied as a postprocessing step. It removes redundant and less confident bounding boxes, \nkeeping only the most confident and representative ones for each detected object. \nSuitable Architectural Examples: \n1."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 90,
    "source": "deep learning.pdf",
    "chunk_text": "YOLO (You Only Look Once) - A prominent One-Stage Detector \n \n● Architecture: YOLO models (YOLOv1, YOLOv2, YOLOv3, YOLOv4, YOLOv5, \nYOLOv7, YOLOv8, etc.) are known for their speed. They treat object detection as \na regression problem. \n○ The input image is divided into an S×S grid. \n○ Each grid cell predicts B bounding boxes, a confidence score for each box \n(indicating the probability of an object being present in that box), and C \nconditional class probabilities (probability of an object being a specific \nclass, given that an object is present). \n○ The final output is a tensor of size S×S×(B×5+C). The 5 for each bounding \nbox corresponds to (x,y,w,h) coordinates and objectness score."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 90,
    "source": "deep learning.pdf",
    "chunk_text": "● Simultaneous Operation: YOLO directly predicts both the bounding box \ncoordinates and class probabilities for each grid cell in a single forward pass."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 91,
    "source": "deep learning.pdf",
    "chunk_text": "91 \n \nThere's no separate region proposal stage. The network learns to directly map \nimage pixels to bounding box parameters and class scores. \n2. Faster R-CNN - A classic Two-Stage Detector \n \n● Architecture: Faster R-CNN significantly improved upon its predecessors (R-CNN, \nFast R-CNN) by introducing a Region Proposal Network (RPN). \n○ Backbone CNN: Extracts a feature map from the input image. \n○ Region Proposal Network (RPN): This is a small convolutional network that \nslides over the feature map generated by the backbone. At each slidingwindow location, it predicts two things: \n■ Objectness Score: Whether a region contains an object or not (binary \nclassification)."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 91,
    "source": "deep learning.pdf",
    "chunk_text": "■ Bounding Box Refinements: Adjustments to a set of predefined \n\"anchor boxes\" at that location to better fit potential objects. \n○ RoI Pooling / RoI Align: The proposed regions (RoIs) from the RPN are \nthen pooled (or aligned, for more precise results) to a fixed size feature \nvector. \n○ Classification and Regression Head: These fixed-size feature vectors are fed \ninto a fully connected layer (or small CNNs) with two output branches: \n■ Classification Head: Predicts the actual class probabilities for each \nRoI. \n■ Bounding Box Regression Head: Further refines the bounding box \ncoordinates for each classified object."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 91,
    "source": "deep learning.pdf",
    "chunk_text": "● Simultaneous Operation: Faster R-CNN processes the image in two stages, but the \nRPN and the detection network share the same convolutional features, making it an \nend-to-end trainable system. The RPN provides the initial \"localization\" proposals, \nand the subsequent stage refines these localizations and performs the final \n\"classification.\" \n3. SSD (Single Shot MultiBox Detector) - Another One-Stage Detector"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 92,
    "source": "deep learning.pdf",
    "chunk_text": "92 \n \n \n● Architecture: SSD is also a one-stage detector that aims for speed and accuracy. \n○ It uses a VGG or ResNet-like backbone for feature extraction. \n○ It predicts bounding boxes and class probabilities at multiple scales (from \ndifferent convolutional layers) of the feature map. This is achieved by using \na set of default (anchor) boxes with varying scales and aspect ratios at each \nfeature map location. \n● Simultaneous Operation: Similar to YOLO, SSD performs classification and \nlocalization simultaneously by directly predicting offsets to default boxes and their \ncorresponding class scores from feature maps at various resolutions. This multiscale prediction helps in detecting objects of different sizes effectively."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 92,
    "source": "deep learning.pdf",
    "chunk_text": "(OR) \n \n \n(b) \nExplain how Region Proposal Networks (RPN) are integrated into Faster R-CNN, and \nevaluate its performance in terms of speed and accuracy compared to previous models. \nA Region Proposal Network (RPN) is a specialized neural network that predicts object \nregions or \"proposals\" in an image.Its integration into the Faster R-CNN (Region-based \nConvolutional Neural Network) architecture marked a pivotal moment in object detection, \ncreating a unified, end-to-end deep learning model that significantly outpaced its \npredecessors in both speed and accuracy. \nThe core innovation of the RPN is that it shares the powerful, deep convolutional features of \nthe main object detection network, making the region proposal step nearly instantaneous."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 92,
    "source": "deep learning.pdf",
    "chunk_text": "This resolved the major computational bottleneck present in earlier models like R-CNN and \nFast R-CNN, which relied on slow, external algorithms like Selective Search to generate \npotential object locations. \nIntegration into Faster R-CNN Architecture \nCO5"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 93,
    "source": "deep learning.pdf",
    "chunk_text": "93 \n \n \nThe Faster R-CNN model is composed of two main modules that share a common set of \nconvolutional layers (the \"backbone,\" e.g., VGG or ResNet): \n1. Region Proposal Network (RPN): This network's sole job is to identify a set of highquality rectangular region proposals that are likely to contain objects. \n2. Fast R-CNN Detector: This network takes the proposed regions from the RPN and \nperforms final classification (e.g., \"person,\" \"car\") and refines the bounding box \ncoordinates. \nThe integration works as follows: \n1. Shared Feature Map: An input image is first processed by the backbone CNN, producing \na single, high-level feature map. This feature map is the crucial link, as it's used by both the \nRPN and the final detector. \n2."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 93,
    "source": "deep learning.pdf",
    "chunk_text": "Anchor Boxes: The RPN slides a small network over this feature map. At each location, \nit considers multiple predefined bounding boxes of different scales and aspect ratios, known \nas anchor boxes. \n3. Dual Outputs: For each anchor box, the RPN outputs two predictions: \n○ An objectness score: The probability that an anchor contains any object \nversus being background. \n○ Bounding box regression: Adjustments to the anchor's coordinates to make it \nfit a potential object more tightly. \n4. Proposal Hand-off: The highest-scoring region proposals from the RPN are then passed \nto the second stage. An RoI (Region of Interest) Pooling layer extracts a fixed-size feature"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 94,
    "source": "deep learning.pdf",
    "chunk_text": "94 \n \nvector for each proposal from the same shared feature map. \n5. Final Detection: These feature vectors are fed into the Fast R-CNN module, which \nmakes the final class prediction and further refines the bounding box for each object. \nThis unified architecture allows the RPN to be trained jointly with the detector, enabling it \nto learn how to generate proposals that are specifically tailored for the main detection \nnetwork, thereby improving overall accuracy. \n \n· The most significant impact of the RPN was on speed. By replacing the slow CPUbased Selective Search algorithm (which took ~2 seconds per image) with a lightweight \nneural network running on the GPU, the time spent on generating proposals plummeted to \nabout 10 milliseconds."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 94,
    "source": "deep learning.pdf",
    "chunk_text": "This elimination of the primary bottleneck allowed the entire \ndetection pipeline to be nearly real-time, making it over 10 times faster than Fast R-CNN \nand about 250 times faster than the original R-CNN. \n· While speed was the headline improvement, accuracy also saw a notable boost. Unlike \nSelective Search, which is a fixed, hand-engineered algorithm, the RPN is a trainable \nnetwork. By training it jointly with the detection network, the RPN learns to generate \nproposals that are optimized for the detector and the specific classes in the dataset. This \nability to learn high-quality, data-specific proposals helps the detector perform better, \nleading to a higher mean Average Precision (mAP) compared to models that use static \nproposal methods."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 94,
    "source": "deep learning.pdf",
    "chunk_text": "PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \n \nCO"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 95,
    "source": "deep learning.pdf",
    "chunk_text": "95 \n \n16. \n(a) \nImplement transfer learning by using VGG as the base model to classify the CIFAR-10 \ndataset using PyTorch. \ni) Load the CIFAR-10 dataset. (3 Mark) \nii) Use a pre-trained VGG model as the base model (4 Mark) \niii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function. (4 Mark) \niv) Train the model with training data and validation data. (4 Mark) \n \n \n \n \nCO2 \n \n \n(OR)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 96,
    "source": "deep learning.pdf",
    "chunk_text": "96 \n \n \n(b) \nThe MNIST dataset consists of 70000 28x28 grayscale digit images in 10 classes. There \nare 60000 training images and 10000 test images. Develop a PyTorch implementation for \nthe following. \ni) Load the MNIST dataset. (4 Mark) \nii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function. (5 Mark) \niii) Train the model with training data and validate it using the test dataset, and \nevaluate its accuracy."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 96,
    "source": "deep learning.pdf",
    "chunk_text": "(6 Mark) \n#Import Libraries \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nimport torchvision \nimport torchvision.transforms as transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \nimport numpy as np \nfrom sklearn.metrics import confusion_matrix, classification_report \nimport seaborn as sns \n \n# Data Preprocessing \ntransform = transforms.Compose([ \n transforms.ToTensor(), \n transforms.Normalize((0.5,), (0.5,)) # Normalize images \n]) \n \n# Load MNIST dataset \nCO2"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 97,
    "source": "deep learning.pdf",
    "chunk_text": "97    train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True,  transform=transform, download=True)  test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False,  transform=transform, download=True)    # Check dataset  image, label = train_dataset[0]  print(\"Image shape:\", image.shape)  print(\"Number of training samples:\", len(train_dataset))    image, label = test_dataset[0]  print(\"Image shape:\", image.shape)  print(\"Number of testing samples:\", len(test_dataset))    # Create DataLoaders  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)    # Define the CNN Model  class CNNClassifier(nn.Module):   def __init__(self):   super(CNNClassifier, self).__init__()   self.conv1 ="
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 97,
    "source": "deep learning.pdf",
    "chunk_text": "nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3,  padding=1)   self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,  padding=1)   self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,  padding=1)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 98,
    "source": "deep learning.pdf",
    "chunk_text": "98     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)   self.fc1 = nn.Linear(128 * 3 * 3, 128) # Adjusted for MNIST image size   self.fc2 = nn.Linear(128, 64)   self.fc3 = nn.Linear(64, 10) # 10 classes in MNIST     def forward(self, x):   x = self.pool(torch.relu(self.conv1(x)))   x = self.pool(torch.relu(self.conv2(x)))   x = self.pool(torch.relu(self.conv3(x)))   x = x.view(x.size(0), -1) # Flatten the tensor   x = torch.relu(self.fc1(x))   x = torch.relu(self.fc2(x))   x = self.fc3(x)   return x    # Print model summary  from torchsummary import summary  model = CNNClassifier()  if torch.cuda.is_available():   device = torch.device(\"cuda\")   model.to(device)  print('Name: ')  print('Register Number: ')  summary(model, input_size=(1, 28, 28)) # MNIST images are 28x28 with 1  channel"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 99,
    "source": "deep learning.pdf",
    "chunk_text": "99 \n \ncriterion = nn.CrossEntropyLoss() \noptimizer = optim.Adam(model.parameters(), lr=0.001) \n \n# Training Function \ndef train_model(model, train_loader, num_epochs=10): \n for epoch in range(num_epochs): \n model.train() \n running_loss = 0.0 \n for images, labels in train_loader: \n if torch.cuda.is_available(): \n images, labels = images.to(device), labels.to(device) \n \n optimizer.zero_grad() \n outputs = model(images) \n loss = criterion(outputs, labels) \n loss.backward() \n optimizer.step() \n running_loss += loss.item() \n \n print('Name: ') \n print('Register Number: ') \n print(f'Epoch [{epoch+1}/{num_epochs}], Loss: \n{running_loss/len(train_loader):.4f}') \n \n# Train the model \ntrain_model(model, train_loader, num_epochs=10)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 100,
    "source": "deep learning.pdf",
    "chunk_text": "100 \n \n# Testing Function \ndef test_model(model, test_loader): \n model.eval() \n correct = 0 \n total = 0 \n all_preds = [] \n all_labels = [] \n \n with torch.no_grad(): \n for images, labels in test_loader: \n if torch.cuda.is_available(): \n images, labels = images.to(device), labels.to(device) \n \n outputs = model(images) \n _, predicted = torch.max(outputs, 1) \n total += labels.size(0) \n correct += (predicted == labels).sum().item() \n all_preds.extend(predicted.cpu().numpy()) \n all_labels.extend(labels.cpu().numpy()) \n \n accuracy = correct / total \n print('Name: ') \n print('Register Number: ') \n print(f'Test Accuracy: {accuracy:.4f}') \n # Compute confusion matrix \n cm = confusion_matrix(all_labels, all_preds)"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 101,
    "source": "deep learning.pdf",
    "chunk_text": "101     plt.figure(figsize=(8, 6))   print('Name: ')   print('Register Number: ')   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',  xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)   plt.xlabel('Predicted')   plt.ylabel('Actual')   plt.title('Confusion Matrix')   plt.show()   # Print classification report   print('Name: ')   print('Register Number: ')   print(\"Classification Report:\")   print(classification_report(all_labels, all_preds, target_names=[str(i) for i in  range(10)]))  # Test the model  test_model(model, test_loader)    # Function to predict and visualize an image  def predict_image(model, image_index, dataset):   model.eval()   image, label = dataset[image_index]   if torch.cuda.is_available():   image = image.to(device)     with torch.no_grad():"
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 102,
    "source": "deep learning.pdf",
    "chunk_text": "102 \n \n _, predicted = torch.max(output, 1) \n \n class_names = [str(i) for i in range(10)] \n \n print('Name: ') \n print('Register Number: ') \n plt.imshow(image.cpu().squeeze(), cmap=\"gray\") \n plt.title(f'Actual: {class_names[label]}\\nPredicted: \n{class_names[predicted.item()]}') \n plt.axis(\"off\") \n plt.show() \n print(f'Actual: {class_names[label]}, Predicted: \n{class_names[predicted.item()]}') \n \n# Predict and visualize an image \npredict_image(model, image_index=80, dataset=test_dataset) \n \n \n _____________________ \n \n \n \nFor Set 3 QP \n \nPart - A \nQuestion No. \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK3 \nK2 \nK2 \nK2 \nK2 \nK2 \nK2 \nK2 \nK3 \nK3 \nDifficulty Level \n3 \n3 \n2 \n2 \n2 \n2 \n3 \n2 \n3 \n2 \n \n Part - B \nPart - C \nQuestion No."
  },
  {
    "book_id": "e015fb20-36dc-4488-8da5-6442f621fdb5",
    "book_title": "deep learning.pdf",
    "page": 102,
    "source": "deep learning.pdf",
    "chunk_text": "11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK2 \nK6 \nK3 \nK3 \nK2 \nK3 \nK3 \nK6 \nK3 \nK2 \nK6 \nK6 \nDifficulty Level \n3 \n3 \n3 \n3 \n3 \n4 \n3 \n4 \n3 \n3 \n4 \n4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 1,
    "source": "deep learning.pdf",
    "chunk_text": "1 \n \nSET 1 (Answer Key) \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nConsider a single neuron as a linear unit. Train the model with 'sugars' (grams of sugars \nper serving) as input and 'calories' (calories per serving) as output. Let the bias is b=90, and \nthe weight is w=2.5. Estimate the calorie content of cereal with 5 grams of sugar per serving \nwith a network diagram. \nAnswer: \nGiven Values: Input: x=5 grams sugar, Weight: w=2.5, Bias: b=90 \nA linear neuron computes the output as: y = w ⋅ x + b. = =2.5×5+90=102.5 \nPredicted calories = 102.5. \nCO1 \n2."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 1,
    "source": "deep learning.pdf",
    "chunk_text": "How do the complexity of a model and the size of the training dataset contribute to \noverfitting, and what techniques can be employed to mitigate its effects? \nAnswer : \nOverfitting happens when the model is too complex (many parameters) or the training dataset is \ntoo small, leading to memorization instead of generalization. It can be mitigated using \nregularization (L1/L2, dropout), early stopping, data augmentation, cross-validation, simpler \nmodels, or by collecting more data. \nCO1 \n3. \nDetermine the number of parameters if the input image shape is 1024x1024x3 and the \nconvolution layer uses 32 filters of size 7x7 with a stride of 1 and padding of 2."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 1,
    "source": "deep learning.pdf",
    "chunk_text": "Answer: Given Values : Input shape = 1024×1024×3, Filter size = 7×7, Input channels = 3, \nNumber of filters = 32, Stride & padding affect output size, but not parameter count. \nFormula for Parameters in a Convolutional Layer: \ntotal parameters = parameters per filter × number of filters \nparameters per filter = (filter height × filter width × input channels) + 1 (bias) \nNow, Filter weights = 7 × 7 × 3 = 1477 × 7 × 3=147, Add bias = 147 + 1 = 148147 + 1 = 148 per \nfilter, Then For 32 filters: 148 × 32 = 4736148 × 32 = 4736, Total parameters = 4736 \nCO2 \n4. \nWhy are convolutions useful in deep learning? \nAnswer: \nConvolutions are useful in deep learning because they reduce the number of parameters compared \nto fully connected networks, making the model more efficient."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 1,
    "source": "deep learning.pdf",
    "chunk_text": "They can automatically extract \nlocal features such as edges, corners, and textures, which are essential for understanding images. \nConvolutions also provide translation invariance, meaning the same feature can be detected \nanywhere in the image. By stacking multiple convolutional layers, the network can learn \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 2,
    "source": "deep learning.pdf",
    "chunk_text": "2 \n \nhierarchical representations, starting from low-level patterns and progressing to high-level object \nfeatures. \n5. \nDistinguish between the phenomena of exploding gradients and vanishing gradients. \n \nExploding Gradients \n \nVanishing Gradients \n● Gradients become very large during \nbackprop. \n● Gradients become very small (near \nzero). \n● Unstable training, weights diverge. \n● Learning slows or stops, early layers \ndon’t learn. \n● Repeated multiplication → large \nvalues grow. \n● Repeated multiplication → values \nshrink. \n● Very deep networks, RNNs with long \nsequences \n● Very deep networks, RNNs with \nsigmoid/tanh. \n● Gradient clipping, small learning rate. \n● ReLU/variants, batch norm, skip \nconnections. \n \nCO3 \n6."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 2,
    "source": "deep learning.pdf",
    "chunk_text": "How do LSTM and GRU models differ in terms of the number of gates and their \nfunctionalities? \nLSTM (Long Short-Term Memory) \nGRU (Gated Recurrent Unit) \n● 3 gates – Input gate, Forget gate, \nOutput gate. \n● 2 gates – Update gate, Reset gate. \n● Has a separate cell state and hidden \nstate → better for long dependencies. \n● Combines cell state and hidden state \ninto one → simpler. \n● More parameters → computationally \nheavier. \n● Fewer parameters → faster, less \nmemory. \n● Controls what to write, keep, and \noutput explicitly. \n● Controls how much of the past to \nkeep or reset. \n \nCO3 \n7. \nDefine Autoencoder and offer concrete examples of its applications in real-world scenarios?"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 2,
    "source": "deep learning.pdf",
    "chunk_text": "Answer: \nAn Autoencoder is a type of neural network used for unsupervised learning that learns to encode \ninput data into a lower-dimensional representation (latent space) and then reconstruct it back to \nthe original form. It consists of two parts Encoder and decoder. Encoder compresses the input \ninto latent features. Decoder reconstructs the input from latent features.The goal is to learn \nefficient representations by minimizing reconstruction error. \nApplications : Image Denoising, Dimensionality Reduction, Anomaly Detection, Image \nCompression, Recommendation Systems \n \nCO4 \n8. \nWhy do RBMs use Gibbs sampling during training? \nAnswers: \n RBMs use Gibbs sampling because computing exact probabilities over all visible–hidden \nconfigurations is intractable."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 2,
    "source": "deep learning.pdf",
    "chunk_text": "Gibbs sampling alternately samples hidden units given visible units \nand visible units given hidden units, approximating the model’s distribution. This enables \nefficient training using algorithms like Contrastive Divergence. \nCO4 \n9. \nDefine Intersection over union(IoU) in bounding box prediction. \nAnswer: Intersection over Union (IoU) is a metric used to evaluate object detection models. It \nCO5"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 3,
    "source": "deep learning.pdf",
    "chunk_text": "3 \n \nmeasures the overlap between the predicted bounding box and the ground-truth bounding box. \n \n \n10. \nWhat makes the CamVid dataset suitable for urban scene understanding in semantic \nsegmentation? \nAnswer: The CamVid dataset is suitable for urban scene semantic segmentation because it \ncontains video frames of real-world urban driving, with pixel-level annotations for multiple \nobject classes like roads, vehicles, pedestrians, and buildings. Its diverse classes and temporal \ninformation make it ideal for training and evaluating segmentation models in city environments. \nCO5 \n \n \n PART B (5 x 13 = 65 marks) \n \n \n \nCO \n11. \n(a) \nIn a practical deep learning project, how do you determine the most suitable \nactivation function for different layers in a neural network and why?"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 3,
    "source": "deep learning.pdf",
    "chunk_text": "Additionally, \nexplain these activation functions and provide PyTorch implementations? \nActivation function: \nWe want to set boundaries for the overall output value, x*w+b is passed through the activation \nfunction. Let us define the total input as z, where z= x*w+b. Then pass z through some activation \nfunction to limit its value. \n● The most simple networks rely on basic step function that outputs 0 or 1. \n● Regardless of the values it outputs 0 or 1. \n● This is a very strong function since small changes aren’t reflected. \n● There is just an immediate cutoff that splits between 0 and 1. \n \nIt would be nice if we could have a more dynamic function, for example the red line."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 3,
    "source": "deep learning.pdf",
    "chunk_text": "Sigmoid (Logistic) \n● The Sigmoid function (also known as the Logistic function) is one of the most widely \nused activation functions. The function is defined as: \nCO1"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 4,
    "source": "deep learning.pdf",
    "chunk_text": "4 \n \n \n● The function is a common S-shaped curve. \n● The output of the function is centered at 0.5 with a range from 0 to 1. \n● The function is differentiable. That means we can find the slope of the sigmoid curve at \nany two points. \n● The function is monotonic but the function’s derivative is not. \nProblems with Sigmoid activation function \n● Vanishing gradient: looking at the function plot, you can see that when inputs become \nsmall or large, the function saturates at 0 or 1, with a derivative extremely close to 0. \nThus it has almost no gradient to propagate back through the network, so there is almost \nnothing left for lower layers. \n● Computationally expensive: the function has an exponential operation. \n● The output is not zero centered."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 4,
    "source": "deep learning.pdf",
    "chunk_text": "Implementation \n \nHyperbolic Tangent (Tanh) \nAnother very popular and widely used activation function is the Hyperbolic Tangent, also known \nas Tanh. It is defined as: \n \n● The function is a common S-shaped curve as well. \n● The difference is that the output of Tanh is zero centered with a range from -1 to 1 \n(instead of 0 to 1 in the case of the Sigmoid function) \n● The same as the Sigmoid, this function is differentiable \n● The same as the Sigmoid, the function is monotonic, but the function’s derivative is not. \nProblems with Tanh activation function"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 5,
    "source": "deep learning.pdf",
    "chunk_text": "5 \n \n● Vanishing gradient: looking at the function plot, you can see that when inputs become \nsmall or large, the function saturates at -1 or 1, with a derivative extremely close to 0. \nThus it has almost no gradient to propagate back through the network, so there is almost \nnothing left for lower layers. \n● Computationally expensive: the function has an exponential operation. \n Implementation \n \nRectified Linear Unit (ReLU) \n● The Rectified Linear Unit (ReLU) is the most commonly used activation function in deep \nlearning. \n● The function returns 0 if the input is negative, but for any positive input, it returns that \nvalue back. The function is defined as: \n \n● Graphically, the ReLU function is composed of two linear pieces to account for non \nlinearities."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 5,
    "source": "deep learning.pdf",
    "chunk_text": "A function is non-linear if the slope isn’t constant. So, the ReLU function is \nnon-linear around 0, but the slope is always either 0 (for negative inputs) or 1 (for \npositive inputs). \n● The ReLU function is continuous, but it is not differentiable at the origin. \n● The output of ReLU does not have a maximum value (It is not saturated) and this helps \nGradient Descent \n● The function is very fast to compute (Compare to Sigmoid and Tanh) \nProblem with ReLU \n● ReLU works great in most applications, but it is not perfect. It suffers from a problem \nknown as the dying ReLU. \nDying ReLU \nDuring training, some neurons effectively die, meaning they stop outputting anything other than 0."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 5,
    "source": "deep learning.pdf",
    "chunk_text": "In some cases, you may find that half of your network’s neurons are dead, especially if you used a \nlarge learning rate. A neuron dies when its weights get tweaked in such a way that the weighted \nsum of its inputs are negative for all instances in the training set. When this happens, it just keeps"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 6,
    "source": "deep learning.pdf",
    "chunk_text": "6 \n \noutputting 0s, and gradient descent does not affect it anymore since the gradient of the ReLU \nfunction is 0 when its input is negative. \n Implementation \n \nSoftmax Activation Function \n● Softmax function calculates the probability distribution of the event over K different \nevents. \n● This function will calculate the probabilities of each target class over all possible target \nclasses. \n● The range will be 0 to 1, and the sum of all the probabilities will be equal to one. \n● The model returns the probabilities of each class and the target class chosen will have the \nhighest probability. \n \nImplementation \n \n \n \n \n(OR) \n \n \n(b) \n(i) Create the appropriate PyTorch implementation of the following neural network."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 6,
    "source": "deep learning.pdf",
    "chunk_text": "It should include model creation, model instantiation, loss function selection and \noptimizer selection.(6 Marks) \nCO1"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 7,
    "source": "deep learning.pdf",
    "chunk_text": "7 \n \n \n \n(ii) Explain the confusion matrix for binary class and multiclass prediction. (7 Marks) \nClassification Metrics \n● Classification is about predicting the class labels given input data. \n● In binary classification, there are only two possible output classes. \n● A very common example of binary classification is spam detection, where the \ninput data could include the email text and metadata (sender, sending time), and \nthe output label is either “spam” or “not spam.” \nConfusion Matrix \n● Confusion Matrix is a performance measurement for the machine learning \nclassification problems where the output can be two or more classes. \n● It is a table with combinations of predicted and actual values."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 7,
    "source": "deep learning.pdf",
    "chunk_text": "● A confusion matrix is defined as the table that is often used to describe the \nperformance of a classification model on a set of the test data for which the true \nvalues are known."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 8,
    "source": "deep learning.pdf",
    "chunk_text": "8 \n \n \nTrue Positive: We predicted positive and it’s true. \nTrue Negative: We predicted negative and it’s true. \nFalse Positive (Type 1 Error)- We predicted positive and it’s false. \nFalse Negative (Type 2 Error)- We predicted negative and it’s false. \nAccuracy: \nAccuracy simply measures how often the classifier correctly predicts. We can define \naccuracy as the ratio of the number of correct predictions and the total number of \npredictions. \n \nMulticlass Confusion Matrix \n \nImplementation"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 9,
    "source": "deep learning.pdf",
    "chunk_text": "9 \n \n12. \n(a) \nThe CIFAR-10 dataset consists of 60000 32x32 color images in 10 classes, with 6000 \nimages per class. There are 50000 training images and 10000 test images. Develop the \nPyTorch implementation for the following. \ni) Load the CIFAR-10 dataset (3 Marks) \nii) Do the necessary Preprocessing (3 Marks) \niii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function (3 Marks) \niv) Train the model with training data and validation data. (4 Marks) \n \nAnswer: \n \n \n \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 10,
    "source": "deep learning.pdf",
    "chunk_text": "10 \n \n \n \n \n(OR) \n \n \n(b) \nExplain the following with examples \n(i) Padding (4 Marks) \n(ii) Striding (4 Marks) \n(iii) Pooling (5 Marks) \ni) Padding \nThere are two problems arise with convolution: \n1. Every time after convolution operation, original image size getting shrinks \nIn image classification task there are multiple convolution layers so after multiple \nconvolution operation, our original image will really get small but we don’t want \nthe image to shrink every time. \n2. When kernel moves over original images, it touches the edge of the image less \nnumber of times and touches the middle of the image more number of times and \nit overlaps also in the middle. So, the corner features of any image or on the \nedges aren’t used much in the output."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 10,
    "source": "deep learning.pdf",
    "chunk_text": "In order to solve these two issues, a new concept is introduced called padding. Padding preserves \nthe size of the original image. \n \nPadded image convolved with 2*2 kernel \nSo if a 𝑛∗𝑛 matrix convolved with an f*f matrix the with padding p then the size of the output \nimage will be (n - f + 2p + 1) * (n - f + 2p + 1) where p =1 in this case. \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 11,
    "source": "deep learning.pdf",
    "chunk_text": "11 \n \n \n(ii) Striding \n \n● Stride is the number of pixels shifts over the input matrix. \n● When the stride is 1 then we move the filters one pixel at a time. \n● When the stride is 2 then the filters jump 2 pixels at a time as we slide them around. \n● This will produce smaller output volumes spatially. \n \nleft image: stride =0, middle image: stride = 1, right image: stride =2 \nOutput Dimension \n● We can compute the spatial size of the output volume as a function of the input \nvolume size (W), the receptive field size of the Conv Layer neurons (F), the stride \nwith which they are applied (S), and the amount of zero padding used (P) on the \nborder. \n● Formula for calculating how many neurons “fit” is given by ((W−F+2P)/S)+1."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 11,
    "source": "deep learning.pdf",
    "chunk_text": "● For example for a 7x7 input and a 3x3 filter with stride 1 and pad 0 we would get \na 5x5 output. With stride 2 we would get a 3x3 output. \n(iii) Pooling \n● A pooling layer is another building block of a CNN. \n● Pooling function is to progressively reduce the spatial size of the representation to reduce \nthe network complexity and computational cost. \nThere are two types of widely used pooling in CNN layer: \n● Max Pooling \n● Average Pooling \nMax Pooling \n● Max pooling is simply a rule to take the maximum of a region and it helps to proceed \nwith the most important features from the image. \n● Max pooling selects the brighter pixels from the image. \n● It is useful when the background of the image is dark and we are interested in only the \nlighter pixels of the image."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 12,
    "source": "deep learning.pdf",
    "chunk_text": "12 \n \n \nAverage Pooling \n● Average Pooling is different from Max Pooling in the sense that it retains much \ninformation about the “less important” elements of a block, or pool. \n● Whereas Max Pooling simply throws them away by picking the maximum value, \n● Average Pooling blends them in. This can be useful in a variety of situations, where such \ninformation is useful. \n \nPooling Dimension \n● The pooling layer downsamples the volume spatially, independently in each depth slice \nof the input volume. \n● In this example, the input volume of size [224x224x64] is pooled with filter size 2, stride \n2 into output volume of size [112x112x64]. Notice that the volume depth is preserved. \n \n \n \n \n \n13."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 12,
    "source": "deep learning.pdf",
    "chunk_text": "(a) \nUsing GloVe word embeddings, implement the following tasks: \n(i) Develop a function that retrieves the embedding vector of a given word from pretrained GloVe embeddings. (5 Marks) \n(ii) Implement a function that computes and returns the most similar words to a given \nword by comparing their embedding vectors using cosine similarity. (8 Marks) \n \nimport numpy as np \n \ndef load_glove_embeddings(glove_file_path): \nCO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 13,
    "source": "deep learning.pdf",
    "chunk_text": "13 \n \n embeddings = {} \n with open(glove_file_path, 'r', encoding='utf8') as f: \n for line in f: \n parts = line.strip().split() \n word = parts[0] \n vector = np.array(parts[1:], dtype=np.float32) \n embeddings[word] = vector \n return embeddings \n \ndef get_word_embedding(word, glove_embeddings): \n \"\"\" \n Retrieves the embedding vector for a given word. \n \n Args: \n word (str): The word to retrieve. \n glove_embeddings (dict): A dictionary mapping words to vectors. \n \n Returns: \n np.ndarray or None: Embedding vector if the word exists, otherwise None."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 13,
    "source": "deep learning.pdf",
    "chunk_text": "\"\"\" \n return glove_embeddings.get(word) \n \n \nfrom sklearn.metrics.pairwise import cosine_similarity \n \ndef find_most_similar_words(word, glove_embeddings, top_n=5): \n \"\"\" \n Finds the most similar words to the input word using cosine similarity. \n \n Args: \n word (str): The word to find similarities for. \n glove_embeddings (dict): Preloaded GloVe embeddings. \n top_n (int): Number of similar words to return. \n \n Returns: \n List[Tuple[str, float]]: List of (word, similarity_score) tuples. \n \"\"\" \n if word not in glove_embeddings: \n return [] \n \n word_vec = glove_embeddings[word].reshape(1, -1) \n similarities = {} \n \n for other_word, vec in glove_embeddings.items(): \n if other_word == word: \n continue \n sim = cosine_similarity(word_vec, vec.reshape(1, -1))[0][0] \n similarities[other_word] = sim"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 14,
    "source": "deep learning.pdf",
    "chunk_text": "14 \n \n # Sort by similarity and return the top N \n sorted_similar = sorted(similarities.items(), key=lambda x: x[1], reverse=True) \n return sorted_similar[:top_n] \n \n \n \n(OR) \n \n \n(b) \nDescribe the forward and backward pass mechanisms in bidirectional RNNs. Why are they \nparticularly useful in NLP tasks like machine translation? \nAnswer: \nA Bidirectional RNN consists of two RNNs running in opposite directions: \n· One processes the sequence forward (left to right). \n· The other processes it backward (right to left). \n· Their outputs are concatenated at each timestep to form the final representation. \n \nA Bidirectional RNN consists of two RNNs running in opposite directions: \n· One processes the sequence forward (left to right). \n· The other processes it backward (right to left)."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 14,
    "source": "deep learning.pdf",
    "chunk_text": "· Their outputs are concatenated at each time step to form the final representation. \n \nCO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 16,
    "source": "deep learning.pdf",
    "chunk_text": "16 \n \n \n \n \n \n14. \n(a) \ni) Enumerate and explain the various types of Generative Adversarial Network (GAN) \narchitectures. (7 Marks) \nGenerative Adversarial Networks (GANs) operate on an adversarial principle, pitting a \nGenerator (G) against a Discriminator (D) to learn to produce realistic data. Over time, \nnumerous architectures have emerged to improve training stability, output quality, and \ncontrol over generation. Here are some of the most significant types: \n1. Vanilla GAN (Original GAN): \n○ Architecture: The foundational model, typically using Multi-Layer \nPerceptrons (MLPs) for both G and D.The Generator maps a random noise \nvector to a data sample, and the Discriminator outputs a probability of the \ninput being real."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 16,
    "source": "deep learning.pdf",
    "chunk_text": "○ Working Principle: G aims to produce samples that fool D into \nclassifying them as real. D's goal is to accurately distinguish real data from \nG's fakes. This creates a minimax game. \n○ Contribution: Introduced the core adversarial training concept, laying the \ngroundwork for all subsequent GAN research. \n○ Limitations: Suffers from training instability, often leading to mode \ncollapse (G produces a limited variety of outputs) and difficulty generating \nhigh-resolution, diverse samples. \n2. Deep Convolutional GAN (DCGAN): \n○ Architecture: Replaced the MLPs of vanilla GANs with Convolutional \nNeural Networks (CNNs) in both G and D. The Generator uses \n\"transposed convolutions\" (deconvolutions) for upsampling, and the \nDiscriminator uses strided convolutions for downsampling."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 16,
    "source": "deep learning.pdf",
    "chunk_text": "Batch \nNormalization is extensively used, and specific activation functions (e.g., \nReLU in G, Leaky ReLU in D) are common. \n○ Working Principle: Applies the power of CNNs to learn hierarchical \nfeatures in images, improving the quality and stability of image generation \ncompared to vanilla GANs. \n○ Contribution: Established architectural guidelines and best practices for \nbuilding stable and effective GANs for visual data. \n○ Improvement Over: Significantly enhanced the stability and visual \nquality of generated images from vanilla GANs. \n3. Conditional GAN (cGAN): \n○ Architecture: Extends any base GAN (like Vanilla or DCGAN) by \nincorporating auxiliary \"conditional\" information (e.g., class labels, text \ndescriptions, or another image) into both the Generator and the \nDiscriminator."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 16,
    "source": "deep learning.pdf",
    "chunk_text": "This conditioning is typically concatenated with the input \nnoise vector for G and with the data input for D. \n○ Working Principle: G learns to generate specific types of data \nconditioned on the provided input, while D learns to discriminate the \nauthenticity of the data given that condition. \n○ Contribution: Enables targeted and controlled data generation, allowing \nusers to specify characteristics of the desired output. \n○ Example Use: Generating an image of a \"cat\" when given the label \"cat,\" \nor transforming a sketch into a photo. \n4. Wasserstein GAN (WGAN) and WGAN-GP (Gradient Penalty): \nCO4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "17 \n \n○ Architecture: Primarily modifies the loss function and the \nDiscriminator's role. It replaces the original GAN's Jensen-Shannon \ndivergence-based loss with the Wasserstein distance (Earth Mover's \nDistance).The Discriminator is re-termed a \"Critic\" and outputs a raw \nscore (no sigmoid activation). WGAN-GP further refines this by adding a \n\"gradient penalty\" term to the Critic's loss, enforcing the Lipschitz \nconstraint more robustly than WGAN's weight clipping. \n○ Working Principle: The Wasserstein distance provides a more stable and \nmeaningful gradient, even when the real and fake data distributions are \nnon-overlapping, which was a major problem for earlier GANs."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "○ Contribution: Drastically improved training stability and reduced mode \ncollapse, providing a reliable metric for generator convergence. \n○ Improvement Over: Addressed the core training instability and mode \ncollapse issues prevalent in Vanilla and DCGANs. \n5. CycleGAN: \n○ Architecture: Composed of two pairs of Generators and Discriminators, \ndesigned for unpaired image-to-image translation between two domains \n(e.g., Domain A and Domain B). It features a critical \"cycle consistency \nloss\" which ensures that translating an image from A to B and then back to \nA yields the original image A. \n○ Working Principle: The cycle consistency loss acts as a powerful \nregularization, allowing the model to learn mappings between domains \nwithout requiring perfectly aligned (paired) training data."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "○ Contribution: Revolutionized image-to-image translation by removing \nthe stringent requirement for paired datasets, opening up new applications. \n○ Example Use: Transforming horses into zebras, changing summer scenes \nto winter scenes, or converting photos into specific artistic styles. \nii) Describe in detail the practical applications of GANs across different domains (6 \nMarks) \nGANs have transcended theoretical research to become powerful tools across a wide \narray of practical domains, fundamentally changing how we approach data generation and \nmanipulation. \n1. Image and Video Synthesis & Editing: \n○ Photorealistic Image Generation: GANs can create highly realistic images \nof objects, landscapes, and even human faces that are indistinguishable \nfrom real photos."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "This is used in generating synthetic data for AI training, \nvirtual photography for e-commerce, and creating unique visual assets for \nart and advertising.Examples include StyleGAN-generated faces used for \ndiverse online profiles. \n○ Image-to-Image Translation: This is a major application, enabling \nconversion between different image domains. \n■ Style Transfer: Applying artistic styles (e.g., Van Gogh's style) to \nregular photos (e.g., CycleGAN). \n■ Semantic Image Synthesis: Generating photorealistic images from \nsemantic layouts or sketches (e.g., turning a rough drawing of a cat \ninto a realistic cat photo). \n■ Domain Adaptation: Transforming images between different \nconditions, like converting day scenes to night scenes, or summer \nlandscapes to winter ones."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 17,
    "source": "deep learning.pdf",
    "chunk_text": "○ Photo Inpainting/Outpainting: Filling in missing or corrupted parts of an \n2. image with realistic content, or extending an image beyond its original"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 18,
    "source": "deep learning.pdf",
    "chunk_text": "18 \n \nboundaries, widely used in photo restoration and creative content generation. \n3. Super-Resolution: Enhancing the resolution and detail of low-resolution images, \nvaluable in forensics, medical imaging, and improving video quality. \n4. Deepfakes (Ethical Concern): While demonstrating powerful generative \ncapabilities, the ability to create highly realistic fake videos or audio of \nindividuals has severe ethical implications concerning misinformation and \nimpersonation. \n2. Data Augmentation and Synthetic Data Generation: \n○ Medical Imaging: Generating synthetic medical scans (e.g., X-rays, MRIs) \nto augment limited datasets for training diagnostic AI models, especially \nfor rare diseases, thus improving model robustness and ensuring patient \nprivacy."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 18,
    "source": "deep learning.pdf",
    "chunk_text": "○ Autonomous Driving: Creating diverse synthetic road scenes, including \nrare events, adverse weather conditions, and various lighting scenarios, to \ntrain self-driving car algorithms more safely and comprehensively, \nreducing the need for costly and risky real-world data collection. \n○ Fraud Detection: Generating synthetic fraudulent transaction data to train \nmodels, particularly when real fraud cases are scarce but crucial for \neffective detection. \n○ Privacy Preservation: Generating synthetic versions of sensitive datasets \nthat retain statistical properties of the original data but contain no real \nindividual information, useful for sharing data with privacy concerns. \n3."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 18,
    "source": "deep learning.pdf",
    "chunk_text": "Entertainment and Creative Industries: \n○ Video Game Development: Automatically generating realistic textures, \ncharacters, landscapes, and entire game levels, significantly speeding up \ncontent creation pipelines. \n○ Film and Animation: Assisting in character design, generating background \nelements, creating special effects, and even synthesizing realistic crowd \nscenes. \n○ Generative Art: Creating unique and novel artworks, music, and even \narchitectural designs that explore new aesthetic possibilities, blurring the \nlines between human and AI creativity. \n4. Scientific Research and Design: \n○ Drug Discovery: Generating novel molecular structures with desired \nchemical properties, accelerating the search for new drugs and treatments."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 18,
    "source": "deep learning.pdf",
    "chunk_text": "○ Material Science: Designing new materials with specific characteristics by \nexploring the vast space of possible atomic and molecular configurations. \n○ Fashion Design: Generating new clothing designs, patterns, and virtual \ntry-on experiences, providing designers with a tool to rapidly prototype \nideas. \n5. Telecommunications and Networking: \n○ Network Anomaly Detection: Generating synthetic anomalous network \ntraffic to train intrusion detection systems, improving their ability to spot \nnovel threats. \nChannel Modeling: Simulating complex wireless communication channels \nto test and optimize communication protocols. \n \n \n(OR)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 19,
    "source": "deep learning.pdf",
    "chunk_text": "19 \n \n \n(b) \nDescribe the different types of autoencoders and explain their architectures, objectives, \nand typical applications with suitable diagrams. \nDifferent Types of Autoencoders: \n1. Undercomplete Autoencoder (Basic Autoencoder) \n \n· Objective: To learn a low-dimensional representation (Z) of the input data such that \nthe reconstruction error between Xand X^ is minimized. The primary goal is \ndimensionality reduction and feature learning. \n· Typical Applications: \n● Dimensionality Reduction: Similar to Principal Component Analysis (PCA) but \ncan capture non-linear relationships. \n● Feature Learning: Extracting salient features from data. \n● Data Compression: Compressing data by storing its latent representation \n2."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 19,
    "source": "deep learning.pdf",
    "chunk_text": "Sparse Autoencoder ( Over complex ) \n \nObjective: To learn representations where only a small subset of hidden units are active \n(non-zero) for any given input. This encourages the network to learn specialized feature \ndetectors. \n● The loss function includes the reconstruction error plus a sparsity penalty. \nCommon sparsity penalties include: \n○ L1 Regularization: \n \n■ Where aj is the activation of the j-th hidden unit, and λ is the \nCO4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 20,
    "source": "deep learning.pdf",
    "chunk_text": "20 \n \nsparsity regularization parameter. \n○ KL Divergence: \n \n■ Where ρ is the desired average activation (sparsity target), ρ^j is \nthe actual average activation of hidden unit j over a batch, and β is \nthe weight of the sparsity penalty. \nApplications: \n● Feature Learning: Learning more interpretable and distributed features. \n● Denoising: Can indirectly help with denoising as it focuses on salient features. \n● Representation Learning: Useful when a higher-dimensional but sparse \nrepresentation is desired. \n3. Denoising Autoencoder (DAE) \n \nApplications: \n● Denoising: Directly removing noise from images, audio, or other data. \n● Robust Feature Learning: Learning features that are more resilient to real-world \ndata imperfections."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 20,
    "source": "deep learning.pdf",
    "chunk_text": "● Data Imputation: Filling in missing values by treating missing data as noise. \n4. Variational Autoencoder (VAE)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 21,
    "source": "deep learning.pdf",
    "chunk_text": "21 \n \n \nTypical Applications: \n● Generative Modeling: Generating new, realistic data samples (e.g., images, text, \nmusic). \n● Dimensionality Reduction: Learning a meaningful latent representation. \n● Anomaly Detection: Anomalous data points tend to have high reconstruction \nerrors or fall into low-density regions of the learned latent space. \nLatent Space Interpolation: Smoothly transitioning between different generated \nsamples by interpolating in the latent space \n \n \n \n \n15. \n(a) \nExplain bounding box prediction in YOLO algorithm with necessary diagrams. \nAnswer: \n \n \nCO5"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 26,
    "source": "deep learning.pdf",
    "chunk_text": "26 \n \n \n \n \n \n(OR) \n \n \n(b) \n \nDiscuss the various region proposal methods used in object detection. Compare their \nefficiency and suitability for integration with modern object detection frameworks. \n \n \n \n \nCO5"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 29,
    "source": "deep learning.pdf",
    "chunk_text": "29 \n \n \n \n \n \n \n \n PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \n \nCO \n16. \n(a) \nCreate the PyTorch implementation Denoising Autoencoder for the MNIST dataset. \ni) Load the MNIST dataset. ( 3 Mark) \nii) Create a model of Denoising Autoencoder ( 4 Mark) \niii) Compile the model with optimizer and loss function ( 3 Mark) \niv) Fit the model with training data and validation data. ( 5 Mark) \n \nAnswer: \n \n# (i) LOAD THE MNIST DATASET \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader, random_split \n \nCO4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 30,
    "source": "deep learning.pdf",
    "chunk_text": "30    # 📁 Transforms: Normalize and Convert to Tensors  transform = transforms.Compose([   transforms.ToTensor(), # Converts to [0,1]  ])    # Download MNIST  train_dataset = datasets.MNIST(root='./data', train=True, download=True,  transform=transform)  test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)    # Split training into train/val  train_data, val_data = random_split(train_dataset, [50000, 10000])    # Dataloaders  train_loader = DataLoader(train_data, batch_size=128, shuffle=True)  val_loader = DataLoader(val_data, batch_size=128, shuffle=False)  test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # (ii) DENOISING AUTOENCODER MODEL  class DenoisingAutoencoder(nn.Module):   def __init__(self):"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 30,
    "source": "deep learning.pdf",
    "chunk_text": "super(DenoisingAutoencoder, self).__init__()   self.encoder = nn.Sequential(   nn.Linear(28*28, 128),   nn.ReLU(),   nn.Linear(128, 64),   )   self.decoder = nn.Sequential(   nn.Linear(64, 128),   nn.ReLU(),   nn.Linear(128, 28*28),   nn.Sigmoid() # keep output in [0, 1]   )     def forward(self, x):   x = x.view(x.size(0), -1) # Flatten input   encoded = self.encoder(x)   decoded = self.decoder(encoded)   return decoded.view(x.size(0), 1, 28, 28)    # (iii) COMPILE THE MODEL  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  model = DenoisingAutoencoder().to(device)  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=0.001)    # Add Gaussian Noise  def add_noise(inputs, noise_factor=0.5):   noisy = inputs + noise_factor * torch.randn_like(inputs)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 30,
    "source": "deep learning.pdf",
    "chunk_text": "return torch.clip(noisy, 0., 1.)    # (iv) TRAINING FUNCTION"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 31,
    "source": "deep learning.pdf",
    "chunk_text": "31    def train_model(model, train_loader, val_loader, epochs=10):   for epoch in range(epochs):   model.train()   train_loss = 0   for images, _ in train_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)   optimizer.zero_grad()   loss.backward()   optimizer.step()   train_loss += loss.item()     model.eval()   val_loss = 0   with torch.no_grad():   for images, _ in val_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)   val_loss += loss.item()     print(f\"Epoch [{epoch+1}/{epochs}] ➤ Train Loss:  {train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\")    # Train the model  train_model(model,"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 31,
    "source": "deep learning.pdf",
    "chunk_text": "train_loader, val_loader)          # (i) LOAD THE MNIST DATASET  import torch  import torch.nn as nn  import torch.optim as optim  from torchvision import datasets, transforms  from torch.utils.data import DataLoader, random_split    # 📁 Transforms: Normalize and Convert to Tensors  transform = transforms.Compose([   transforms.ToTensor(), # Converts to [0,1]  ])    # Download MNIST  train_dataset = datasets.MNIST(root='./data', train=True, download=True,  transform=transform)  test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 32,
    "source": "deep learning.pdf",
    "chunk_text": "32    # Split training into train/val  train_data, val_data = random_split(train_dataset, [50000, 10000])    # Dataloaders  train_loader = DataLoader(train_data, batch_size=128, shuffle=True)  val_loader = DataLoader(val_data, batch_size=128, shuffle=False)  test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)    # (ii) DENOISING AUTOENCODER MODEL  class DenoisingAutoencoder(nn.Module):   def __init__(self):   super(DenoisingAutoencoder, self).__init__()   self.encoder = nn.Sequential(   nn.Linear(28*28, 128),   nn.ReLU(),   nn.Linear(128, 64),   )   self.decoder = nn.Sequential(   nn.Linear(64, 128),   nn.ReLU(),   nn.Linear(128, 28*28),   nn.Sigmoid() # keep output in [0, 1]   )     def forward(self, x):   x = x.view(x.size(0), -1) # Flatten input   encoded ="
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 32,
    "source": "deep learning.pdf",
    "chunk_text": "self.encoder(x)   decoded = self.decoder(encoded)   return decoded.view(x.size(0), 1, 28, 28)    # (iii) COMPILE THE MODEL  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  model = DenoisingAutoencoder().to(device)  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=0.001)    # Add Gaussian Noise  def add_noise(inputs, noise_factor=0.5):   noisy = inputs + noise_factor * torch.randn_like(inputs)   return torch.clip(noisy, 0., 1.)    # (iv) TRAINING FUNCTION  def train_model(model, train_loader, val_loader, epochs=10):   for epoch in range(epochs):   model.train()   train_loss = 0   for images, _ in train_loader:   images = images.to(device)   noisy_imgs = add_noise(images)   outputs = model(noisy_imgs)   loss = criterion(outputs, images)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 32,
    "source": "deep learning.pdf",
    "chunk_text": "optimizer.zero_grad()   loss.backward()   optimizer.step()"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 33,
    "source": "deep learning.pdf",
    "chunk_text": "33 \n \n train_loss += loss.item() \n \n model.eval() \n val_loss = 0 \n with torch.no_grad(): \n for images, _ in val_loader: \n images = images.to(device) \n noisy_imgs = add_noise(images) \n outputs = model(noisy_imgs) \n loss = criterion(outputs, images) \n val_loss += loss.item() \n \n print(f\"Epoch [{epoch+1}/{epochs}] ➤ Train Loss: \n{train_loss/len(train_loader):.4f} | Val Loss: {val_loss/len(val_loader):.4f}\") \n \n# Train the model \ntrain_model(model, train_loader, val_loader) \n \n \n \n \n \n \n(OR) \n \n \n(b) \nUsing the PyTorch implementation of a Restricted Boltzmann Machine (RBM), develop a \ncollaborative filtering-based recommender system on the MovieLens dataset."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 33,
    "source": "deep learning.pdf",
    "chunk_text": "Answers: \n# Imports \nimport numpy as np \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torch.utils.data import DataLoader \nimport pandas as pd \nfrom sklearn.model_selection import train_test_split \n \n# (1) LOAD AND PREPROCESS MOVIELENS DATA \n# Download from: https://files.grouplens.org/datasets/movielens/ml-100k.zip and extract \nit. \n \nratings = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\", \n\"timestamp\"]) \nn_users = ratings.user_id.nunique() \nn_items = ratings.item_id.nunique() \n \n# Create user-item matrix \ndef create_user_item_matrix(df, n_users, n_items): \n data_matrix = np.zeros((n_users, n_items), dtype=np.float32) \n for row in df.itertuples(): \n data_matrix[row.user_id - 1, row.item_id - 1] = row.rating \nCO4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 34,
    "source": "deep learning.pdf",
    "chunk_text": "34     return data_matrix    data = create_user_item_matrix(ratings, n_users, n_items)  train_data, test_data = train_test_split(data, test_size=0.2)    train_tensor = torch.FloatTensor(train_data)  test_tensor = torch.FloatTensor(test_data)    # (2) DEFINE RESTRICTED BOLTZMANN MACHINE (RBM)  class RBM(nn.Module):   def __init__(self, n_visible, n_hidden):   super(RBM, self).__init__()   self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)   self.h_bias = nn.Parameter(torch.zeros(n_hidden))   self.v_bias = nn.Parameter(torch.zeros(n_visible))     def sample_h(self, v):   wx = torch.matmul(v, self.W.t()) + self.h_bias   prob = torch.sigmoid(wx)   return prob, torch.bernoulli(prob)     def sample_v(self, h):   wx = torch.matmul(h, self.W) + self.v_bias   prob = torch.sigmoid(wx)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 34,
    "source": "deep learning.pdf",
    "chunk_text": "return prob, torch.bernoulli(prob)     def forward(self, v):   h_prob, h_sample = self.sample_h(v)   v_prob, v_sample = self.sample_v(h_sample)   return v_prob     def contrastive_divergence(self, v0, k=1, lr=0.01):   v = v0.clone()   for step in range(k):   h_prob0, h0 = self.sample_h(v)   v_prob, v = self.sample_v(h0)   h_prob1, _ = self.sample_h(v)     # Update weights   self.W.data += lr * (torch.matmul(h_prob0.t(), v0) - torch.matmul(h_prob1.t(), v)) /  v0.size(0)   self.v_bias.data += lr * torch.sum(v0 - v, dim=0) / v0.size(0)   self.h_bias.data += lr * torch.sum(h_prob0 - h_prob1, dim=0) / v0.size(0)     loss = torch.mean((v0 - v) ** 2)   return loss    # ⚙ (3) COMPILE MODEL  n_visible = n_items  n_hidden = 64  rbm = RBM(n_visible, n_hidden)    # (4) TRAIN RBM MODEL  def"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 34,
    "source": "deep learning.pdf",
    "chunk_text": "train_rbm(rbm, train_tensor, epochs=10, batch_size=64, lr=0.01):"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 35,
    "source": "deep learning.pdf",
    "chunk_text": "35     train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)   for epoch in range(epochs):   total_loss = 0   for batch in train_loader:   batch = batch.clone()   batch[batch == 0] = -1 # mask unrated items   loss = rbm.contrastive_divergence(batch, k=1, lr=lr)   total_loss += loss.item()   print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")    train_rbm(rbm, train_tensor)    # (5) MAKE RECOMMENDATIONS  def recommend(rbm, user_vector, top_k=10):   with torch.no_grad():   input_vec = user_vector.clone()   input_vec[input_vec == 0] = -1 # mask missing ratings   output = rbm(input_vec)   predicted_ratings = output[0]   recommended_items = torch.argsort(predicted_ratings, descending=True)   return recommended_items[:top_k]    # Example: Recommend"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 35,
    "source": "deep learning.pdf",
    "chunk_text": "for user 0  user_id = 0  user_vector = train_tensor[user_id].unsqueeze(0)  recommendations = recommend(rbm, user_vector)  print(\"Top recommended movie indices for user 0:\", recommendations.tolist())        # Imports  import numpy as np  import torch  import torch.nn as nn  import torch.optim as optim  from torch.utils.data import DataLoader  import pandas as pd  from sklearn.model_selection import train_test_split    # (1) LOAD AND PREPROCESS MOVIELENS DATA  # Download from: https://files.grouplens.org/datasets/movielens/ml-100k.zip and extract it   ratings = pd.read_csv(\"ml-100k/u.data\", sep=\"\\t\", names=[\"user_id\", \"item_id\", \"rating\",  \"timestamp\"])  n_users = ratings.user_id.nunique()  n_items = ratings.item_id.nunique()    # Create user-item matrix  def create_user_item_matrix(df,"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 35,
    "source": "deep learning.pdf",
    "chunk_text": "n_users, n_items):   data_matrix = np.zeros((n_users, n_items), dtype=np.float32)   for row in df.itertuples():   data_matrix[row.user_id - 1, row.item_id - 1] = row.rating"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 36,
    "source": "deep learning.pdf",
    "chunk_text": "36     return data_matrix    data = create_user_item_matrix(ratings, n_users, n_items)  train_data, test_data = train_test_split(data, test_size=0.2)    train_tensor = torch.FloatTensor(train_data)  test_tensor = torch.FloatTensor(test_data)    # (2) DEFINE RESTRICTED BOLTZMANN MACHINE (RBM)  class RBM(nn.Module):   def __init__(self, n_visible, n_hidden):   super(RBM, self).__init__()   self.W = nn.Parameter(torch.randn(n_hidden, n_visible) * 0.1)   self.h_bias = nn.Parameter(torch.zeros(n_hidden))   self.v_bias = nn.Parameter(torch.zeros(n_visible))     def sample_h(self, v):   wx = torch.matmul(v, self.W.t()) + self.h_bias   prob = torch.sigmoid(wx)   return prob, torch.bernoulli(prob)     def sample_v(self, h):   wx = torch.matmul(h, self.W) + self.v_bias   prob = torch.sigmoid(wx)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 36,
    "source": "deep learning.pdf",
    "chunk_text": "return prob, torch.bernoulli(prob)     def forward(self, v):   h_prob, h_sample = self.sample_h(v)   v_prob, v_sample = self.sample_v(h_sample)   return v_prob     def contrastive_divergence(self, v0, k=1, lr=0.01):   v = v0.clone()   for step in range(k):   h_prob0, h0 = self.sample_h(v)   v_prob, v = self.sample_v(h0)   h_prob1, _ = self.sample_h(v)     # Update weights   self.W.data += lr * (torch.matmul(h_prob0.t(), v0) - torch.matmul(h_prob1.t(), v)) /  v0.size(0)   self.v_bias.data += lr * torch.sum(v0 - v, dim=0) / v0.size(0)   self.h_bias.data += lr * torch.sum(h_prob0 - h_prob1, dim=0) / v0.size(0)     loss = torch.mean((v0 - v) ** 2)   return loss    # ⚙ (3) COMPILE MODEL  n_visible = n_items  n_hidden = 64  rbm = RBM(n_visible, n_hidden)    # (4) TRAIN RBM MODEL"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 37,
    "source": "deep learning.pdf",
    "chunk_text": "37    def train_rbm(rbm, train_tensor, epochs=10, batch_size=64, lr=0.01):   train_loader = DataLoader(train_tensor, batch_size=batch_size, shuffle=True)   for epoch in range(epochs):   total_loss = 0   for batch in train_loader:   batch = batch.clone()   batch[batch == 0] = -1 # mask unrated items   loss = rbm.contrastive_divergence(batch, k=1, lr=lr)   total_loss += loss.item()   print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}\")    train_rbm(rbm, train_tensor)    # (5) MAKE RECOMMENDATIONS  def recommend(rbm, user_vector, top_k=10):   with torch.no_grad():   input_vec = user_vector.clone()   input_vec[input_vec == 0] = -1 # mask missing ratings   output = rbm(input_vec)   predicted_ratings = output[0]   recommended_items = torch.argsort(predicted_ratings,"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 37,
    "source": "deep learning.pdf",
    "chunk_text": "descending=True)   return recommended_items[:top_k]    # Example: Recommend for user 0  user_id = 0  user_vector = train_tensor[user_id].unsqueeze(0)  recommendations = recommend(rbm, user_vector)  print(\"Top recommended movie indices for user 0:\", recommendations.tolist())               ____________________            For Set 1 QP    Part - A  Question No."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 37,
    "source": "deep learning.pdf",
    "chunk_text": "1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK3 \nK4 \nK3 \nK2 \nK4 \nK4 \nK2 \nK2 \nK2 \nK3 \nDifficulty Level \n3 \n3 \n4 \n2 \n3 \n3 \n3 \n2 \n3 \n2 \n \nPart - B \nPart - C \nQuestion No. \n11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK2 \nK6 \nK6 \nK2 \nK6 \nK3 \nK3 \nK2 \nK2 \nK2 \nK6 \nK4 \nDifficulty Level \n3 \n3 \n4 \n3 \n3 \n3 \n3 \n3 \n3 \n3 \n4 \n3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 38,
    "source": "deep learning.pdf",
    "chunk_text": "38 \n \nSET 2 (Answer Key) \n \n \n \nCommon to (AI-DS, AI-ML) \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nCreate the model for the following network using PyTorch. \n \nAnswer: \n \n \nCO1 \n2. \nHow does dropout regularization work in neural networks? \nAnswer: \nDropout regularization randomly disables a fraction of neurons during training to prevent \noverfitting. This forces the network to learn more robust and generalized features. \n \nCO1 \n3. \nSummarize the key concepts and principles that drive different strategies in transfer learning. \nAnswer: \nDeep Transfer Learning Strategies \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 39,
    "source": "deep learning.pdf",
    "chunk_text": "39 \n \n• Direct use of pre-trained models \n• Leveraging feature extraction from pre-trained models \n• Fine-tuning layers of pre-trained models \n4. \nDetermine the convolution layer output shape if the input image shape is 128x128x3 and it uses 32 \nfilters of size 5x5 with a stride of 1 and padding of 2. \nAnswer: \n \nCO2 \n5. \nDistinguish between the phenomena of exploding gradients and vanishing gradients. \nAnswer: \nExploding gradients occur when weights grow too large during backpropagation, making training \nunstable. \n Vanishing gradients happen when gradients shrink, causing very slow or no learning, especially \nin deep RNNs. \nCO3 \n6. \nWhy do we need bidirectional RNNs in sequence modeling?"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 39,
    "source": "deep learning.pdf",
    "chunk_text": "Answer: \nBidirectional RNNs process data in both forward and backward directions, capturing past and future \ncontext, which improves performance in tasks like sentiment analysis and speech recognition. \nCO3 \n7. \nMention one key advantage of using denoising autoencoders for feature learning. \nAnswer: \nOne key advantage of using denoising autoencoders for feature learning is that they learn robust \nand meaningful features by reconstructing the original input from a corrupted version, helping the \nmodel generalize better and resist noise in real-world data. \nCO4 \n8. \nHow do the Generator and Discriminator in GANs engage in a competitive learning process? \nAnswer: \nThe generator tries to produce realistic data, while the discriminator tries to distinguish real \nfrom fake data."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 39,
    "source": "deep learning.pdf",
    "chunk_text": "Each improves by competing against the other in a minimax game. \nCO4 \n9. \nDifferentiate semantic segmentation and instance segmentation. \nAnswer: \nSemantic Segmentation classifies each pixel in an image into a category (e.g., sky, road), but does \nnot differentiate between objects of the same class. \nInstance Segmentation not only classifies each pixel but also distinguishes between individual \ninstances of the same class (e.g., multiple cars). \n \nCO5 \n10. \nDefine non-maximum suppression and explain how it helps refine the output of object detection \nmodels. \nAnswer: \nNon-Maximum Suppression (NMS) removes overlapping bounding boxes by keeping only the \none with the highest confidence score, helping to eliminate duplicate detections. \nCO5 \n \n PART B (5 x 13 = 65 marks)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 40,
    "source": "deep learning.pdf",
    "chunk_text": "40 \n \n \n \n \nCO \n11. \n(a) \nWrite a PyTorch program to implement a linear regression model using the dataset where \ny=2X+1+e, with X ranging from 1 to 50 and e as random noise. Define a linear regression \nmodel, use Mean Squared Error (MSE) as the loss function, and optimize it with Stochastic \nGradient Descent (SGD). Train the model for 50 epochs, ensuring the inclusion of the \nforward pass, loss computation, backpropagation, and parameter updates. Finally, display \nthe learned parameters (weights and bias) after training \n \n \nCO1"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 41,
    "source": "deep learning.pdf",
    "chunk_text": "41 \n \n \n \n \n \n \n(OR) \n \n \n(b) \n(i) Differentiate between shallow and deep neural networks. (6 Marks) \nCO1"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 42,
    "source": "deep learning.pdf",
    "chunk_text": "42 \n \n \n(ii) Explain the concept of loss functions in deep learning. Discuss popular loss functions \nsuch as mean squared error (MSE), categorical cross-entropy, and binary cross-entropy. (7 \nMarks)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 43,
    "source": "deep learning.pdf",
    "chunk_text": "43 \n \n \n \n \n \n \n \n \n \n12. \n(a) \nImplement transfer learning by using VGG as the base model to classify the CIFAR-100 \ndataset using PyTorch. \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 44,
    "source": "deep learning.pdf",
    "chunk_text": "44 \n \ni) Load the CIFAR-100 dataset (3 Mark) \nii) Use a pre-trained VGG model as the base model and modify its classifier for \nCIFAR-100. (3 Mark) \niii) Create a sequential model with the appropriate number of neurons in the \noutput layer, activation function, and loss function (3 Mark) \niv) Train the model with training data and validate it using the test dataset, and \nevaluate its accuracy. (4 Mark)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 45,
    "source": "deep learning.pdf",
    "chunk_text": "45 \n \n \n \n \n \n(OR) \n \n \n(b) \nExplain data augmentation and its various techniques, providing illustrative examples for \neach approach. \nData augmentation \n· Data augmentation is a process of artificially increasing the amount of data by \ngenerating new data points from existing data. \n· Data augmentation includes adding minor alterations to data or using machine \nlearning models to generate new data points in the latent space of original data to \namplify the dataset. \nSynthetic data: When data is generated artificially without using real-world images."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 45,
    "source": "deep learning.pdf",
    "chunk_text": "Synthetic data \nare often produced by Generative Adversarial Networks \nAugmented data: Derived from original images with some sort of minor geometric \ntransformations (such as flipping, translation, rotation, or the addition of noise) in order to increase \nthe diversity of the training set. \nData Augmentation Techniques \n· Flip \n· Rotation \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 46,
    "source": "deep learning.pdf",
    "chunk_text": "46 \n \n· Scale \n· Crop \n· Translation \n· Gaussian Noise \nFlip \n· Images can be flipped horizontally and vertically. \n· A vertical flip is equivalent to rotating an image by 180 degrees and then performing \na horizontal flip. \n \nRotation \n· One key thing to note about this operation is that image dimensions may not be \npreserved after rotation. \n· If your image is a square, rotating it at right angles will preserve the image size. \n· If it’s a rectangle, rotating it by 180 degrees would preserve the size. \n· Rotating the image by finer angles will also change the final image size. \n \n· The image can be scaled outward or inward. \n· While scaling outward, the final image size will be larger than the original image \nsize. \n· Scaling inward reduces the image size."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 46,
    "source": "deep learning.pdf",
    "chunk_text": "Crop \n· Unlike scaling, we just randomly sample a section from the original image. \n· We then resize this section to the original image size. \n· This method is popularly known as random cropping."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 47,
    "source": "deep learning.pdf",
    "chunk_text": "47 \n \nTranslation \n· Translation just involves moving the image along the X or Y direction (or both). \n· This method of augmentation is very useful as most objects can be located at almost \nanywhere in the image. \n \nGaussian Noise \nOver-fitting usually happens when your neural network tries to learn high frequency features \n(patterns that occur a lot) that may not be useful. Gaussian noise, which has zero mean, essentially \nhas data points in all frequencies, effectively distorting the high frequency features. This also means \nthat lower frequency components (usually, your intended data) are also distorted, but your neural \nnetwork can learn to look past that. Adding just the right amount of noise can enhance the learning \ncapability. \n \n \n \n \n \n \n \n13."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 47,
    "source": "deep learning.pdf",
    "chunk_text": "(a) \nDiscuss the vanishing and exploding gradient problems in RNNs. How do LSTM and \nGRU architectures mitigate these issues? \nVanishing and Exploding Gradient Problems in RNNs: \n● Vanishing Gradient Problem: \n○ Description: During backpropagation through time (BPTT), gradients \nshrink exponentially as they are propagated back through many time steps. \nThis is because the derivatives of activation functions (like sigmoid, \ncommonly used in older RNNs) are often small (e.g., between 0 and 0.25 \nfor sigmoid). When these small values are multiplied together over many \nlayers/time steps (due to the chain rule), the gradient quickly approaches \nzero."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 47,
    "source": "deep learning.pdf",
    "chunk_text": "○ Impact: Small or vanishing gradients make it difficult for the network to \nlearn long-term dependencies, meaning information from earlier parts of a \nsequence has little influence on updates to weights that affect later parts. \nThis prevents the RNN from capturing relationships between distant \nelements in a sequence, such as words far apart in a long sentence. \n● Exploding Gradient Problem: \n○ Description: While less emphasized in the provided text, the chain rule \ncan also lead to gradients becoming extremely large. This happens when \nthe values being multiplied in the chain rule are consistently large, causing \nthe gradient to grow exponentially. \n○ Impact: Exploding gradients can lead to unstable training, where the \nCO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 48,
    "source": "deep learning.pdf",
    "chunk_text": "48 \n \nmodel weights receive very large updates, causing the learning process to \ndiverge and the model to perform poorly. \nHow LSTM and GRU Architectures Mitigate These Issues: \nLSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) architectures were \nspecifically designed to address the vanishing and exploding gradient problems prevalent \nin simple RNNs. They achieve this through their unique gating mechanisms: \n● LSTMs (Long Short-Term Memory RNNs): \n \n○ LSTMs introduce a \"cell state\" and various \"gates\" (input gate, forget gate, \nand output gate) that regulate the flow of information. \n○ The cell state acts as a long-term memory, capable of carrying information \nacross many time steps without significant degradation."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 48,
    "source": "deep learning.pdf",
    "chunk_text": "○ The forget gate determines what information to discard from the cell state, \npreventing old, irrelevant information from accumulating. \n○ The input gate controls which new information gets stored in the cell \nstate. \n○ The output gate controls what part of the cell state is outputted as the \nhidden state. \n○ These gates, implemented with sigmoid and tanh activation functions, \nallow LSTMs to selectively remember or forget information, creating a \n\"constant error carousel\" that helps gradients flow more effectively over \nlong distances, thus mitigating the vanishing gradient problem. \n● GRUs (Gated Recurrent Unit RNNs): \n \n○ GRUs are a simplified version of LSTMs, designed to be computationally \nmore efficient while still effectively addressing the gradient problems."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 48,
    "source": "deep learning.pdf",
    "chunk_text": "○ They combine the forget and input gates into a single \"update gate\" and \nalso have a \"reset gate.\" \n○ The update gate determines how much of the past information (from the \nprevious hidden state) should be passed on to the current hidden state and"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 49,
    "source": "deep learning.pdf",
    "chunk_text": "49 \n \nhow much new information should be incorporated. \n○ The reset gate decides how much of the previous hidden state to forget \nbefore computing the new hidden state. \n○ By having fewer gates than LSTMs, GRUs offer a more streamlined \nstructure that still allows for better control over information flow and helps \nto preserve gradients over longer sequences, combating both vanishing and \nexploding gradients. \nBoth LSTMs and GRUs enable RNNs to capture long-term dependencies that simple \nRNNs struggle with, making them highly effective for sequential data tasks like text \ngeneration and speech recognition. \n \n \n(OR) \n \n \n(b) \nExplain BiRNN and the need for bidirectional traversal with the Pytorch implementation. \n· BiRNN stands for Bidirectional Recurrent Neural Network."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 49,
    "source": "deep learning.pdf",
    "chunk_text": "· It is an extension of a standard RNN where two RNNs are run: \n· One processes the input forward (from beginning to end). \n· The other processes the input backward (from end to beginning). \n· The outputs from both directions are then combined (usually concatenated or \nsummed) at each time step. \n \n· Consider the sentence: \n\"Michael Eats Dosa in Chennai\" \n· To classify words like \"dosa\", the model needs past context (\"eats\") \nand for \"Chennai\", the model needs future context (\"in\"). \nBenefits of BiRNNs: \n· Better context understanding in sequence tasks (e.g., language modeling, \nspeech recognition). \n· Improved accuracy in tasks like: \no Named Entity Recognition \nCO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 50,
    "source": "deep learning.pdf",
    "chunk_text": "50    o Part-of-Speech Tagging  o Sentiment Analysis  o Machine Translation    import torch  import torch.nn as nn    class BiRNN(nn.Module):   def __init__(self, input_size, hidden_size, output_size, num_layers=1):   super(BiRNN, self).__init__()   self.hidden_size = hidden_size   self.num_layers = num_layers     # Bidirectional RNN   self.rnn = nn.RNN(input_size, hidden_size, num_layers,   batch_first=True, bidirectional=True)     # The output layer uses hidden_size * 2 because of bidirection   self.fc = nn.Linear(hidden_size * 2, output_size)     def forward(self, x):   # Initialize hidden state: (num_layers * 2, batch_size, hidden_size)   h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_size)     # Forward propagate RNN   out, _ = self.rnn(x, h0)     # Pass the last time"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 50,
    "source": "deep learning.pdf",
    "chunk_text": "step output from both directions to the FC layer   out = self.fc(out[:, -1, :]) # shape: (batch_size, output_size)   return out      model = BiRNN(input_size, hidden_size, output_size)            14."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 50,
    "source": "deep learning.pdf",
    "chunk_text": "(a) \nExplain Sparse Autoencoders using L1 regularization and KL divergence. Also, \nimplement a sparse autoencoder with PyTorch implementation. \nAutoencoders are a type of neural network that learns a compressed, distributed \nrepresentation (encoding) of input data in an unsupervised manner. They consist of two \nmain parts: an encoder that maps the input to a hidden (latent) representation, and a \ndecoder that reconstructs the input from this hidden representation. The goal is to minimize \nthe reconstruction error between the input and the output. \nSparse Autoencoders \nWhile standard autoencoders aim to learn an efficient representation, they can sometimes \nCO4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 51,
    "source": "deep learning.pdf",
    "chunk_text": "51 \n \nlearn the identity function if the hidden layer has enough capacity, leading to poor feature \nextraction and overfitting. Sparse autoencoders address this by introducing a sparsity \nconstraint on the hidden layer activations. This constraint encourages the hidden units to \nbe mostly inactive (their activations are close to zero) for any given input. This forces the \nnetwork to learn a more distributed representation where different hidden units specialize \nin detecting different features in the input data. \nThere are primarily two common ways to enforce sparsity in autoencoders: \n1. L1 Regularization \n2."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 51,
    "source": "deep learning.pdf",
    "chunk_text": "KL Divergence \nL1 Regularization for Sparsity \nL1 regularization (also known as Lasso regularization) adds a penalty term to the loss \nfunction that is proportional to the absolute value of the hidden unit activations. \nThe total loss function for an L1 sparse autoencoder is given by: \n \nWhere: \n● x is the input data. \n● x′ is the reconstructed output. \n● ∥x−x′∥2 is the reconstruction loss, often Mean Squared \nError (MSE). \n● λ is the regularization parameter, controlling the strength of the sparsity penalty. \nA larger λ promotes more sparsity. \n● aj is the activation of the j-th hidden unit. \n● s is the number of hidden units."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 51,
    "source": "deep learning.pdf",
    "chunk_text": "By penalizing the sum of absolute activations, L1 regularization encourages many of the \naj values to become exactly zero, effectively \"turning off\" a significant portion of the \nhidden units for a given input. This directly promotes sparsity. \nKL Divergence for Sparsity \nInstead of directly penalizing the absolute activations, KL divergence enforces sparsity by \nencouraging the average activation of each hidden unit to be close to a predefined small \nvalue, ρ (rho), which represents the desired sparsity level (e.g., 0.05 or 0.1). \nThe Kullback-Leibler (KL) divergence measures the difference between two probability \ndistributions."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 51,
    "source": "deep learning.pdf",
    "chunk_text": "In the context of sparse autoencoders, we treat the average activation of a \nhidden unit as a Bernoulli random variable with a probability of activation ρ^j, and we want \nthis to be close to a target Bernoulli distribution with probability ρ."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 52,
    "source": "deep learning.pdf",
    "chunk_text": "52 \n \nThe KL divergence term for a single hidden unit j is: \n \nWhere: \n● β is the weight of the sparsity penalty, controlling its influence on the total loss. \n● The other terms are as defined for L1 regularization. \nThe KL divergence term is minimized when ρ^j is close to ρ. If ρ^j is much larger than ρ, \nthe penalty increases, pushing ρ^j down."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 52,
    "source": "deep learning.pdf",
    "chunk_text": "If ρ^j is much smaller than ρ, the penalty also  increases (though less drastically for very small ρ^j), encouraging some activation    import torch.nn.functional as F  from torchvision import datasets, transforms  from torch.utils.data import DataLoader    # Hyperparameters  input_size = 784 # For MNIST  hidden_size = 128  batch_size = 64  learning_rate = 1e-3  num_epochs = 10  l1_lambda = 1e-5 # L1 regularization coefficient    # Dataset  transform = transforms.ToTensor()  train_loader = DataLoader(   datasets.MNIST(root='./data', train=True, download=True, transform=transform),   batch_size=batch_size,   shuffle=True  )    # Autoencoder with L1 sparsity  class SparseAutoencoder(nn.Module):   def __init__(self):   super(SparseAutoencoder, self).__init__()   self.encoder ="
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 52,
    "source": "deep learning.pdf",
    "chunk_text": "nn.Linear(input_size, hidden_size)   self.decoder = nn.Linear(hidden_size, input_size)     def forward(self, x):   x = x.view(-1, input_size)   hidden = torch.sigmoid(self.encoder(x))   output = torch.sigmoid(self.decoder(hidden))   return output, hidden    model = SparseAutoencoder()  criterion = nn.MSELoss()  optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 53,
    "source": "deep learning.pdf",
    "chunk_text": "53 \n \n# Training \nfor epoch in range(num_epochs): \n total_loss = 0 \n for batch, _ in train_loader: \n batch = batch.view(-1, input_size) \n optimizer.zero_grad() \n outputs, hidden = model(batch) \n mse_loss = criterion(outputs, batch) \n l1_loss = l1_lambda * torch.sum(torch.abs(hidden)) \n loss = mse_loss + l1_loss \n loss.backward() \n optimizer.step() \n total_loss += loss.item() \n \n print(f\"Epoch {epoch+1}, Loss: {total_loss / len(train_loader):.4f}\") \n \n \n \n \n(OR) \n \n \n(b) \nExplain the architecture and working of Generative Adversarial Networks (GANs). \n \nA GAN consists of two primary neural networks: \n1. Generator (G): This network is responsible for creating new data samples (e.g., \nimages, text, audio) that are as realistic as possible, mimicking the real data \ndistribution."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 53,
    "source": "deep learning.pdf",
    "chunk_text": "It takes a random noise vector as input, often sampled from a \nsimple distribution like a Gaussian or uniform distribution. This noise vector \nacts as the \"creative seed\" for the generator. The generator's architecture \ntypically involves layers that upsample this noise into the desired data format \n(e.g., transposed convolutional layers for image generation). \n2. Discriminator (D): This network acts as a \"critic\" or \"authenticator.\" It takes \nan input data sample and outputs a probability (a value between 0 and 1) \nindicating whether it believes the sample is real (from the actual training \ndataset) or fake (generated by the generator)."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 53,
    "source": "deep learning.pdf",
    "chunk_text": "The discriminator is typically a \nstandard classifier, often using convolutional layers for image data, that learns \nto distinguish between real and fake inputs. \nHow GANs Work (The Adversarial Process) \nThe working principle of GANs can be likened to a game between a forger (the \nGenerator) and a detective (the Discriminator): \n● The Forger (Generator): Tries to create fake banknotes (data) that are so \nconvincing the detective can't tell them apart from real ones. \n● The Detective (Discriminator): Tries to become an expert at telling the \ndifference between real banknotes and fake ones. \nThis competition drives both networks to improve: \n1. Generator's Goal: To produce data that is indistinguishable from real data, \nthereby \"fooling\" the discriminator."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 53,
    "source": "deep learning.pdf",
    "chunk_text": "Its objective is to maximize the \nprobability of the discriminator making a mistake (classifying generated data \nCO4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 54,
    "source": "deep learning.pdf",
    "chunk_text": "54 \n \nas real). \n2. Discriminator's Goal: To accurately distinguish between real and fake data. \nIts objective is to minimize the probability of making mistakes (correctly \nclassifying real data as real and generated data as fake) \n \n \n \n \n \n \n \n15. \n(a) \nExplain the architecture of Fast R-CNN for object detection. Describe the role of Selective \nSearch, ROI Pooling, and the two output branches in the detection pipeline. \n \n \nCO5"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 55,
    "source": "deep learning.pdf",
    "chunk_text": "55 \n \n \n \n \n \n \n \n \n \n(OR) \n \n \n(b) \nExplain the working of a Single Shot Detector (SSD) and discuss how it effectively \ndetects objects of varying sizes in an image. \nCO5"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 56,
    "source": "deep learning.pdf",
    "chunk_text": "56 \n \n \n \n \n \n \n \n PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \nCO \n16. \n(a) \nCreate the PyTorch implementation for the stock price prediction using RNN. \n(i) Implement the input preprocessing . (4 Marks) \n(ii) Implement a stock price prediction class with an appropriate loss function and optimizer \nfor multi-class classification. (6 Marks) \n(iii) Implement a function to train the model using the training dataset. (5 Marks) \n \n \n \n \n \nCO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 57,
    "source": "deep learning.pdf",
    "chunk_text": "57        # Import Libraries  import numpy as np  import torch  import torch.nn as nn  from sklearn.preprocessing import MinMaxScaler  from sklearn.model_selection import train_test_split    # (i) INPUT PREPROCESSING — 4 Marks  np.random.seed(42)  prices = np.cumsum(np.random.randn(1000) * 2 + 0.1) + 100 # Random walk    def label_movement(prices):   movement = []   for i in range(1, len(prices)):   diff = prices[i] - prices[i - 1]   if diff > 0.5:   movement.append(2) # Up   elif diff < -0.5:   movement.append(0) # Down   else:   movement.append(1) # No Change   return np.array(movement)    labels = label_movement(prices)  scaler = MinMaxScaler()  scaled_prices = scaler.fit_transform(prices[:-1].reshape(-1, 1)) # exclude last price    def create_sequences(data, labels, seq_len=10):   X, y"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 57,
    "source": "deep learning.pdf",
    "chunk_text": "= [], []   for i in range(len(data) - seq_len):   X.append(data[i:i+seq_len])   y.append(labels[i+seq_len])   return np.array(X), np.array(y)    X, y = create_sequences(scaled_prices, labels)  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)  X_train = torch.tensor(X_train, dtype=torch.float32)  X_test = torch.tensor(X_test, dtype=torch.float32)  y_train = torch.tensor(y_train, dtype=torch.long)  y_test = torch.tensor(y_test, dtype=torch.long)    # (ii) MODEL DEFINITION + LOSS + OPTIMIZER — 6 Marks  class StockRNN(nn.Module):   def __init__(self, input_size=1, hidden_size=64, num_layers=1, num_classes=3):   super(StockRNN, self).__init__()   self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)   self.fc = nn.Linear(hidden_size,"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 57,
    "source": "deep learning.pdf",
    "chunk_text": "num_classes)   def forward(self, x):   out, _ = self.rnn(x)   out = out[:, -1, :] # Last timestep   return self.fc(out)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 58,
    "source": "deep learning.pdf",
    "chunk_text": "58    model = StockRNN()  criterion = nn.CrossEntropyLoss()  optimizer = torch.optim.Adam(model.parameters(), lr=0.001)    # (iii) TRAINING FUNCTION — 5 Marks  def train_model(model, X_train, y_train, epochs=20):   model.train()   for epoch in range(epochs):   optimizer.zero_grad()   output = model(X_train)   loss = criterion(output, y_train)   loss.backward()   optimizer.step()   _, preds = torch.max(output, 1)   acc = (preds == y_train).float().mean()   print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}, Accuracy:  {acc.item():.4f}\")    train_model(model, X_train, y_train)    # (Optional) EVALUATION  def evaluate(model, X_test, y_test):   model.eval()   with torch.no_grad():   output = model(X_test)   _, preds = torch.max(output, 1)   acc = (preds == y_test).float().mean()"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 58,
    "source": "deep learning.pdf",
    "chunk_text": "print(f\"Test Accuracy: {acc.item():.4f}\")    evaluate(model, X_test, y_test)          (OR)      (b)    Develop a Named Entity Recognition (NER) system using PyTorch with the following  requirements:  (i) Implement an LSTM-based neural network architecture  (ii) Include proper data preprocessing for sequence labeling  (iii) Compile the model with appropriate loss function and optimizer  (iv) Train the model and evaluate its performance    # Imports  import torch  import torch.nn as nn  from collections import defaultdict  from sklearn.metrics import classification_report  CO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 59,
    "source": "deep learning.pdf",
    "chunk_text": "59      # Sample Data (Tiny Dataset for Demo)  train_data = [   [(\"John\", \"B-PER\"), (\"lives\", \"O\"), (\"in\", \"O\"), (\"New\", \"B-LOC\"), (\"York\", \"ILOC\")],   [(\"Mary\", \"B-PER\"), (\"is\", \"O\"), (\"from\", \"O\"), (\"Paris\", \"B-LOC\")],  ]  val_data = [   [(\"Steve\", \"B-PER\"), (\"works\", \"O\"), (\"in\", \"O\"), (\"London\", \"B-LOC\")]  ]    # Preprocessing - Build Vocabulary  word2idx = defaultdict(lambda: len(word2idx)); tag2idx = defaultdict(lambda:  len(tag2idx))  PAD, UNK = \"<PAD>\", \"<UNK>\"; word2idx[PAD]; word2idx[UNK]; tag2idx[PAD]  for sentence in train_data:   for word, tag in sentence:   word2idx[word]; tag2idx[tag]  max_len = max(len(s) for s in train_data + val_data)    # Encode Sentences  def encode(sentence, word2idx, tag2idx, max_len):   words = [word for word, tag in sentence]   tags = [tag for word,"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 59,
    "source": "deep learning.pdf",
    "chunk_text": "tag in sentence]   word_ids = [word2idx.get(w, word2idx[UNK]) for w in words]   tag_ids = [tag2idx[t] for t in tags]   while len(word_ids) < max_len:   word_ids.append(word2idx[PAD])   tag_ids.append(tag2idx[PAD])   return word_ids, tag_ids    X_train = torch.tensor([encode(s, word2idx, tag2idx, max_len)[0] for s in train_data])  y_train = torch.tensor([encode(s, word2idx, tag2idx, max_len)[1] for s in train_data])  X_val = torch.tensor([encode(s, word2idx, tag2idx, max_len)[0] for s in val_data])  y_val = torch.tensor([encode(s, word2idx, tag2idx, max_len)[1] for s in val_data])    # Define LSTM-based NER Model  class NERLSTM(nn.Module):   def __init__(self, vocab_size, tagset_size, embedding_dim=64, hidden_dim=128):   super(NERLSTM, self).__init__()   self.embedding ="
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 59,
    "source": "deep learning.pdf",
    "chunk_text": "nn.Embedding(vocab_size, embedding_dim,  padding_idx=word2idx[PAD])   self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True,  bidirectional=True)   self.fc = nn.Linear(hidden_dim * 2, tagset_size)   def forward(self, x):   x = self.embedding(x)   x, _ = self.lstm(x)   return self.fc(x)    # Compile Model: Loss + Optimizer  model = NERLSTM(len(word2idx), len(tag2idx))  criterion = nn.CrossEntropyLoss(ignore_index=tag2idx[PAD])  optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 60,
    "source": "deep learning.pdf",
    "chunk_text": "60      # Train Model  def train(model, X, y, epochs=10):   model.train()   for epoch in range(epochs):   optimizer.zero_grad()   out = model(X).view(-1, len(tag2idx)) # (batch*seq_len, num_tags)   loss = criterion(out, y.view(-1))   loss.backward()   optimizer.step()   print(f\"Epoch {epoch+1}: Loss = {loss.item():.4f}\")  train(model, X_train, y_train)    # Evaluate Model  def evaluate(model, X, y_true):   model.eval()   with torch.no_grad():   y_pred = torch.argmax(model(X), dim=2)   true_labels, pred_labels = [], []   for true, pred in zip(y_true, y_pred):   for t, p in zip(true, pred):   if t != tag2idx[PAD]:   true_labels.append(t.item())   pred_labels.append(p.item())   idx2tag = {v: k for k, v in tag2idx.items()}   print(classification_report(   [idx2tag[i] for i in true_labels],"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 60,
    "source": "deep learning.pdf",
    "chunk_text": "[idx2tag[i] for i in pred_labels],   target_names=[k for k in tag2idx if k != PAD]   ))  evaluate(model, X_val, y_val)       _____________________              For Set 2 QP    Part - A  Question No."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 60,
    "source": "deep learning.pdf",
    "chunk_text": "1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK6 \nK2 \nK2 \nK3 \nK4 \nK2 \nK2 \nK4 \nK4 \nK2 \nDifficulty Level \n3 \n3 \n2 \n4 \n3 \n2 \n2 \n3 \n3 \n3 \n \n Part - B \nPart - C \nQuestion No. \n11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK3 \nK4 \nK6 \nK2 \nK3 \nK3 \nK3 \nK2 \nK4 \nK2 \nK6 \nK6 \nDifficulty Level \n3 \n3 \n4 \n3 \n3 \n3 \n3 \n3 \n3 \n3 \n4 \n4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 61,
    "source": "deep learning.pdf",
    "chunk_text": "61 \n \nSET 3 (Answer Key) \n \n \n19AI413– DEEP LEARNING AND ITS APPLICATIONS \nTime: Three hours Maximum marks: 100 \n \n \nAnswer All Questions \n \n PART A (10 x 2 = 20 marks) \n \n \n \nCO \n1. \nHow does the choice of a loss function impact the training of a neural network? \nAnswer: \nSelecting the right loss function is crucial in machine learning as it directly impacts the model's \ntraining and performance.The loss function measures the difference between the predicted and \nactual values, guiding the optimization process to improve the model \nCO1 \n2. \nHow does batch normalization help in accelerating the training of deep neural networks? \nAnswer: \nDropout regularization randomly disables a fraction of neurons during training to prevent \noverfitting."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 61,
    "source": "deep learning.pdf",
    "chunk_text": "This forces the network to learn more robust and generalized features. \nCO1 \n3. \nHow does data augmentation enhance the robustness of machine learning models? \nAnswer: \n• It reduces the cost of collection of data. \n• It reduces the cost of labelling data. \n• It improves the model prediction accuracy. \n• It prevents data scarcity. \n• It frames better data models. \n• It reduces data overfitting. \n• It creates variability and flexibility in data models. \n• It increases the generalization ability of the data models. \n• It helps in resolving the class imbalance issue in the classification \nCO2 \n4. \nDepict and explain the multiple filter processing for a three-channel image. \nAnswer: \n \nCO2 \n5. \nName the three gates in LSTM and state one function of each."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 61,
    "source": "deep learning.pdf",
    "chunk_text": "Answer: \n• Input gate: Controls what new information is stored. \n• Forget gate: Decides which old information to discard. \n• Output gate: Regulates the output from the current cell state. \nCO3 \n6. \nDifferentiate between one-to-many and many-to-many RNN architectures with examples. \nAnswer: \nLSTM has 3 gates: input, forget, and output; separates memory cell and hidden state. \nCO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 62,
    "source": "deep learning.pdf",
    "chunk_text": "62 \n \n \n \n \n PART B (5 x 13 = 65 marks) \n \n \n \n \n \nCO \n11. \n(a) \nDiscuss the significance of selecting appropriate loss functions for regression, binary \nclassification, and multiclass classification tasks, and demonstrate their application using \nPyTorch. \nAnswer: \n● Selecting the right loss function is crucial in machine learning as it directly impacts the \nmodel's training and performance. The loss function measures the difference between the \npredicted and actual values, guiding the optimization process to improve the model. \n1. Loss Functions for Regression Tasks \n \nIn regression problems, the output is a continuous numerical value. The most commonly \nused loss functions are: \nCO1 \nGRU has 2 gates: update and reset; combines memory and hidden state, making it faster. \n7."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 62,
    "source": "deep learning.pdf",
    "chunk_text": "Define Minimax Loss and Wasserstein Loss \nAnswer: \nMinimax Loss is used in standard GANs, where the generator tries to minimize and the \ndiscriminator tries to maximize the probability of correctly classifying real vs. fake data. \nWasserstein Loss, used in WGANs, replaces probability-based loss with the Earth Mover’s \nDistance, providing more stable training and better gradient flow. \nCO4 \n8. \nWhat is the function of the discriminator in a Generative Adversarial Network (GAN)? \nAnswer: \n• The function of the discriminator in a Generative Adversarial Network (GAN) is to \ndistinguish between real and fake data."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 62,
    "source": "deep learning.pdf",
    "chunk_text": "• It evaluates whether a given sample is from the real dataset or generated by the generator, \nand provides feedback to help both networks improve — encouraging the generator to \nproduce more realistic data. \nCO4 \n9. \nDifferentiate between R-CNN and SPPNet. \nAnswer: \n \nCO5 \n10. \nDefine panoptic segmentation. How does it combine the strengths of semantic and instance \nsegmentation? \nAnswer: \nPanoptic segmentation assigns both a class label and an instance ID to each pixel. It combines \nsemantic segmentation (classifying every pixel) with instance segmentation (differentiating \nobject instances). \nCO5"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 63,
    "source": "deep learning.pdf",
    "chunk_text": "63 \n \n· Mean Squared Error (MSE): Penalizes large errors more due to squaring. It is \nsensitive to outliers. \n \nWhere: n: number of examples or samples. y: true values. ^y: predicted values. \n● · \nsmaller value indicates that the ground truth and predicted values are closer \nto each other. \nImplementation : (using Class Based Approach) \n \nOr Implementation: (using API) \n \n· Mean Absolute Error (MAE): Less sensitive to outliers as it considers absolute \ndifferences. \n \nWhere: n: number of examples or samples. y: true values. ^y: predicted values. \n· Here also, a lower value indicates a better model \nImplementation : (using Class Based Approach)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 64,
    "source": "deep learning.pdf",
    "chunk_text": "64 \n \nImplementation: (using API) \n \n2. Loss Function for Binary Classification Tasks \nBinary classification problems involve two classes (e.g., spam vs. not spam). The most \ncommonly used loss function is: \n● Binary Cross-Entropy (BCE) Loss: Measures the difference between the predicted \nprobability and the actual class. \nImplementation : (using Class Based Approach) \n \nImplementation: (using API) \n \n3. Loss Function for Multiclass Classification Tasks \nFor multiclass classification problems where an instance belongs to one of multiple classes, the \ncommonly used loss function is: \n● Categorical Cross-Entropy (CCE) / Cross-Entropy Loss: Used for one-hot \nencoded labels and softmax outputs. \n \np is the softmax probability vector from the model's logits, and y is the ground truth label."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 65,
    "source": "deep learning.pdf",
    "chunk_text": "65 \n \nImplementation: (using API) \n \n \n \n(OR) \n \n \n(b) \n(i) Using PyTorch, implement a neural network to predict 'CompressiveStrength' using the \nremaining columns as input features. Create a model with three hidden layers, each having \n512 units and the ReLU activation, an output layer with one unit and no activation, and also \ninput_shape should be specified in the first layer. Apply the necessary preprocessing before \ntraining. (7Marks) \n(ii) Explain the concept of regularization in deep learning. Discuss common regularization \ntechniques such as L1 and L2 regularization, dropout, and batch normalization. (6 Marks) \n \n \nCO1"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 67,
    "source": "deep learning.pdf",
    "chunk_text": "67 \n \n \n(ii) Explain the concept of regularization in deep learning. Discuss common regularization \ntechniques such as L1 and L2 regularization, dropout, and batch normalization. (6 Marks) \nRegularization is a technique used in deep learning to prevent overfitting — a situation \nwhere a model performs well on training data but poorly on unseen data. Regularization \nintroduces additional information or constraints to the model, helping it generalize better to \nnew data. \nCommon Regularization Techniques: \n1. L1 Regularization (Lasso): \n○ Adds the absolute value of the weights to the loss function\n \n○ Encourages sparsity by driving some weights to zero. \n○ Useful for feature selection. \n2. L2 Regularization (Ridge):"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 68,
    "source": "deep learning.pdf",
    "chunk_text": "68 \n \n○ Adds the squared value of the weights to the loss function: \n \n○ Prevents large weights, promoting simpler models. \n○ Helps in weight decay and smoother generalization. \n3. Dropout: \n○ Randomly \"drops out\" (sets to zero) a percentage of neurons during training. \n○ Prevents co-adaptation of neurons. \n○ Forces the network to learn redundant, robust features. \n4. Batch Normalization: \n○ Normalizes the output of a layer for each mini-batch. \n○ Reduces internal covariate shift, improving training speed and stability. \nHas a regularizing effect and can sometimes reduce the need for dropout. \n \n \n \n \n12."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 68,
    "source": "deep learning.pdf",
    "chunk_text": "(a) \n(i) Develop the PyTorch implementation of the model with the input shape of (512,512,1) \n(7 Marks) \n \nimport torch \nimport torch.nn as nn \nclass CustomCNN(nn.Module): \n def __init__(self): \n super(CustomCNN, self).__init__() \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 69,
    "source": "deep learning.pdf",
    "chunk_text": "69     # Convolutional Layers   self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3, stride=1,  padding='same') # Output: (32, 512, 512)   self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1,  padding='same') # Output: (64, 512, 512)   # Flatten layer   self.flatten = nn.Flatten() # Output: (64 * 512 * 512 = 16777216)   # Fully Connected Layer   self.dense = nn.Linear(16777216, 6) # Output: (6)   def forward(self, x):   x = torch.relu(self.conv1(x))   x = torch.relu(self.conv2(x))   x = self.flatten(x)   x = self.dense(x)   return x  # Instantiate the model  model = CustomCNN()  # Print model summary  print(model)  # Print total parameters  total_params = sum(p.numel() for p in model.parameters())  print(f\"Total trainable parameters:"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 69,
    "source": "deep learning.pdf",
    "chunk_text": "{total_params}\")    (ii) Explain the significance of the convolution layer in deep learning models, particularly  its role in feature extraction and pattern recognition, and provide insights into its impact?"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 69,
    "source": "deep learning.pdf",
    "chunk_text": "(6 Marks) \nThe convolutional layer plays a pivotal role in deep learning models, especially in tasks \nrelated to computer vision and image analysis."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 70,
    "source": "deep learning.pdf",
    "chunk_text": "70 \n \nFeature Extraction: \nConvolutional layers are designed to automatically learn and extract meaningful features \nfrom input data. In computer vision, these features could be edges, textures, shapes, or \nother higher-level patterns. \nThey use small kernels (also known as filters) that slide across the input data, capturing \nlocal information and producing feature maps. Each filter is responsible for detecting a \nspecific pattern or feature. \nThe hierarchical structure of convolutional layers allows the network to learn complex \nfeatures by combining simple ones, which is crucial for understanding and representing the \nunderlying structure of the data. \nPattern Recognition: \nConvolutional layers excel at pattern recognition in images."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 70,
    "source": "deep learning.pdf",
    "chunk_text": "By learning hierarchical \nfeatures, they can recognize intricate patterns, such as objects, shapes, or textures, at \nvarious levels of abstraction. \nThrough the use of multiple convolutional layers followed by pooling layers, deep learning \nmodels can gradually build up a hierarchy of features, enabling them to recognize \nincreasingly complex patterns and objects. \nThe ability to automatically learn these patterns from data makes convolutional layers \nhighly adaptable and capable of generalizing well to new, unseen examples. \n \n \n(OR) \n \n \n(b) \nDescribe the concept of Convolutional Neural Networks (CNNs) and their role in computer \nvision tasks. Explain the key components of CNNs, including convolutional layers, pooling \nlayers, and fully connected layers."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 70,
    "source": "deep learning.pdf",
    "chunk_text": "Discuss how CNNs automatically learn hierarchical \npatterns and extract meaningful features from images. \nDefinition: \nA Convolutional Neural Network (CNN) is a specialized deep learning architecture \ndesigned for processing grid-like data (e.g., images, videos). CNNs automatically learn \nhierarchical features through convolutional operations, making them the backbone of \nmodern computer vision. \nRole in Computer Vision: \nCNNs excel at tasks like: \n● Image Classification (e.g., identifying objects in photos). \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 71,
    "source": "deep learning.pdf",
    "chunk_text": "71 \n \n● Object Detection (e.g., localizing multiple objects in an image). \n● Semantic Segmentation (e.g., pixel-wise labeling of images). \n● Face Recognition, Medical Imaging, and more. \nKey Components of CNNs \n1. Convolutional Layers \n● Purpose: Extract local features (e.g., edges, textures) via learnable filters (kernels). \n● Operation: \n○ Slides a kernel (e.g., 3×3) across the input, computing dot products. \n○ Outputs a feature map highlighting detected patterns. \n● Parameters: \n○ kernel_size: Spatial dimensions of the filter (e.g., 3×3). \n○ stride: Step size of the kernel (default=1). \n○ padding: Preserves spatial dimensions (e.g., padding='same'). \n● Activation: ReLU introduces nonlinearity (e.g., ReLU(conv(x))). \n2."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 71,
    "source": "deep learning.pdf",
    "chunk_text": "Pooling Layers \n● Purpose: Reduce spatial dimensions (downsampling) to: \n○ Decrease computational cost. \n○ Introduce translation invariance. \n● Types: \n○ Max Pooling: Selects the maximum value in a window (e.g., 2×2). \n○ Average Pooling: Takes the mean value in a window. \n● Output: Smaller but deeper feature maps. \n3. Fully Connected (Dense) Layers \n● Purpose: Classify features extracted by convolutional/pooling layers. \n● Operation: Flattens the 3D feature maps into a 1D vector for traditional MLP-like \nclassification. \n● Use Case: Final layers in CNNs (e.g., for ImageNet’s 1000-class output)."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 72,
    "source": "deep learning.pdf",
    "chunk_text": "72 \n \nHow CNNs Learn Hierarchical Patterns \n1. Low-Level Features: \n○ Early layers detect edges, colors, and textures (via small kernels). \n2. Mid-Level Features: \n○ Deeper layers combine edges into shapes (e.g., circles, corners). \n3. High-Level Features: \n○ Final layers recognize complex objects (e.g., \"cat ears\" or \"car wheels\"). \nExample: \n● Input Image → Conv1 (Edges) → Conv2 (Textures) → Conv3 (Object Parts) → \nOutput (Full Object). \n \nWhy CNNs Outperform Traditional Methods \n● Parameter Sharing: Kernels are reused across the image, reducing parameters. \n● Local Connectivity: Focuses on local regions, not the entire image. \n● Hierarchical Learning: Mimics human visual perception (simple → complex)."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 72,
    "source": "deep learning.pdf",
    "chunk_text": "Applications of CNNs \nTask \nExample \nImage Classification \nResNet on ImageNet \nObject Detection \nYOLO, Faster R-CNN \nMedical Imaging \nTumor detection in MRI scans \nAutonomous Driving \nLane and pedestrian detection \n \n \n \n \n \n13. \n(a) \nExplain the necessity of word embedding in Natural Language Processing (NLP) and \nCO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 73,
    "source": "deep learning.pdf",
    "chunk_text": "73 \n \noutline its various methodologies. \n· Natural Language Processing (NLP) aims to enable computers to understand, \ninterpret, and generate human language.However, computers fundamentally operate on \nnumbers, not words. This is where word embeddings become indispensable. \nTraditional methods of representing words, like one-hot encoding or Bag-of-Words \n(BoW), have significant limitations: \n● Sparsity and High Dimensionality: One-hot encoding creates a vector where each \nword is represented by a unique dimension, resulting in extremely sparse (mostly \nzeros) and high-dimensional vectors, especially for large vocabularies. This is \ncomputationally expensive and inefficient."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 73,
    "source": "deep learning.pdf",
    "chunk_text": "● Lack of Semantic Meaning: Traditional methods treat each word as an \nindependent entity, failing to capture the semantic relationships between words. For \nexample, \"king\" and \"queen\" are just different words, with no inherent connection \nunderstood by the machine. Similarly, \"cat\" and \"feline\" would be seen as distinct, \neven though they are semantically similar. \n● No Contextual Understanding: These methods disregard the context in which a \nword appears. The same word can have different meanings depending on its \nsurrounding words (e.g., \"bank\" of a river vs. a financial \"bank\"), but traditional \nmethods assign a single representation."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 73,
    "source": "deep learning.pdf",
    "chunk_text": "● Inability to Generalize: If a model is trained on \"fish\" but encounters \"cod\" during \ntesting, it won't understand the semantic similarity if it hasn't seen \"cod\" before, \nleading to poor generalization. \nWord embeddings address these issues by providing dense, low-dimensional, and \nsemantically rich numerical representations of words. They bridge the gap between \nhuman language and machine understanding by: \n● Capturing Semantic and Syntactic Relationships: Words with similar meanings \nor that appear in similar contexts are mapped to vectors that are close to each other \nin the vector space. This allows for arithmetic operations (e.g., \"king\" - \"man\" + \n\"woman\" ≈ \"queen\"), demonstrating an understanding of analogies and \nrelationships."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 73,
    "source": "deep learning.pdf",
    "chunk_text": "● Dimensionality Reduction: Instead of sparse, high-dimensional vectors, word \nembeddings represent words as dense vectors of much lower dimensions (typically \ntens to hundreds). This significantly reduces computational complexity and"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 74,
    "source": "deep learning.pdf",
    "chunk_text": "74 \n \nmemory requirements. \n● Contextual Understanding (in advanced embeddings): Newer embedding \ntechniques can generate different vectors for the same word based on its context, \neffectively handling polysemy (words with multiple meanings). \n● Improved Performance in Downstream NLP Tasks: By providing a richer and \nmore efficient representation of words, embeddings significantly boost the \nperformance of various NLP tasks, including: \no Text Classification: Sentiment analysis, spam detection, topic categorization. \n○ Named Entity Recognition (NER): Identifying people, organizations, \nlocations. \n○ Machine Translation: Understanding word relationships across languages. \n○ Information Retrieval and Search Engines: More accurate matching of \nqueries to relevant documents."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 74,
    "source": "deep learning.pdf",
    "chunk_text": "○ Question Answering: Better understanding of questions and answers. \n○ Text Generation: Creating coherent and contextually relevant text. \nVarious Methodologies of Word Embedding \n· Word embedding methodologies can be broadly categorized into: \n1. Frequency-Based Methods \nThese methods rely on statistical measures of word co-occurrence within a corpus. \n● Bag-of-Words (BoW): \n○ Concept: Represents a document as a multiset of its words, disregarding \ngrammar and word order. It simply counts the frequency of each word in a \ndocument. \n○ Limitations: High dimensionality, sparsity, and no semantic meaning or \ncontextual understanding. It treats \"good\" and \"bad\" as equally distinct as \n\"cat\" and \"dog\"."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 74,
    "source": "deep learning.pdf",
    "chunk_text": "● Term Frequency-Inverse Document Frequency (TF-IDF): \n○ Concept: A statistical measure that evaluates how relevant a word is to a \ndocument in a collection of documents. It increases with the number of \ntimes a word appears in the document but is offset by the frequency of the"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 75,
    "source": "deep learning.pdf",
    "chunk_text": "75 \n \nword in the corpus. This helps to down-weight common words (like \"the\", \n\"a\") that provide little unique information. \n○ Limitations: Still results in sparse vectors and doesn't capture semantic \nrelationships between words directly. \n2. Prediction-Based (Neural Network-based) Methods \n· These methods learn word embeddings by training neural networks to predict words \nbased on their context, or vice-versa. \n● Word2Vec: \n○ Concept: Developed by Google, Word2Vec is a group of shallow, twolayer neural network models trained to reconstruct the linguistic context of \nwords. It learns to map semantically similar words to geometrically close \nembedding vectors."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 75,
    "source": "deep learning.pdf",
    "chunk_text": "It has two main architectures: \n■ Continuous Bag-of-Words (CBOW): Predicts the current word \nbased on its surrounding context words. It's generally faster and \nperforms well for frequent words. \n■ Skip-gram: Predicts the surrounding context words given a target \nword. It's better at capturing rare words and phrases. \n○ Key Feature: Captures semantic relationships (e.g., King - Man + Woman \n≈ Queen). \n● GloVe (Global Vectors for Word Representation): \n○ Concept: Developed by Stanford, GloVe combines the advantages of both \nglobal matrix factorization (like Latent Semantic Analysis) and local \ncontext window methods (like Word2Vec). It constructs word embeddings \nby analyzing global word-word co-occurrence statistics from a large corpus."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 75,
    "source": "deep learning.pdf",
    "chunk_text": "It explicitly models the ratio of co-occurrence probabilities. \n○ Key Feature: Captures both global statistical information and local \ncontextual information. \n● FastText: \n○ Concept: Developed by Facebook AI Research, FastText extends \nWord2Vec by treating each word as a \"bag of character n-grams.\" This \nmeans it considers subword information."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 76,
    "source": "deep learning.pdf",
    "chunk_text": "76 \n \n○ Key Feature: Effective for morphologically rich languages (where words \nhave many forms due to prefixes/suffixes) and can handle out-of-vocabulary \n(OOV) words by inferring their embeddings from their character n-grams. \n3. Contextualized Embeddings \nThese are more advanced methods that generate dynamic word embeddings, meaning the \nvector representation of a word changes based on its surrounding context within a \nsentence. This effectively addresses the issue of polysemy. \n● ELMo (Embeddings from Language Models): \n○ Concept: ELMo is a deep contextualized word representation that models \nboth complex characteristics of word use (e.g., syntax and semantics) and \nhow these uses vary across linguistic contexts (e.g., to model polysemy)."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 76,
    "source": "deep learning.pdf",
    "chunk_text": "It \nuses a bidirectional LSTM to generate word embeddings. \n○ Key Feature: Provides different vectors for a word depending on its usage \nin a sentence. \n● BERT (Bidirectional Encoder Representations from Transformers): \n○ Concept: Developed by Google, BERT is a transformer-based model that \ngenerates context-aware embeddings by considering the context of words \nfrom both left and right sides simultaneously. It's pre-trained on large text \ncorpora using tasks like Masked Language Modeling (predicting masked \nwords) and Next Sentence Prediction. \n○ Key Feature: Bidirectional context understanding, leading to highly \nnuanced word representations. It has revolutionized many NLP tasks."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 76,
    "source": "deep learning.pdf",
    "chunk_text": "● GPT (Generative Pre-trained Transformer) series (GPT-2, GPT-3, GPT-4, \netc.): \n○ Concept: While primarily known for text generation, GPT models also \nproduce powerful contextualized embeddings. They are transformer-based \nand pre-trained to predict the next word in a sequence. Although primarily \nunidirectional in their original form, their vast pre-training on diverse data \nenables them to learn rich representations. \n○ Key Feature: Excellent for language generation and various downstream \nNLP tasks through fine-tuning."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 77,
    "source": "deep learning.pdf",
    "chunk_text": "77 \n \nWord Method \nExample Vectors (Illustrative) \nOne-hot \nhappy = [1,0,0]; excited = [0,1,0]; angry=[0,0,1] \nBag of Words \nSentence: \"happy happy angry\" → [2,0,1] \nTF-IDF \neights words based on rarity \nWord2Vec (CBOW/Skip-Gram) \nhappy = [0.8,0.6]; excited=[0.75,0.7]; angry=[-0.6,-\n0.8] \n \n \n \n(OR) \n \n \n(b) \nExplain the Long Short-Term Memory (LSTM) cell, utilizing a diagram and PyTorch \nimplementation to illustrate its workings. \nLSTM (Long Short-Term Memory) is a special type of Recurrent Neural Network (RNN) \ncell designed to remember information over long sequences and avoid the vanishing \ngradient problem in standard RNNs. \n· It does so by maintaining a cell state that runs through the sequence with minimal \nchanges, controlled by gates that decide what to keep, update, or forget."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 77,
    "source": "deep learning.pdf",
    "chunk_text": "An LSTM cell has 3 main gates: \n· Forget Gate (fₜ): Decides what information to discard from the cell state. \n· Input Gate (iₜ): Decides what new information to add to the cell state. \n· Output Gate (oₜ): Decides what to output based on the updated cell state. \nAnd an internal candidate cell state (𝒞̃ₜ) that suggests new info to add. \n \nAn LSTM's core strength lies in its \"gates,\" which regulate the flow of information into \nand out of the cell state, allowing it to remember or forget information over long \nsequences. \n1. Forget Gate Layer (First Step): \nCO3"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 78,
    "source": "deep learning.pdf",
    "chunk_text": "78 \n \n \n \no Purpose: To decide what information from the previous cell state (Ct−1) should be \ndiscarded. \no Mechanism: A sigmoid layer looks at the previous hidden state (ht−1) and the current \ninput (xt). \no Output: Generates a number between 0 and 1 for each number in Ct−1. \n§ '1' means \"completely keep this.\" \n§ '0' means \"completely get rid of this.\" \no Example (Language Model): If the cell state contains the gender of the previous \nsubject, and a new subject is encountered, the forget gate will output '0' for the old subject's \ngender, effectively forgetting it. \n2. Input Gate Layer & Tanh Layer (Second Step): \n \no Purpose: To decide what new information will be stored in the current cell state (Ct)."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 78,
    "source": "deep learning.pdf",
    "chunk_text": "This step has two parts: \no Input Gate Layer: \n§ Mechanism: A sigmoid layer examines ht−1 and xt. \n§ Output: Decides which new values will be updated (represented by it). \no Tanh Layer (Candidate Values):"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 79,
    "source": "deep learning.pdf",
    "chunk_text": "79 \n \n§ Mechanism: A tanh layer also looks at ht−1 and xt. \n§ Output: Creates a vector of new candidate values (C~t) that could potentially be added \nto the state. These values are scaled between -1 and 1. \no Combined Goal: These two components work together to determine what new \ninformation is relevant to incorporate. \no Example (Language Model): This is where the LSTM identifies and prepares to add \nthe gender of the new subject to the cell state. \n2. Updating the Cell State (Third Step): \n \no Purpose: To transform the old cell state (Ct−1) into the new cell state (Ct) by \nincorporating the decisions made by the forget and input gates. \no Mechanism: \n§ The old cell state (Ct−1) is multiplied by the forget gate's output (ft), effectively \ndiscarding the unwanted information."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 79,
    "source": "deep learning.pdf",
    "chunk_text": "§ The new candidate values (C~t) are scaled by the input gate's output (it) (meaning only \nthe selected new information is considered). \n§ These two results are then added together to form the new cell state (Ct=ft∗Ct−1+it∗C~t\n). \no Example (Language Model): This is the moment where the old subject's gender \ninformation is actually removed, and the new subject's gender information is actively \nincorporated into the cell state. \n2. Output Gate Layer (Fourth Step):"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 80,
    "source": "deep learning.pdf",
    "chunk_text": "80 \n \n \no Purpose: To decide what information from the current cell state (Ct) will be output as \nthe hidden state (ht). \no Mechanism: \n§ An output sigmoid layer looks at ht−1 and xt to determine which parts of the cell state \nare relevant for output. \n§ The new cell state (Ct) is passed through a tanh function (to push values between -1 and \n1). \n§ The tanh output of Ct is then multiplied element-wise by the output of the sigmoid \noutput gate. This filters the cell state, only outputting the relevant parts. \no Example (Language Model): After processing a subject, the LSTM might output \ninformation relevant to a verb (e.g., whether the subject is singular or plural), which would \nguide the conjugation of the next verb in the sentence."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 80,
    "source": "deep learning.pdf",
    "chunk_text": "import torch \nimport torch.nn as nn \n \nclass LSTMCell(nn.Module): \n def __init__(self, input_size, hidden_size): \n super(LSTMCell, self).__init__() \n self.input_size = input_size \n self.hidden_size = hidden_size \n \n # Linear layers for input, forget, cell, and output gates \n self.i2h = nn.Linear(input_size, 4 * hidden_size) \n self.h2h = nn.Linear(hidden_size, 4 * hidden_size) \n \n def forward(self, x, hidden): \n h_prev, c_prev = hidden # hidden state and cell state"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 81,
    "source": "deep learning.pdf",
    "chunk_text": "81 \n \n # Concatenate x and h_prev then apply linear transformations \n gates = self.i2h(x) + self.h2h(h_prev) \n i_gate, f_gate, c_gate, o_gate = gates.chunk(4, dim=1) \n \n i = torch.sigmoid(i_gate) \n f = torch.sigmoid(f_gate) \n o = torch.sigmoid(o_gate) \n g = torch.tanh(c_gate) \n \n c_next = f * c_prev + i * g \n h_next = o * torch.tanh(c_next) \n \n return h_next, c_next \n \n \n \n \n \n \n14. \n(a) \nExplain the fundamental architectural components of Autoencoders in neural networks, \noutlining their roles and functions, and demonstrate these concepts through a PyTorch \nimplementation. \n \nCO4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 82,
    "source": "deep learning.pdf",
    "chunk_text": "82 \n \n \n \n \n \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \n \n# 1. Define the Autoencoder Architecture \nclass Autoencoder(nn.Module): \n def __init__(self, input_dim, hidden_dim): \n super(Autoencoder, self).__init__()"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 83,
    "source": "deep learning.pdf",
    "chunk_text": "83 \n \n \n # Encoder \n self.encoder = nn.Sequential( \n nn.Linear(input_dim, hidden_dim), # Input layer to hidden layer \n nn.ReLU(True), # Non-linear activation \n # You can add more layers here for a deeper encoder \n ) \n \n # Decoder \n self.decoder = nn.Sequential( \n nn.Linear(hidden_dim, input_dim), # Hidden layer to output layer \n nn.Sigmoid() # Sigmoid for output (pixel values between 0 and 1) \n ) \n \n def forward(self, x): \n encoded = self.encoder(x) \n decoded = self.decoder(encoded) \n return decoded \n \n# 2."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 83,
    "source": "deep learning.pdf",
    "chunk_text": "Hyperparameters and Data Loading \nINPUT_DIM = 28 * 28 # For MNIST images (28x28 pixels) \nHIDDEN_DIM = 128 # Dimensionality of the latent space (bottleneck) \nBATCH_SIZE = 64 \nNUM_EPOCHS = 20 \nLEARNING_RATE = 1e-3 \n \n# MNIST Dataset loading \ntransform = transforms.Compose([ \n transforms.ToTensor(), # Convert PIL Image to PyTorch Tensor \n transforms.Normalize((0.5,), (0.5,)) # Normalize pixel values to [-1, 1] \n]) \n \n# Download and load training data \ntrain_dataset = datasets.MNIST(root='./data', train=True, download=True, \ntransform=transform) \ntrain_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True) \n \n# 3."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 83,
    "source": "deep learning.pdf",
    "chunk_text": "Model, Loss, and Optimizer Initialization \nmodel = Autoencoder(INPUT_DIM, HIDDEN_DIM) \ncriterion = nn.MSELoss() # Mean Squared Error for reconstruction loss \noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE) \n \n# Move model to GPU if available \ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \nmodel.to(device) \n \nprint(f\"Using device: {device}\") \nprint(model) # Print model architecture \n \n# 4. Training Loop \nprint(\"Starting training...\") \nfor epoch in range(NUM_EPOCHS): \n total_loss = 0 \n for batch_idx, (data, _) in enumerate(train_loader):"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 84,
    "source": "deep learning.pdf",
    "chunk_text": "84 \n \n # Flatten the images from (batch_size, 1, 28, 28) to (batch_size, 784) \n data = data.view(-1, INPUT_DIM).to(device) \n \n # Zero the gradients \n optimizer.zero_grad() \n \n # Forward pass \n reconstructed_data = model(data) \n \n # Calculate loss \n loss = criterion(reconstructed_data, data) \n \n # Backward pass and optimize \n loss.backward() \n optimizer.step() \n \n total_loss += loss.item() \n \n avg_loss = total_loss / len(train_loader) \n print(f\"Epoch [{epoch+1}/{NUM_EPOCHS}], Loss: {avg_loss:.4f}\") \n \nprint(\"Training finished.\") \n \n# 5."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 84,
    "source": "deep learning.pdf",
    "chunk_text": "Visualization of Reconstruction (Optional)  model.eval() # Set model to evaluation mode  with torch.no_grad():   # Get a batch of test images   test_dataset = datasets.MNIST(root='./data', train=False, download=True,  transform=transform)   test_loader = DataLoader(test_dataset, batch_size=10, shuffle=True)     data_iter = iter(test_loader)   images, _ = next(data_iter)     # Flatten images and move to device   original_images = images.view(-1, INPUT_DIM).to(device)     # Reconstruct images   reconstructed_images = model(original_images).cpu() # Move back to CPU for  plotting   original_images = original_images.cpu()     # Unnormalize images for plotting (assuming original normalization was (0.5, 0.5))   # reconstructed_images = reconstructed_images * 0.5 + 0.5   # original_images ="
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 84,
    "source": "deep learning.pdf",
    "chunk_text": "original_images * 0.5 + 0.5     # Plot original and reconstructed images   fig, axes = plt.subplots(2, 10, figsize=(20, 4))   for i in range(10):   # Original Images   axes[0, i].imshow(original_images[i].reshape(28, 28), cmap='gray')   axes[0, i].axis('off')   # Reconstructed Images   axes[1, i].imshow(reconstructed_images[i].reshape(28, 28), cmap='gray')"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 85,
    "source": "deep learning.pdf",
    "chunk_text": "85 \n \n axes[1, i].axis('off') \n \naxes[0, 0].set_title(\"Original\") \n axes[1, 0].set_title(\"Reconstructed\") \n plt.show() \n \n \n \n \n(OR) \n \n \n(b) \nDevelop a PyTorch-based implementation to train a Generative Adversarial Network (GAN) \non the MNIST dataset. Include an appropriate training loop, loss functions, and optimizer \nsetup."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 85,
    "source": "deep learning.pdf",
    "chunk_text": "# PyTorch GAN on MNIST - Full Implementation \n \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nfrom torchvision import datasets, transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \n \n# Generator Network \nclass Generator(nn.Module): \n def __init__(self, latent_dim): \n super(Generator, self).__init__() \n self.model = nn.Sequential( \n nn.Linear(latent_dim, 128), \n nn.ReLU(True), \n nn.Linear(128, 256), \n nn.BatchNorm1d(256), \n nn.ReLU(True), \n nn.Linear(256, 512), \n nn.BatchNorm1d(512), \n nn.ReLU(True), \n nn.Linear(512, 784), \n nn.Tanh() \nCO4"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 86,
    "source": "deep learning.pdf",
    "chunk_text": "86     )   def forward(self, z):   img = self.model(z)   return img.view(z.size(0), 1, 28, 28)    # Discriminator Network  class Discriminator(nn.Module):   def __init__(self):   super(Discriminator, self).__init__()   self.model = nn.Sequential(   nn.Linear(784, 512),   nn.LeakyReLU(0.2, inplace=True),   nn.Linear(512, 256),   nn.LeakyReLU(0.2, inplace=True),   nn.Linear(256, 1),   nn.Sigmoid()   )   def forward(self, img):   img_flat = img.view(img.size(0), -1)   return self.model(img_flat)    # Hyperparameters and DataLoader  latent_dim = 100  batch_size = 128  lr = 0.0002  epochs = 50  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")    transform = transforms.Compose([   transforms.ToTensor(),   transforms.Normalize([0.5], [0.5])  ])  mnist ="
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 86,
    "source": "deep learning.pdf",
    "chunk_text": "datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 87,
    "source": "deep learning.pdf",
    "chunk_text": "87    dataloader = DataLoader(mnist, batch_size=batch_size, shuffle=True)    # Initialize networks and optimizers  generator = Generator(latent_dim).to(device)  discriminator = Discriminator().to(device)  optimizer_G = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))  optimizer_D = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))  adversarial_loss = nn.BCELoss()    # Training Loop  for epoch in range(epochs):   for i, (imgs, _) in enumerate(dataloader):   real_imgs = imgs.to(device)   valid = torch.ones(imgs.size(0), 1, device=device)   fake = torch.zeros(imgs.size(0), 1, device=device)     # Train Generator   optimizer_G.zero_grad()   z = torch.randn(imgs.size(0), latent_dim, device=device)   gen_imgs = generator(z)   g_loss ="
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 87,
    "source": "deep learning.pdf",
    "chunk_text": "adversarial_loss(discriminator(gen_imgs), valid)   g_loss.backward()   optimizer_G.step()     # Train Discriminator   optimizer_D.zero_grad()   real_loss = adversarial_loss(discriminator(real_imgs), valid)   fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)   d_loss = (real_loss + fake_loss) / 2   d_loss.backward()   optimizer_D.step()     if i % 100 == 0:"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 88,
    "source": "deep learning.pdf",
    "chunk_text": "88 \n \n print(f\"Epoch [{epoch}/{epochs}] Batch {i}/{len(dataloader)} \\ \n Loss D: {d_loss.item():.4f}, Loss G: {g_loss.item():.4f}\") \n \n# Visualize generated images \ndef generate_images(generator, n=25): \n z = torch.randn(n, latent_dim).to(device) \n gen_imgs = generator(z).cpu().detach() \n gen_imgs = gen_imgs.view(n, 1, 28, 28) \n fig, axes = plt.subplots(5, 5, figsize=(5, 5)) \n for i, ax in enumerate(axes.flatten()): \n ax.imshow(gen_imgs[i].squeeze(), cmap=\"gray\") \n ax.axis(\"off\") \n plt.tight_layout() \n plt.show() \n \ngenerate_images(generator) \n \n \n \n \n \n15. \n(a) \nDescribe object classification and localization. How do modern object detection models \nperform both tasks simultaneously? Support your answer with suitable architectural \nexamples. \n \nCO5"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 89,
    "source": "deep learning.pdf",
    "chunk_text": "89 \n \n \n \n \nobject detection models can be categorized into two types: \n1. Two-Stage Detectors: These models typically perform the object detection \nprocess in two distinct steps: \n○ Region Proposal: In the first stage, the model generates a set of \"region \nproposals\" or \"regions of interest (RoIs)\" that are likely to contain objects. \nThis is essentially the localization part of the initial guess. \n○ Classification and Bounding Box Regression: In the second stage, these \nproposed regions are then fed into a classification network to determine the \nobject's class and a regression network to refine the bounding box \ncoordinates, making them more precise. \n2. One-Stage Detectors: These models perform both tasks (classification and \nlocalization) in a single forward pass through the network."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 89,
    "source": "deep learning.pdf",
    "chunk_text": "They directly predict \nbounding boxes and class probabilities for various locations across the image. \nThis approach generally leads to faster inference times, making them suitable for \nreal-time applications. \nHere's how they achieve simultaneous operation: \n· Shared Feature Extraction Backbone: Both two-stage and one-stage detectors \ntypically start with a convolutional neural network (CNN) backbone (e.g., \nResNet, VGG, Darknet) that extracts rich, hierarchical features from the input \nimage. These features are then shared by both the classification and localization \nbranches. The early layers capture low-level features (edges, textures), while \ndeeper layers capture high-level semantic features."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 89,
    "source": "deep learning.pdf",
    "chunk_text": "· Anchor Boxes / Priors: Many models utilize \"anchor boxes\" (also known as prior \nboxes or default boxes). These are predefined bounding box shapes and sizes at \nvarious locations across the image. The model then predicts offsets from these \nanchor boxes (for localization) and class probabilities for each anchor box (for \nclassification).This allows the model to predict multiple objects at different scales"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 90,
    "source": "deep learning.pdf",
    "chunk_text": "90 \n \nand aspect ratios simultaneously. \n· Grid-based Prediction (in One-Stage Detectors): Models like YOLO divide the \ninput image into a grid. Each grid cell is responsible for predicting objects whose \ncenter falls within that cell. For each cell, it predicts a fixed number of bounding \nboxes, their confidence scores (objectness: whether an object is present in that \nbox), and class probabilities. This directly links spatial information (grid cell) to \nthe object's presence and class. \n· Multi-task Loss Function: The training of these models involves a composite \nloss function that simultaneously optimizes for both classification accuracy and \nbounding box regression accuracy."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 90,
    "source": "deep learning.pdf",
    "chunk_text": "○ Classification Loss: Typically cross-entropy loss, measures how well the \nmodel predicts the correct class label for each object. \n○ Localization (Regression) Loss: Often L1 or L2 loss (or more advanced \nforms like IoU loss), measures the difference between the predicted bounding \nbox coordinates and the ground truth bounding box coordinates. \n· Non-Maximum Suppression (NMS): After the model generates numerous \nbounding box predictions (many overlapping), NMS is applied as a postprocessing step. It removes redundant and less confident bounding boxes, \nkeeping only the most confident and representative ones for each detected object. \nSuitable Architectural Examples: \n1."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 90,
    "source": "deep learning.pdf",
    "chunk_text": "YOLO (You Only Look Once) - A prominent One-Stage Detector \n \n● Architecture: YOLO models (YOLOv1, YOLOv2, YOLOv3, YOLOv4, YOLOv5, \nYOLOv7, YOLOv8, etc.) are known for their speed. They treat object detection as \na regression problem. \n○ The input image is divided into an S×S grid. \n○ Each grid cell predicts B bounding boxes, a confidence score for each box \n(indicating the probability of an object being present in that box), and C \nconditional class probabilities (probability of an object being a specific \nclass, given that an object is present). \n○ The final output is a tensor of size S×S×(B×5+C). The 5 for each bounding \nbox corresponds to (x,y,w,h) coordinates and objectness score."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 90,
    "source": "deep learning.pdf",
    "chunk_text": "● Simultaneous Operation: YOLO directly predicts both the bounding box \ncoordinates and class probabilities for each grid cell in a single forward pass."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 91,
    "source": "deep learning.pdf",
    "chunk_text": "91 \n \nThere's no separate region proposal stage. The network learns to directly map \nimage pixels to bounding box parameters and class scores. \n2. Faster R-CNN - A classic Two-Stage Detector \n \n● Architecture: Faster R-CNN significantly improved upon its predecessors (R-CNN, \nFast R-CNN) by introducing a Region Proposal Network (RPN). \n○ Backbone CNN: Extracts a feature map from the input image. \n○ Region Proposal Network (RPN): This is a small convolutional network that \nslides over the feature map generated by the backbone. At each slidingwindow location, it predicts two things: \n■ Objectness Score: Whether a region contains an object or not (binary \nclassification)."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 91,
    "source": "deep learning.pdf",
    "chunk_text": "■ Bounding Box Refinements: Adjustments to a set of predefined \n\"anchor boxes\" at that location to better fit potential objects. \n○ RoI Pooling / RoI Align: The proposed regions (RoIs) from the RPN are \nthen pooled (or aligned, for more precise results) to a fixed size feature \nvector. \n○ Classification and Regression Head: These fixed-size feature vectors are fed \ninto a fully connected layer (or small CNNs) with two output branches: \n■ Classification Head: Predicts the actual class probabilities for each \nRoI. \n■ Bounding Box Regression Head: Further refines the bounding box \ncoordinates for each classified object."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 91,
    "source": "deep learning.pdf",
    "chunk_text": "● Simultaneous Operation: Faster R-CNN processes the image in two stages, but the \nRPN and the detection network share the same convolutional features, making it an \nend-to-end trainable system. The RPN provides the initial \"localization\" proposals, \nand the subsequent stage refines these localizations and performs the final \n\"classification.\" \n3. SSD (Single Shot MultiBox Detector) - Another One-Stage Detector"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 92,
    "source": "deep learning.pdf",
    "chunk_text": "92 \n \n \n● Architecture: SSD is also a one-stage detector that aims for speed and accuracy. \n○ It uses a VGG or ResNet-like backbone for feature extraction. \n○ It predicts bounding boxes and class probabilities at multiple scales (from \ndifferent convolutional layers) of the feature map. This is achieved by using \na set of default (anchor) boxes with varying scales and aspect ratios at each \nfeature map location. \n● Simultaneous Operation: Similar to YOLO, SSD performs classification and \nlocalization simultaneously by directly predicting offsets to default boxes and their \ncorresponding class scores from feature maps at various resolutions. This multiscale prediction helps in detecting objects of different sizes effectively."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 92,
    "source": "deep learning.pdf",
    "chunk_text": "(OR) \n \n \n(b) \nExplain how Region Proposal Networks (RPN) are integrated into Faster R-CNN, and \nevaluate its performance in terms of speed and accuracy compared to previous models. \nA Region Proposal Network (RPN) is a specialized neural network that predicts object \nregions or \"proposals\" in an image.Its integration into the Faster R-CNN (Region-based \nConvolutional Neural Network) architecture marked a pivotal moment in object detection, \ncreating a unified, end-to-end deep learning model that significantly outpaced its \npredecessors in both speed and accuracy. \nThe core innovation of the RPN is that it shares the powerful, deep convolutional features of \nthe main object detection network, making the region proposal step nearly instantaneous."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 92,
    "source": "deep learning.pdf",
    "chunk_text": "This resolved the major computational bottleneck present in earlier models like R-CNN and \nFast R-CNN, which relied on slow, external algorithms like Selective Search to generate \npotential object locations. \nIntegration into Faster R-CNN Architecture \nCO5"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 93,
    "source": "deep learning.pdf",
    "chunk_text": "93 \n \n \nThe Faster R-CNN model is composed of two main modules that share a common set of \nconvolutional layers (the \"backbone,\" e.g., VGG or ResNet): \n1. Region Proposal Network (RPN): This network's sole job is to identify a set of highquality rectangular region proposals that are likely to contain objects. \n2. Fast R-CNN Detector: This network takes the proposed regions from the RPN and \nperforms final classification (e.g., \"person,\" \"car\") and refines the bounding box \ncoordinates. \nThe integration works as follows: \n1. Shared Feature Map: An input image is first processed by the backbone CNN, producing \na single, high-level feature map. This feature map is the crucial link, as it's used by both the \nRPN and the final detector. \n2."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 93,
    "source": "deep learning.pdf",
    "chunk_text": "Anchor Boxes: The RPN slides a small network over this feature map. At each location, \nit considers multiple predefined bounding boxes of different scales and aspect ratios, known \nas anchor boxes. \n3. Dual Outputs: For each anchor box, the RPN outputs two predictions: \n○ An objectness score: The probability that an anchor contains any object \nversus being background. \n○ Bounding box regression: Adjustments to the anchor's coordinates to make it \nfit a potential object more tightly. \n4. Proposal Hand-off: The highest-scoring region proposals from the RPN are then passed \nto the second stage. An RoI (Region of Interest) Pooling layer extracts a fixed-size feature"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 94,
    "source": "deep learning.pdf",
    "chunk_text": "94 \n \nvector for each proposal from the same shared feature map. \n5. Final Detection: These feature vectors are fed into the Fast R-CNN module, which \nmakes the final class prediction and further refines the bounding box for each object. \nThis unified architecture allows the RPN to be trained jointly with the detector, enabling it \nto learn how to generate proposals that are specifically tailored for the main detection \nnetwork, thereby improving overall accuracy. \n \n· The most significant impact of the RPN was on speed. By replacing the slow CPUbased Selective Search algorithm (which took ~2 seconds per image) with a lightweight \nneural network running on the GPU, the time spent on generating proposals plummeted to \nabout 10 milliseconds."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 94,
    "source": "deep learning.pdf",
    "chunk_text": "This elimination of the primary bottleneck allowed the entire \ndetection pipeline to be nearly real-time, making it over 10 times faster than Fast R-CNN \nand about 250 times faster than the original R-CNN. \n· While speed was the headline improvement, accuracy also saw a notable boost. Unlike \nSelective Search, which is a fixed, hand-engineered algorithm, the RPN is a trainable \nnetwork. By training it jointly with the detection network, the RPN learns to generate \nproposals that are optimized for the detector and the specific classes in the dataset. This \nability to learn high-quality, data-specific proposals helps the detector perform better, \nleading to a higher mean Average Precision (mAP) compared to models that use static \nproposal methods."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 94,
    "source": "deep learning.pdf",
    "chunk_text": "PART C (1 x 15 = 15 marks) \n \n (Case study/Comprehensive type Questions) \n \n \n \n \n \nCO"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 95,
    "source": "deep learning.pdf",
    "chunk_text": "95 \n \n16. \n(a) \nImplement transfer learning by using VGG as the base model to classify the CIFAR-10 \ndataset using PyTorch. \ni) Load the CIFAR-10 dataset. (3 Mark) \nii) Use a pre-trained VGG model as the base model (4 Mark) \niii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function. (4 Mark) \niv) Train the model with training data and validation data. (4 Mark) \n \n \n \n \nCO2 \n \n \n(OR)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 96,
    "source": "deep learning.pdf",
    "chunk_text": "96 \n \n \n(b) \nThe MNIST dataset consists of 70000 28x28 grayscale digit images in 10 classes. There \nare 60000 training images and 10000 test images. Develop a PyTorch implementation for \nthe following. \ni) Load the MNIST dataset. (4 Mark) \nii) Create a sequential model with the appropriate number of neurons in the output \nlayer, activation function, and loss function. (5 Mark) \niii) Train the model with training data and validate it using the test dataset, and \nevaluate its accuracy."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 96,
    "source": "deep learning.pdf",
    "chunk_text": "(6 Mark) \n#Import Libraries \nimport torch \nimport torch.nn as nn \nimport torch.optim as optim \nimport torchvision \nimport torchvision.transforms as transforms \nfrom torch.utils.data import DataLoader \nimport matplotlib.pyplot as plt \nimport numpy as np \nfrom sklearn.metrics import confusion_matrix, classification_report \nimport seaborn as sns \n \n# Data Preprocessing \ntransform = transforms.Compose([ \n transforms.ToTensor(), \n transforms.Normalize((0.5,), (0.5,)) # Normalize images \n]) \n \n# Load MNIST dataset \nCO2"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 97,
    "source": "deep learning.pdf",
    "chunk_text": "97    train_dataset = torchvision.datasets.MNIST(root=\"./data\", train=True,  transform=transform, download=True)  test_dataset = torchvision.datasets.MNIST(root=\"./data\", train=False,  transform=transform, download=True)    # Check dataset  image, label = train_dataset[0]  print(\"Image shape:\", image.shape)  print(\"Number of training samples:\", len(train_dataset))    image, label = test_dataset[0]  print(\"Image shape:\", image.shape)  print(\"Number of testing samples:\", len(test_dataset))    # Create DataLoaders  train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)    # Define the CNN Model  class CNNClassifier(nn.Module):   def __init__(self):   super(CNNClassifier, self).__init__()   self.conv1 ="
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 97,
    "source": "deep learning.pdf",
    "chunk_text": "nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3,  padding=1)   self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3,  padding=1)   self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3,  padding=1)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 98,
    "source": "deep learning.pdf",
    "chunk_text": "98     self.pool = nn.MaxPool2d(kernel_size=2, stride=2)   self.fc1 = nn.Linear(128 * 3 * 3, 128) # Adjusted for MNIST image size   self.fc2 = nn.Linear(128, 64)   self.fc3 = nn.Linear(64, 10) # 10 classes in MNIST     def forward(self, x):   x = self.pool(torch.relu(self.conv1(x)))   x = self.pool(torch.relu(self.conv2(x)))   x = self.pool(torch.relu(self.conv3(x)))   x = x.view(x.size(0), -1) # Flatten the tensor   x = torch.relu(self.fc1(x))   x = torch.relu(self.fc2(x))   x = self.fc3(x)   return x    # Print model summary  from torchsummary import summary  model = CNNClassifier()  if torch.cuda.is_available():   device = torch.device(\"cuda\")   model.to(device)  print('Name: ')  print('Register Number: ')  summary(model, input_size=(1, 28, 28)) # MNIST images are 28x28 with 1  channel"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 99,
    "source": "deep learning.pdf",
    "chunk_text": "99 \n \ncriterion = nn.CrossEntropyLoss() \noptimizer = optim.Adam(model.parameters(), lr=0.001) \n \n# Training Function \ndef train_model(model, train_loader, num_epochs=10): \n for epoch in range(num_epochs): \n model.train() \n running_loss = 0.0 \n for images, labels in train_loader: \n if torch.cuda.is_available(): \n images, labels = images.to(device), labels.to(device) \n \n optimizer.zero_grad() \n outputs = model(images) \n loss = criterion(outputs, labels) \n loss.backward() \n optimizer.step() \n running_loss += loss.item() \n \n print('Name: ') \n print('Register Number: ') \n print(f'Epoch [{epoch+1}/{num_epochs}], Loss: \n{running_loss/len(train_loader):.4f}') \n \n# Train the model \ntrain_model(model, train_loader, num_epochs=10)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 100,
    "source": "deep learning.pdf",
    "chunk_text": "100 \n \n# Testing Function \ndef test_model(model, test_loader): \n model.eval() \n correct = 0 \n total = 0 \n all_preds = [] \n all_labels = [] \n \n with torch.no_grad(): \n for images, labels in test_loader: \n if torch.cuda.is_available(): \n images, labels = images.to(device), labels.to(device) \n \n outputs = model(images) \n _, predicted = torch.max(outputs, 1) \n total += labels.size(0) \n correct += (predicted == labels).sum().item() \n all_preds.extend(predicted.cpu().numpy()) \n all_labels.extend(labels.cpu().numpy()) \n \n accuracy = correct / total \n print('Name: ') \n print('Register Number: ') \n print(f'Test Accuracy: {accuracy:.4f}') \n # Compute confusion matrix \n cm = confusion_matrix(all_labels, all_preds)"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 101,
    "source": "deep learning.pdf",
    "chunk_text": "101     plt.figure(figsize=(8, 6))   print('Name: ')   print('Register Number: ')   sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',  xticklabels=test_dataset.classes, yticklabels=test_dataset.classes)   plt.xlabel('Predicted')   plt.ylabel('Actual')   plt.title('Confusion Matrix')   plt.show()   # Print classification report   print('Name: ')   print('Register Number: ')   print(\"Classification Report:\")   print(classification_report(all_labels, all_preds, target_names=[str(i) for i in  range(10)]))  # Test the model  test_model(model, test_loader)    # Function to predict and visualize an image  def predict_image(model, image_index, dataset):   model.eval()   image, label = dataset[image_index]   if torch.cuda.is_available():   image = image.to(device)     with torch.no_grad():"
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 102,
    "source": "deep learning.pdf",
    "chunk_text": "102 \n \n _, predicted = torch.max(output, 1) \n \n class_names = [str(i) for i in range(10)] \n \n print('Name: ') \n print('Register Number: ') \n plt.imshow(image.cpu().squeeze(), cmap=\"gray\") \n plt.title(f'Actual: {class_names[label]}\\nPredicted: \n{class_names[predicted.item()]}') \n plt.axis(\"off\") \n plt.show() \n print(f'Actual: {class_names[label]}, Predicted: \n{class_names[predicted.item()]}') \n \n# Predict and visualize an image \npredict_image(model, image_index=80, dataset=test_dataset) \n \n \n _____________________ \n \n \n \nFor Set 3 QP \n \nPart - A \nQuestion No. \n1 \n2 \n3 \n4 \n5 \n6 \n7 \n8 \n9 \n10 \nKnowledge Level \nK3 \nK2 \nK2 \nK2 \nK2 \nK2 \nK2 \nK2 \nK3 \nK3 \nDifficulty Level \n3 \n3 \n2 \n2 \n2 \n2 \n3 \n2 \n3 \n2 \n \n Part - B \nPart - C \nQuestion No."
  },
  {
    "book_id": "8ac7446c-edb0-4b33-a3c7-b1ac8b2ee378",
    "book_title": "deep learning.pdf",
    "page": 102,
    "source": "deep learning.pdf",
    "chunk_text": "11 \n(a) \n11 \n(b) \n12 \n(a) \n12 \n(b) \n13 \n(a) \n13 \n(b) \n14 \n(a) \n14 \n(b) \n15 \n(a) \n15 \n(b) \n16 \n(a) \n16 \n(b) \nKnowledge Level \nK2 \nK6 \nK3 \nK3 \nK2 \nK3 \nK3 \nK6 \nK3 \nK2 \nK6 \nK6 \nDifficulty Level \n3 \n3 \n3 \n3 \n3 \n4 \n3 \n4 \n3 \n3 \n4 \n4"
  }
]